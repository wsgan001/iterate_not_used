---
author: evo
comments: true
date: 2018-04-26 03:37:09+00:00
layout: post
link: http://106.15.37.116/2018/04/26/entropy/
slug: entropy
title: 信息论-熵
wordpress_id: 3902
categories:
- 随想与反思
tags:
- '@todo'
- '@want_to_know'
---

<!-- more -->

[mathjax]


# REF






  * 七月在线 机器学习


  * 《机器学习》周志华




# TODO






  * **机器学习书中的KL散度的东西需要添加下。**


  * **而且感觉这一篇太大了，要不要拆分下。**




# MOTIVE






  * a





* * *






# 缘由：


实际上在机器学习中，经常用到熵，比如最大熵模型，以及决策树剪枝的时候，因此把熵单独拿出来，好好总结下。不然总是这里一点那里一点。




# 信息量和熵




## 什么是信息量


比如说，讲课，讲到的知识越不熟悉，说明信息量越大，如果是都知道的，那么熵就是0，就没有信息量。**听到这个地方，我就感觉，一个信息的信息量不仅仅与信息本身有关，还与接受信息的人有关，因为对于不同的人来说，信息量是不同的，然后又想到了量子力学中观测者对于结果的干扰。。看下有没有相似性。**

现在我们把看一下刚才的 \(log_2p\) 是什么？

比如说我们想定义一个东西的信息量，就是一个事件发生的概率越小的话，那么信息量就越大，那么如何定义信息量呢？

首先我们对要定义的这个东西有两个原则：




  * 某事件发生的概率小，则该事件的信息量大。


  * 如果两个事件X和Y独立，即 p(xy)=p(x)p(y) ，假定X和Y的信息量分别为h(X)和h(Y)，则二者同时发生的信息量应该为 h(XY)=h(X)+h(Y)。


上面的相加比较符合我们对信息的感觉。

那么怎么弄成加起来的形式呢？最简单的形式就是取对数，由于概率是0~1，因此对数后是负的，因此我们这么定义事件 X 发生的信息量： \(h(x)=-log_2x\)

可见，如果x的概率越小，它如果发生了，那么它的信息量是很大的。

那么我们能不能定义信息量的期望呢？比如事件X的信息量的期望如何计算呢？

OK，这时候我们就介绍一下熵：


## 什么是熵？

什么是熵呢？对随机事件的信息量求期望，就得到了熵的定义：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/EdFbl0GDdd.png?imageslim)

可见，熵是来定义这个随机事件的信息量的强弱大小的。**嗯，不错。**

有两点需要注意：

1. 这个地方的 p(x) 是一个函数，不是一个数。
2. 在经典熵的定义，底数是 2，单位是 bit   。但是实际上有时候也用以 e 、10为底的，为什么可以改变这个底数呢？因为我们不关心这个信息量的值具体是什么，我们关心的是当一个事件的信息量取最大的时候，他的 p 应该取什么，即可以通过取什么参数来使信息量达到最大。在本例中，为分析方便使用底数 e，此时单位是 nat (奈特)。

## OK，我们来研究下熵里面的这个函数 f(x)=xlnx


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/AhCc5DcHBa.png?imageslim)

由于在 x=0 的时候，lnx 是没有定义的，但 \(\underset{lim}{x\rightarrow 0}f(x)=0\) ，因此为了使这个函数连续，因此我们强制定义 \(f(0)=0\)。


## 实际上我们可以把这个函数画出来看下


代码如下：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/87IB393edc.png?imageslim)



![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Kd7Da8I23l.png?imageslim)


# OK，我们开始对熵进行学习

## 对熵的理解


由于 \(p(x)\) 是0~1的，因此 \(ln p(x)\)就是小于0的，那么熵就是大于等于0 的，而且由于 \(p(x)\) 实际上是一个分布函数，那么 \(H(X)\) 就可以看成一个函数的函数，也就是一个函数到一个值的映射。**在讲到这里的时候提到了泛函和变分推导，泛函是什么？变分推导是什么？ **

熵是随机变量不确定性的度量：

  * 随机变量的不确定性越大，熵值越大。
  * 若随机变量退化成定值，即若某一个事件以概率1发生。那么这件事的熵就是0，就是极小值。


而这种不确定性度量的本质就是信息熵的期望。**？是这样吗？**

均匀分布是 ”最不确定“ 的分布。**厉害了**

上面提到了熵的最小值是0，那么它有上界吗？上界是多少？

比如说，对于一个两点分布而言：


## 两点分布的熵




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/m1h5HEF49E.png?imageslim)

那么把两点分布变成三点分布呢？


## 三点分布的熵


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0cGaE92gcE.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8F3JGF2Bm3.png?imageslim)

第一幅图的高度是熵，后面两幅都是旋转后的，可见，p1=p2=1/3的时候，三点的概率分布最大。**这幅图与dirichlet分布有关吗？**

可见，n 个的概率都相等的时候，熵最大，所以熵的最大值就是： \(log|X|\)。

也即：

\[0\leq H(X)\leq log|X|\]

OK，下面我们再从另外一个角度来探讨熵是什么？






# 组合数的关系


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8lLIiKFKKb.png?imageslim)

## 推到


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/D7jH1jemb2.png?imageslim)

其中 \(ln N!\rightarrow N(lnN-1)\) 的推导如下：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/bA83gebG5G.png?imageslim)

对上面的推导进行解释：

最后一步是因为ni/N 是频率，由大数定理，频率等于概率，因此当N无穷大的额时候，

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/EIlFI62k48.png?imageslim)就是熵。

所以当N无穷大到无穷的时候：


## 自封闭系统的运动总是倒向均匀分布


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/K657Jl8k9C.png?imageslim)


## 下面我们继续思考：根据函数形式判断概率分布

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/IhkC6g6ege.png?imageslim)

也就是说，正太分布的概率密度函数的对数形式是一个二次函数形式的。

那么反过来，如果一个分布的对数可以写成一个二次形式，那么这个分布必然是一个正态分布。

比如我们再看一下Gamma分布：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/AcB78cg8dB.png?imageslim)

这个地方提一下：二项分布的共轭分布就是Gamma分布，从2到n就是多形式分布，从Gamme分布到N就是directlit分布。


## 之前我们知道，均匀分布熵是最大的，那么给定条件呢？

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kDAdf8hG3H.png?imageslim)

利害，这怎么求？

过程如下：

首先，我们先建立一个目标函数：我们不知道 p(x) 是什么样子的，我们想要做的就是将 H(x) 取极大，然后求出这个时候的 p(x)。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/mDK7f1c8ae.png?imageslim)

使用方差公式化简约束条件：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/dHC61mCD1e.png?imageslim)

也就是说，我们现在知道了 x 的期望和 x^2 的期望。那么显然，这是一个带约束的极值问题，可以用Lagrange乘子法：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/fCBja0CHGc.png?imageslim)

求偏导得：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/3FaCF9cC22.png?imageslim)

可见 p(x) 的对数形式是一个关于x得平方的。根据我们刚才的引理，这样的分布一定是一个正态分布。** 利害的。在看一下 之前相关的引理。**



OK，上面都是一个随机变量的问题，现在准备看下多个随机变量之间的关系：




# 联合熵和条件熵




## 定义


联合熵 H(X,Y)：

两个随机变量 X，Y 的联合分布，可以形成联合熵  Joint Entropy。注意：H(X,Y) 与 H(XY) 这两种写法是不加以区别的。

条件熵 H(X|Y)=H(X,Y) – H(Y)

即 (X,Y) 发生所包含的熵，减去 Y 单独发生包含的熵就是在 Y 发生的前提下，X 发生 “新” 带来的熵。


## 推导一下条件熵的定义式




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/KkHLg1ke09.png?imageslim)

解释一下：上面的对于 \(p(y)\) 写成 \(\sum_{x}^{ }p(x,y)\)的形式，是因为边缘分布等于各自的联合分布的和。

OK，上面已经得到了条件熵的定义式，继续推导：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0jGElHdF5g.png?imageslim)

解释一下：导数第二部的括号中的意思是：x取某一个值的时候p(y)的条件分布的熵。因此可以写成最后的形式。


## OK 我们现在可以提出相对熵的概念：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/GCd7ifBc35.png?imageslim)

显然，上面的式子中的求和可以看作是在p(x)的分布下求这个式子的期望

我们对上面的相对熵的式子进行理解：如果某一个真实分布p(x)，我们没有用真实分布来求信息熵，我们用q(x)来求信息熵，所以最后是这个样子\(\sum p(x)ln(q(x))\) 而这个与我们实际的\(\sum p(x)ln(p(x))\) 二者之间的差就是上面说的相对熵。利害。

相对熵还是很重要的：相对熵可以度量两个随机变量的 “距离” 。

由凸函数中的 Jensen 不等式可以得：D(p||q)≥0、 D(q||p) ≥0 。

在 “贝叶斯网络” 、 “变分推导” 等章节会再次遇到相对熵。**遇到之后回来补充下**

相对熵比较麻烦的地方就是，这两个不是对称的定义，一般的，D(p||q) ≠D(q||p)，既然不是对称的情况，哪我们在实践中用谁相对谁呢？


## 在实践中用谁相对谁呢？




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kD7k5Efegl.png?imageslim)

注意，P是存在的未知的一个东西，Q是假定的算出的简单的一个东西。所以两个KL散度还是不大一样的。


## 两个 KL 散度的区别

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Km4eHCKA2L.png?imageslim)

如果KL=0，当且仅当p=q才可以。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/JgdDDGAK21.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/JkL7l4BiLc.png?imageslim)



OK，我们开始介绍互信息

# 互信息

## 互信息得定义

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6D2ff2ecjd.png?imageslim)

如果 X，Y 是独立的，那么 p(x,y) 就等于p(x)p(y)，因此相等的时候互信息就是0，因此互信息表示的是二者之间的相关性的一种度量。

同样，我们也可以推出来条件熵：


## 计算条件熵的定义式：\(H(X)-I(X,Y)\)




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/33926I9dHE.png?imageslim)

根据条件熵的定义式，可以得到


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/64ebJag228.png?imageslim)

所以条件熵可以写成这两种形式：




  * H(X|Y) = H(X,Y) - H(Y)   条件熵定义


  * H(X|Y) = H(X) - I(X,Y)   根据互信息定义展开得到


同样，可以衍生出：

* 对偶式
    * H(Y|X)= H(X,Y) - H(X)
    * H(Y|X)= H(Y) - I(X,Y)
* I(X,Y)= H(X) + H(Y) - H(X,Y)
* H(X|Y) ≤H(X) ， H(Y|X) ≤H(Y)


最后一个不等式 H(X|Y) ≤H(X) ， H(Y|X) ≤H(Y)  意味着：随着给定的条件越来越多，不确定性越来越少，最终走向确定性。

Venn 图可以说明上述的这些公式：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/hb67lckbel.png?imageslim)

## 一道思考题：天平与假币


有13枚硬币，其中有1枚是假币，但不知道是重还是轻。现给定一架没有砝码的天平，问至少需要多少次称量才能找到这枚假币？

答：3次。如何称量？如何证明？








# 信息增益：


概念：当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。

信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。

定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：g(D,A)=H(D) – H(D|A)    显然，这即为训练数据集D和特征A的互信息。


## 基本记号：




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/h98cF0kC92.png?imageslim)

## 信息增益的计算方法


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kcjm3IL1jJ.png?imageslim)

## 经验条件熵H(D|A)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/7L5J8FF1HI.png?imageslim)


## 其他目标

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/DmDlA7G1gL.png?imageslim)


## 关于基尼指数的讨论(一家之言)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Jhd7gh39Hh.png?imageslim)



# COMMENT：


**信息增益和基尼系数要不要单独拿出来？**互信息要不要单独拆开来？****

**看看还有什么需要补充的？**

**并且，信息论里面还有什么需要掌握的？**
