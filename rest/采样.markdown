
# REF
  1. 七月学院 机器学习
  2. [机器学习中的采样(sampling)方法是要解决什么类型的问题？](https://www.zhihu.com/question/40943513)
  3. [贝叶斯推理-采样与变分简介](https://wenku.baidu.com/view/41f1b295ac51f01dc281e53a580216fc700a53b2.html?re=view)
  4. [LDA变分法和采样法](https://blog.csdn.net/deltaququ/article/details/45892063)



# TODO










  * **采样方法，比如MCMC采样，Gibbs采样，拒绝采样等，到底是为了解决什么类型的问题？ **


  * **这一章与LDA有关系吗？是Gibbs采样有关系吗？**


  * **为什么MCMC采样没有放在这一章里？**




# MOTIVE


对采样进行总结。

上次讲的LDA为什么可以通过采样的方式来进行收敛来进行交代。

********************************************************************************









OK 复习一下上次的：


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3045e9ec09.png)


Beta分布是二项分布的共轭先验分布，它的概率密度的定义是：![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae304640983c.png)这个是归一化因子

而这个B可以表示称一个Gamma函数的形式：![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae30467a2c7b.png)

厉害了，没想到Beta分布还与Gamma函数有关。

如果给定\alpha 和\beta 那么，这个x的期望是多少？ xf(x)


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3046c83339.png)


厉害了，没想到这个Beta分布的期望可以写成这样。





上面的Beta分布拿到主题模型那一张去












# 怎么在圆内随机取点？




## 一个简单的采样问题


给定区间\([a_x,b_x]×[a_y,b_y]\)，使得二维随机点(x,y)落在等概率落在区间的某个点上。

分析：因为两个维度是独立的，分别生成两个随机数即可。

产生二维随机数代码与效果


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abded207e3c4.png)





## 那么圆内均匀取点可以怎么做？


给定定点\(O(x_0 ,y_0)\)和半径r，使得二维随机点(x,y)等概率落在圆内。

分析：直接使用x=x_0 +r*cosθ，y=y_0 +r*sinθ是否可以呢？具体试验一下。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abded63887f5.png)





## 还有什么办法呢？


显然上述做法是不对的。但可以使用二维随机点的做法，若落在圆外，则重新生成点。即有选择的取点，结果如下：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abded9802dc9.png)


这个就是一种带拒绝的采样分析。

**嗯不错的方法。**


## 附：产生圆内随机数的其他方法




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdedd908185.png)


关键是这个： sqrt(rand2500())






# 带拒绝的采样分析




## 什么是拒绝采样？


Rejection sampling

在对某区域f(x,y)≤0抽样的过程中，若该区域f(x,y)≤0不容易直接求解，则寻找某容易采样的区域g(x,y)≤0 ， G为F的上界。当采样(x 0 ,y 0 )∈G且落在F内部时，接收该采样；否则拒绝之。

![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdedc31d583.png)

该例中， f(x,y)≤0是圆， g(x,y)≤0是该圆的外包围正方形。

注：区域f(x,y)≤0的可行解集合记做F；区域g(x,y)≤0的可行解集合记做G；显然F⊆G

MCMC是沿着这个思路走的。


## 进一步思考拒绝采样


上述方法能够一定程度的估算圆周率，老师试过，大概是千万次才能得到3.14。虽然精度很差，但是思路还是很重要的。

OK，现在我们想一下：上述抽样问题能否用来解决一般概率分布函数的抽样问题？如：根据均匀分布函数得到正态分布的抽样？

![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee15758c0.png)

比如说我们知道q(z)这种分布，然后乘以一个k之后kq(z)就比我的\(\widetilde{p}(z)\) 要大，那么我随机取一个点，在红色的时候就要，在黑色的时候就不要，这样我就可以把任何一种概率分布都去做采样了。

那么怎么选q(z)呢？最简单的一种做法就是找到\(\widetilde{p}(z)\) 的极大值然后画一条横线，然后说我选的q(z)是一个均匀分布。所以这个总是存在的。

但是这个就太粗糙了，因为拒绝率太高，所以可以根据实际情况。选外包的分布。




## 对某概率分布函数进行采样的意义


可以根据抽样结果估算该分布函数的参数，从而完成参数的学习。




  * 前提：系统已经存在，但参数未知；


  * 方法：通过采样的方式，获得一定数量的样本，从而学习该系统的参数。


举个例子，比如投硬币试验中，进行N次试验，n次朝上，N-n次朝下，可以认为，是进行了N次(独立)抽样。

假定朝上的概率为p，使用对数似然函数作为目标函数：


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae308cdac442.png)





## 应用Bernoulli版本的大数定理




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee3996a27.png)


附：Bernoulli版本的大数定理

**这个要拿到大数定理那里。**


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee44f12b9.png)




现在我们想一下，正常的EM中：


## 用采样改造EM算法本身




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee50e9e7c.png)




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3099ff0acc.png)这个是能算出来的，我们可以按照这个概率去采样![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae309a31f8c9.png)这个函数，假设采样了L次，我们用这L次的采样的期望值来近似真正的值


这种就是MC-EM算法。

上面是采样L次，那么最极端的情况就是采样一次。如果L次可以看作一个批处理方式的梯度下降的话，采样一次就是随机梯度下降。也就是说采样一次 是随机EM算法

而且这里用的是用MLE来推的EM算法。

而昨天说过有MAP的方式，极大后验概率，那么可以得到MAP版本的EM算法，即贝叶斯版本下的EM算法



**还是没明白这个拒绝采样是怎么用在EM算法上的？**



OK，在聊一下马尔科夫链这个问题：

如果你有一个概率分布p(x) 那么你怎么能够在计算机中生成一些样本出来呢？

它的方式就是马尔科夫链。





## 重述采样


采样：给定概率分布p(x)，如何在计算机中生成它的若干样本？

方法：马尔科夫链模型

考虑某随机过程π，它的状态有n个，用1~n表示。记在当前时刻t时位于i状态，它在t+1时刻位于j状态的概率为P(i,j)=P(j|i)：即状态转移的概率只依赖于前一个状态。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee6a04ab6.png)





## 举例


假定按照经济状况将人群分成上、中、下三个阶层，用1、2、3表示。假定当前处于某阶层只和上一代有关，即：考察父代为第i阶层，则子代为第j阶层的概率。假定为如下转移概率矩阵：

![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee7dc016e.png)


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee8786556.png)


概率转移矩阵


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdee96155e4.png)


不同初始概率时候的迭代结果：


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae30ae93238a.png)




相当于这个稳定状态的分布与原始状态没有关系，与状态转移矩阵有关系。





## 马尔科夫随机过程的平稳分布




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdeeb1d6448.png)


马尔科夫随机过程的平稳分布


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdeebd6bf2d.png)


也就是和说：如果状态转移矩阵确定了，求n次幂，归一到一个状态上去了，那么这个分布就是一个稳定分布





![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae30b1f38243.png)这个x是一个行向量，这个非负解也就是P的特征值为1 的特征向量。








## 马尔科夫随机过程与采样


上述平稳分布的马尔科夫随机过程对采样带来很大的启发：对于某概率分布π ，生成一个能够收敛到概率分布π的马尔科夫状态转移矩阵P，则经过有限次迭代，一定可以得到概率分布π。

该方法可使用Monte Carlo模拟来完成，称之为MCMC(Markov Chain Monte Carlo)。


## 细致平稳条件




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdeed984fd6.png)


P(i,j)是从i号状态转移到j号状态的概率，它去乘以当前位于第i号状态的概率。

而这个等式意味着我从i状态转移除去的概率等于转移进i状态的概率，而 \(\forall i,j\)  就是任何两个都满足这个，那么这个一出一进就得到稳态了。

细致平稳条件（detailed balance condition）

任何的两个状态都是平稳的。



刚才是想象的大概是这样，那么下面推一下：

细致平稳条件和平稳分布的关系


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdeee3f28e9.png)




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae31fdf60f3d.png)这个是讲![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae31fe99861c.png)用细致平稳条件替换。而这个就是![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae31fedda037.png)的离散化版本。


也就是说，如果满足细致平稳条件，那么最终满足P收敛到\pi 这个分布上去了。

所以，细致平稳条件真的是一个条件。





现在我们来想象下：

设定接受率


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef00cd426.png)


利害，还可以把不满足改造成满足这个相等条件的。

厉害了![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3200354290.png)，这个的确是最简单的情况。

使用\alpha 改造对应的状态转移矩阵。嗯。改了之后，就能向细致平稳条件去接近。

**到底怎么实现的？而且满足了之后\alpha 怎么处理？为什么改造的\alpha 是接受率？**



现在我们再来想一下：

MCMC：Metropolis-Hastings算法


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef0c44267.png)


我让\(a(i,j)\)，\(a(j,i)\) 这两个值，这个\alpha 是接受率，这个接受率肯定是小于1的一个数。所以我的\(a(i,j)\)就取![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3203749257.png)，而\(a(j,i)\) 就对应的就行？这个\(a(j,i)\) 是恒为1 还是什么？因为![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae3204835f0d.png)中的\(a(i,j)\)越小，比如之前的圆内均匀采样的时候，只有78.5%的概率成功。



Metropolis  在模拟退火算法也是它的名字。**是这样吗？什么是模拟退火算法？**

Metropolis-Hastings算法


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef166e6d7.png)


最终就能让这个马尔科夫过程得到我们想要的稳态

改造MCMC算法


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef20a185a.png)


如果把1看作当前的维度，2 看作其它的维度，就得到了![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae320b0a0bd7.png)


## 二维Gibbs采样算法




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef2a96bf0.png)


得到新的值之后，采样得到其它的值。

x得到y，y得到x。**什么意思？**

x_t固定的时候，算y的概率，y_{t+1}固定的时候，算x的概率

将二维Gibbs采样推广到高维


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdef366a083.png)


可见，为什么要讲这个采样呢？因为它可以解释Gibbs采样



其实这个有点像EM，也有点像SMO，也有点像坐标上升。

**没有明白？基本没明白？**

变分没有讲。重点是变分推导/似然下界变分推导和今天讲的Gibbs采样也有异曲同工之妙






# COMMENT：


**采样是在什么时候使用呢？**
