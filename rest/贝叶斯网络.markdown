---
author: evo
comments: true
date: 2018-03-30 03:31:17+00:00
layout: post
link: http://106.15.37.116/2018/03/30/bayesian-network/
slug: bayesian-network
title: 贝叶斯网络
wordpress_id: 1612
categories:
- 随想与反思
tags:
- '@todo'
- '@want_to_know'
---

<!-- more -->

[mathjax]


# REF





 	
  1. 七月在线 机器学习


********************************************************************************


# TODO





 	
  * **需要整理下，还差很多，还需要仔细理解。**




# MOTIVE


总是看到贝叶斯网络和贝叶斯什么的，想总结下，和隐马尔科夫模型 HMM 到底是什么，详细是怎么实现的，理论是什么？比如 前向-后向算法，Viterbi算法，EM算法等等

HMM可以做文本的自然语言处理，也可以做语音的自然语言处理

贝叶斯网络很重要的一个就是HMM模型。

对贝叶斯和贝叶斯网络进行总结：

********************************************************************************




# 知识基础





 	
  * 概率

 	
  * 相对熵、互信息（信息增益）





# 主要内容


这个实际上是机器学习后半部分的学习的基础内容。后面都是围绕贝叶斯网络展开的论述，比如主题模型的LDA和HMM，都是关于贝叶斯网络的。因此本次的相关理论和概念是比较重点的。



 	
  * 朴素贝叶斯

 	
  * 贝叶斯网络的表达

 	
    * 条件概率表参数个数分析

 	
    * 马尔科夫模型




 	
  * D-separation

 	
    * 条件独立的三种类型

 	
    * Markov Blanket




 	
  * 网络的构建流程

 	
    * 混合(离散+连续)网络：线性高斯模型




 	
  * Chow-Liu算法：最大权生成树MSWT






OK，上面就是朴素贝叶斯的相关的东西，下面开始介绍贝叶斯网络


# 贝叶斯网络




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb0ff0e5fc.png)


因为出现环 就不好说因果。如果是完全独立，就是一些离散点，就不需要进行研究，因为朴素贝叶斯就是完全独立的。

贝叶斯网络谈的都是有向图模型。如果出现无向图模型，我们谈的就是马尔科夫网络。

如果说重要度的话，实践中很多时候用的就是贝叶斯网络。

一般而言，贝叶斯网络的有向无环图中的节点表示随机变量，它们可以是可观察到的变量，或隐变量、未知参数等。连接两个节点的箭头代表此两个随机变量是具有因果关系(或非条件独立)。若两个节点间以一个单箭头连接在一起，表示其中一个节点是“因(parents)”，另一个是“果(children)”，两节点就会产生一个条件概率值。

每个结点在给定其直接前驱时，条件独立于其非后继。**这句****没明白？**




## 举个例子：一个简单的贝叶斯网络




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb129ba27b.png)


如果条件a的时候能够推出条件b，并且a，b能够推出c。

如果正常而言取计算a,b,c的联合概率密度，那么肯定是可以求出来的。但是仍然是可以画成一个贝叶斯网络的。虽然是一个简单的贝叶斯网络。

如果我把三个变量变成k个呢？


## 全连接贝叶斯网络


每一对结点之间都有边连接


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb13737cda.png)


这个就是全连接。


## 正常而言有些边是缺失的




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1b6a42e04f.png)


看到这个图，直观上：



 	
  * 既然1和2的入度为0，那么我们认为1和2 是独立的，

 	
  * 如果4给定的时候，6和7是独立的，也就是说6和7在4给定下是条件独立的。** 为什么4给定的收，6和7是独立的？**


因此我们可以想象到   x1,x2,…x7 的联合分布为：


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1b680c4de8.png)


也就是说，要想求它们的联合分布，只需要求出这7个节点各自所属的条件分布的连乘积即可。

联合分布的本质是什么？如果我们能够拿到手任何n个节点的联合分布的话，其实这n个节点的分布状况都是可以计算的。

比如：如果给定了x_1,x_2的联合分布，怎么算x_1？求积分，把x_2积掉就行。所以说，如果把握了联合分布，那么这7中节点的各种分布都OK。所以联合分布是最重要的。**厉害啊。**



OK，我们再看一下上面的 x1,x2,…x7 的联合分布，当它写成那样的时候，背后发生了什么呢？


## 分析一个实际的贝叶斯网络




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb162543e4.png)


通过一些先验的知识来知道它们之间是否发生关系。

举个例子，如果我取计算呼吸困难的出现的概率，那么给定C和B的时候，4中情况之下，D发生的概率都是一个二项分布。

右下角的表格每一行都是一个条件下的二项分布，每一行都是一个条件概率下的表格，也就是一个条件概率表。

所以对于任何一个结点想去描述它的分布情况，本质上就是给定这个结点对应的条件概率表。

所以每一个结点都是一个条件概率表就能描述这么一个图。

现在我们要想描述抽烟这个结点，只需要0.4这一个参数就可以。而对于lung Cancer 这个就需要两个参数来决定它，一个是抽烟的时候的二项分布，一个是不抽烟的时候的二项分布。以此类推。整体上比每个都取两种情况的时候要少很多，即\(1+2+2+4+4=13 vs 2^5\)  。**嗯，的确**

所以说实际的贝叶斯网络省略的边越多越好，因为假如说从Smoking到Dysponea也有一条边，那么Dyspnoea就有8中情况，就是8个参数。参数更多。**能去可以省略吗？怎么省略？**


## 




## 再举一个例子：警报




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb1b6c457f.png)


下面给出一个非常重要的内容：

由于每一个概率都可以通过条件概率表查到，因此全部随机变量的联合分布是可以求出来的：

比如算一下：JohnCall，MaryCall，Alarm，但是没有发生Burgary和Earthquake的概率。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb1c70b37f.png)


也就是说如果给了你每个结点的条件概率表，去计算联合概率就很方便。**嗯。**




## 那么下面我们就需要介绍一些理论上的严格说法：


贝叶斯网络的形式化定义


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb1d2e7ea0.png)





## 特殊的贝叶斯网络：


刚才我们的例子是看起来比较复杂的一种条件概率表，但是有一种简单的：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb1f323fe2.png)


这个就是著名的马尔科夫模型，比如说经常用到的伪随机数发生器。**什么是伪随机数发生器？为什么伪随机数发生器是用的马尔科夫模型？**

![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1b96ac1b2b.png)给定文档，给定文档之后主题的概率，给定主题之后词的概率。

马尔科夫网络是一个无向图模型，

但是马尔科夫模型是一个有向图模型。是一个当前结点只与之前一个结点有关系的贝叶斯网络





其实看到马尔科夫模型，我们会想，为什么这个网络的每一个只与它前一个有关呢？怎么知道的呢？OK，我们先看一下贝叶斯网络里面判断条件独立的几种条件：


# 贝叶斯网络里判断条件独立的几种网络拓扑


tail-to-tail:


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb1fcea6ae.png)


\(P(a,b|c)=P(a|c)*P(b|c)\)  即 c 都是条件，如果把c这个条件忽略掉，那么a和b就是独立的。也就是说，在 c给定的条件之下a和b是独立的。也就是说这是条件独立的。

我们把这样的一个基本的网络形态叫做tail-to-tail.

head-to-tail：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb20b8caf3.png)


在 c 这个条件给定的时候，a和b是条件独立的。

注意，如果c未给定，这个时候a和b不一定独立。嗯。

head-to-head：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb214128f9.png)


对左边和右边同时对 c积分。**为什么 \(P(c|a,b)\)  这个对c积分是1？**

**注意，如果c是已知的，那么就无法对c进行积分。**

如果c给定，a和b就不是独立的了。因为，如果我观测到了c的某种现象，就说明a和b就建立了联系，它们之间就不再是阻断的了。**但是c未知到底是什么意思？未知的话怎么出现这个网络拓扑的？**



所以，贝叶斯网络是一个非常重要的东西，只要通过这三种非常基础的网络拓扑，就能判断出来a和b是不是独立的。




## 举例说明这三种情况




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb21ec41ca.png)


对于Serial来说：在不知道什么肺结核信息的时候，一个人去亚洲和一个人x光有阴影是有关的。但是如果只去观测那些肺结核病人的时候，那么它的x光是否有阴影就于是否来亚洲没有关系了。

对于Diverging来说：如果一个人不知道他是否抽烟，但是他得了支气管炎，那么就会认为他有可能抽烟，那么他还有可能得肺癌。但是如果我知道某一个人抽烟，把所有抽烟的人群放在一起进行观测得话，那么有一部分人是支气管炎，有一部分人是肺癌，他们之间是独立得。在抽烟得人群内部这二者之间是独立的。**为什么是独立的？感觉这种描述还不是很清楚。**

对于Converging来说：如果一个人有支气管炎，那么他有可能呼吸困难。如果我们没有观测到呼吸困难的情况，他如果有支气管炎，那么他跟肺癌是没有关系的。**这样的推理很奇怪吧？ 比如说如果孩子是未知的，那么我们并不能够知道两个父节点是不是有关系，但是，如果孩子是已知的，俺么这两个父母必然是有关系的。**

再找一些自然语言说明这个的例子。

上面这三个拓扑是比较关键的东西。


## OK，现在讲上面的结论推广到结点集合





 	
  * D-separation：有向分离

 	
  * 对于任意的结点集A，B，C，考察所有通过A中任意结点到B中任意结点的路径，若要求A，B条件独
立，则需要所有的路径都被阻断(blocked)，即满足下列两个前提之一：



 	
  * A和B的“head-to-tail型”和“tail-to-tail型”路径都通过C；

 	
  * A和B的“head-to-head型”路径不通过C以及C的子孙；

 	
  * 如果A,B不满足D-separation，A,B有时被称为D-connected.


即虽然上面的a,b,c只是一个结点，但是可以把它看成一个结点集合。


## 举一个关于汽车发动机的例子




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb25d8d88d.png)


Gas和Radio是独立的吗？给定Battery呢？Ignition呢？Starts呢？Moves呢？(答：IIIDD)

什么都不知道：是独立的。因为可以吧左上角的三个看成一坨。

**为什么观测到Moves之后二者是不独立的呢？**

这种通过给定的贝叶斯网络去判断是否独立是非常重要的。




## 再次分析之前的马尔科夫模型




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb27a63b6d.png)





## HMM：




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb2896ff2c.png)


这个模型非常重要。

它可以做自然语言的翻译和自然语言的分词。

它本质上就是一个特殊的贝叶斯网络。


## 下面我们谈一下马尔科夫毯


Markov Blanket

一个结点的Markov Blanket是一个集合，在这个集合中的结点都给定的条件下，该结点
条件独立于其他所有结点。

即：一个结点的Markov Blanket是它的parents,children以及spouses(孩子的其他parent)

一个结点的马尔科夫毯就是它的父母，孩子和配偶。

没明白为什么集合中的结点都给定的条件下，该结点条件独立于其它的所有结点。

**关于马尔科夫毯还是不是很清楚。**


## 举个马尔科夫毯例子




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb2bab0698.png)


背景知识：Serum Calcium(血清钙浓度)高于2.75mmo1/L即为高钙血症。许多恶性肿瘤可并发高钙血症。恶性肿瘤病人离子钙增高的百分比大于总钙，也许可用于肿瘤的过筛试验。当高钙血症的原因难于确定时，必须考虑到恶性肿瘤的存在。http://www.wiki8.com/xueqinggai_131584/

阴影部分的结点集合，是Cancer的 “马尔科夫毯”  (Markov Blanket)

条件独立：P(S,L|C) = P(S|C) * P(L|C)


## 贝叶斯网络的用途




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb2e7b4082.png)






 	
  * 诊断：\(P(病因|症状)\)

 	
  * 预测：\(P(症状|病因)\)

 	
  * 分类：\(max_{class}P(类别|数据)\)


通过给定的样本数据，建立贝叶斯网络的拓扑结构和结点的条件概率分布参数。这往往需要借助先验知识和极大似然估计来完成。

在贝叶斯网络确定的结点拓扑结构和条件概率分布的前提下，可以使用该网络，对未知数据计算条件概率或后验概率，从而达到诊断、预测或者分类的目的。


## 例子：寻找马航MH370




已知MH370最后消失区域，可否根据雷达最后消失区域和洋流、大气等因素：



 	
  * 判断留尼汪岛是否位于可能区域？

 	
  * 残骸漂流到该岛屿的概率有多大？




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb377a62a5.png)


建模分析：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb38c7c71c.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3987ab62.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3a3bf0f0.png)





### 总结：


在每个时刻，物体的当前可能区域是上一时刻所有可能区域和相应转移概率的乘积和，这恰好是矩阵
乘法(矩阵和向量乘法)的定义。

当前可能区域只和上一个时刻的区域有关，而与更上一个时刻无关，因此，是马尔科夫模型。

思考：可以使用“ 漂流位置”建立马尔科夫模型，该可能位置是不可观察的，而将“ 转移位置”认为是“ 漂流位置”的转换结果，“ 转移位置”是残骸的最终真实位置，使用增强的隐马尔科夫模型。

不要过得累加模型的复杂度，适时使用奥卡姆剃刀(Occam‘s Razor)。该模型仅个人观点。


## 




# 贝叶斯网络的构建：




## 怎么通过样本构建贝叶斯网络呢？


如果你拿到手若干个样本，如何去建立贝叶斯网络呢？



 	
  * 如果说，有领域知识，那么首先考虑领域知识，

 	
  * 如果领域知识比较欠缺，那么根据样本实际观测来给定贝叶斯网络。


现在假定领域知识都用完了：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3dd318b8.png)


什么叫D-separation？

怎么计算 ![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1d87d2d627.png)？


## 贝叶斯网络的构建举例




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3e76b557.png)


数出来 MaryCalls的收JohnCalls的次数，数出来JohnCalls的次数，如果二者基本相等，就说明MaryCalls打电话于JohnCalls是独立的。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3eb141fb.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3f43f7c6.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb3f850e64.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb416a12e4.png)


这种是根据样本有关的，而且跟你结点选择的顺序是极大相关的。比如这里选择的顺序是MJABE

所以它很可能与实际的情况不同。

实际上实践中我们建立贝叶斯网络更多的是靠先验性的知识的。一定要重视领域知识。这个是提取特征，建立模型，做最初的目标函数的重要的依据，后面才能谈到如何对这个模型进行训练，进行预测。


## 混合(离散+连续)网络




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb4223d1bc.png)


丰收了 harvest  会决定我的价格，比如cost 大家都丰收，价格就下跌

有的时候有财政补贴 subsidy 也会决定我的价格

cost 决定我是否购买 buys

是否有补助，是否购买是布尔变量，但是收成和价格是连续变量。

所以这是一个混合网络。


### 孩子结点是连续的




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb43476475.png)





### 孩子结点是离散的，父节点是连续的




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb44b06bdb.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb45a46346.png)


上面这些都没有详细说。


# 贝叶斯网络的推导




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb465187fa.png)


如果我这里的条件概率表都给定了，如果呼吸困难发生了，那么他抽烟的概率是什么？


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdb479b8eae.png)




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1da132ac9e.png)这个变成一个累加的




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1da1558c19.png)这个还是能够理解的，**但是这个求和怎么算呢？，任何一个结点的概率值都是可以通过条件概率表查出来。**


而且变换成![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1da18057ef.png)**这样怎么就省事了？怎么就少做乘法了？**



本质上用的是动态规划。**什么是动态规划？为什么本质上是动态规划？**





最后谈一下无向环




# 无向环


可以发现，若贝叶斯网络中存在 “环” (无向)，则因此构造的因子图会得到环。而使用消息传递的思想，这个消息将无限传输下去，不利于概率计算。

解决方法：



 	
  * 删除贝叶斯网络中的若干条边，使得它不含有无向环

 	
  * 重新构造没有环的贝叶斯网络


原贝叶斯网络的近似树结构


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdbb7c82031.png)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdbb86ad520.png)


左边是通过真实的数据算出来的贝叶斯网络，如果我们吧有向的箭头忽略掉，就有一个环

我们能不能做一个近似，对这么一个5个结点的图就用4条边形成一个树形结构，使得这个树形结构与原始的是最接近的？

如何度量这两个分布之间的距离呢？可以用K-L散度。即相对熵。

然后有一个非常好的结论。叫做MSWT。这个算法本质上就是我们谈到的图模型里面的最小生成树，没有任何区别。**什么是最小生成树？**



将两图的相对熵转换成变量的互信息


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdbb8dc2b98.png)





### 最大权生成树MSWT的建立过程





 	
  1. 对于分布P(x)，对于所有的i≠j，计算联合分布P(xi|xj)；

 	
  2. 使用第1步得到的概率分布，计算任意两个结点的互信息I(Xi,Yj)，并把I(Xi,Yj)作为这两个结点连接边的权值；

 	
  3. 计算最大权生成树(Maximum-weight spanning tree)

 	
    1. 初始状态：n个变量(结点)，0条边

 	
    2. 插入最大权重的边

 	
    3. 找到下一个最大的边，并且加入到树中；要求加入后，没有环生成。否则，查找次大的边；

 	
    4. 重复上述过程c过程直到插入了n-1条边(树建立完成)




 	
  4. 选择任意结点作为根，从根到叶子标识边的方向；

 	
  5. 可以保证，这课树的近似联合概率P'(x)和原贝叶斯网络的联合概率P(x)的相对熵最小。


第3步本质上是最小生成树。

这个树就是最接近于原始图的近似的一个图模型。

**没有很理解。**

**二者这种什么时候会用到？只要生成的贝叶斯网络是存在无向环，就必须这样处理吗？**

判断是否有环怎么判断？拓扑排序。等。**有那些方法？**




# 附：Chow-Liu算法 没讲




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abdbbf2f0482.png)









# COMMENT：


**还是很多东西的，还是需要好好的认真学习下的。**


