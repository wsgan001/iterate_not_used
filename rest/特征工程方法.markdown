---
author: evo
comments: true
date: 2018-04-28 07:54:53+00:00
layout: post
link: http://106.15.37.116/2018/04/28/feature-engineering/
slug: feature-engineering
title: 特征工程方法
wordpress_id: 4446
categories:
- 随想与反思
---

<!-- more -->

[mathjax]


# REF





 	
  1. 


[数据挖掘中常见的特征工程方法](https://zhuanlan.zhihu.com/p/33072222)







# TODO





 	
  * **没有看，需要修改，要以书上为基准，博客的只能是辅助**




# MOTIVE





 	
  * aaa





* * *



这几天在做一个数据挖掘相关的东西，炼丹练久了，突然发现对特征工程这一块还存在比较大的空白。于是查阅了一些资料，权当记录阅读笔记。

特征工程在数据挖掘应用中直接影响模型最终的性能；尤其在很多计算机视觉任务中，特征提取的重要性甚至超过了分类器本身（比如CNN提取的feature是比很多hand-crafted features更具备表示能力的）。良好的特征（feature）应该与标签（label）高度相关，并且与其他特征不相关。

特征工程（feature engineering）包括**特征提取**和**特征选择**两个方面。

**特征提取**广义上指的是一种变换, 将处于高维空间的样本通过映射或变换的方式转换到低维空间, 达到降维的目的；

**特征选择**指从一组特征中去除冗余或不相关的特征来降维。

这里暂时只介绍特征选择部分！如有错误，欢迎指正！



* * *





## 主要思路


特征获取需要解决两个问题，

一是**确定选择算法**，在允许的时间内，以可以忍受的代价找出最小的、最能描述类别的特征组合；

二是**确定评价标准**, 衡量特征组合是否最优,得到特征获取操作的停止条件。 因此, 一般分两步进行特征获取，先产生特征子集，然后对子集进行评价，如果满足停止条件，则操作完毕，否则重复前述两步直到条件满足为止。

**按照特征评价标准分类：**



 	
  * 选择使分类器的错误概率最小的特征或者特征组合。

 	
  * 利用距离来度量样本之间相似度。

 	
  * 利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。

 	
  * 利用相关系数, 找出特征和类之间存在的相互关系；
利用特征之间的依赖关系, 来表示特征的冗余性加以去除。





* * *





## Search





 	
  * forward selection：以空集开始，逐渐向里面添加特征，直到满足stopping criterion（常用于高维数据挖掘场景）。

 	
  * backward elimination：包含所有特征并逐渐删除，直到满足stopping criterion。

 	
  * forward selection + backward elimination





* * *





## Evaluation





 	
  * Filters：不依赖于学习算法，对独立特征或特征子空间进行评估；

 	
    * Mutual Information(MI)：度量每个特征与标签的MI值，选取其中**Top N**个的特征。

 	
      * ![I(A,B)=\sum_{i}{\sum_{j}{Pr(a_i, b_j)log\frac{Pr(a_i, b_j)}{Pr(a_i)*Pr( b_j)}}}](https://www.zhihu.com/equation?tex=I%28A%2CB%29%3D%5Csum_%7Bi%7D%7B%5Csum_%7Bj%7D%7BPr%28a_i%2C+b_j%29log%5Cfrac%7BPr%28a_i%2C+b_j%29%7D%7BPr%28a_i%29%2APr%28+b_j%29%7D%7D%7D)




 	
    * Chi-Square：被用于测试两个相互独立的事件A，B的偏差程度。

 	
      * ![\chi^2(t,c)=\frac{N\times(AD-CB)^2}{(A+C)(B+D)(A+B)+(C+D)}](https://www.zhihu.com/equation?tex=%5Cchi%5E2%28t%2Cc%29%3D%5Cfrac%7BN%5Ctimes%28AD-CB%29%5E2%7D%7B%28A%2BC%29%28B%2BD%29%28A%2BB%29%2B%28C%2BD%29%7D)

 	
      * 式中，N 表示训练语料中的文档总数；c 为某一特定类别; t 表示特定的词条； A 表示属于 c 类且包含 t 的文档频数； B 表示不属于 c 类但是包含 t 的文档频数；C 表示属于 c 类但是不包含 t 的文档频数; D 是既不于 c 也不包含 t 的文档频数。

 	
      * 若chi-square足够小，就认为两者是独立的；若chi-square大到一定程度，就认为两者是相关的。




 	
    * Pearson Correlation Coefficients

 	
    * Euclidean Distance

 	
    * T-Test：评估两组样本均数之间的差异程度。

 	
      * ![T=\frac{\bar{X}-\bar{Y}}{\sqrt{\frac{S_1^2}{n_1}+\frac{S_s^2}{n_2}}}](https://www.zhihu.com/equation?tex=T%3D%5Cfrac%7B%5Cbar%7BX%7D-%5Cbar%7BY%7D%7D%7B%5Csqrt%7B%5Cfrac%7BS_1%5E2%7D%7Bn_1%7D%2B%5Cfrac%7BS_s%5E2%7D%7Bn_2%7D%7D%7D)




 	
    * Laplacian Score(LS)：LS通常用于unsupervised learning如聚类中，它假定相互关联的特征在整个特征空间中，其临近记录与该特征向量也很接近。
在Label存在的supervised learning任务，可使用Fisher Criterion Score (FCS)。

 	
    * Information Gain：考察某特征 t 出现与不出现的时候两者差值，即为该特征给系统带来的信息量；差值越大，说明该特征越重要。

 	
      * ![IG(t)=-\sum_{i=1}^m{P(c_i)log{P(c_i)}}+P(t)\sum_{i=1}^m{P(c_i|t)log{P(c_i|t)}}+P(\bar{t})\sum_{i=1}^m{P(c_i|\bar{t})log{P(c_i|\bar{t})}}](https://www.zhihu.com/equation?tex=IG%28t%29%3D-%5Csum_%7Bi%3D1%7D%5Em%7BP%28c_i%29log%7BP%28c_i%29%7D%7D%2BP%28t%29%5Csum_%7Bi%3D1%7D%5Em%7BP%28c_i%7Ct%29log%7BP%28c_i%7Ct%29%7D%7D%2BP%28%5Cbar%7Bt%7D%29%5Csum_%7Bi%3D1%7D%5Em%7BP%28c_i%7C%5Cbar%7Bt%7D%29log%7BP%28c_i%7C%5Cbar%7Bt%7D%29%7D%7D)

 	
      * ![P(c_i)](https://www.zhihu.com/equation?tex=P%28c_i%29) 表示 ![c_i](https://www.zhihu.com/equation?tex=c_i) 类文档在语料中出现的概率； ![P(t)](https://www.zhihu.com/equation?tex=P%28t%29) 表示语料中包含词条 _t_ 的文档的概率； ![P(c_i|t)](https://www.zhihu.com/equation?tex=P%28c_i%7Ct%29) 表示文档包含词条 _t _时属于 ![c_i](https://www.zhihu.com/equation?tex=c_i) 类的条件概率； ![P(\bar{t})](https://www.zhihu.com/equation?tex=P%28%5Cbar%7Bt%7D%29) 表示语料中不包含词条 _t _的文档的概率； ![P(c_i|\bar{t})](https://www.zhihu.com/equation?tex=P%28c_i%7C%5Cbar%7Bt%7D%29) 表示文档不包含词条 _t_ 时属于 ![c_i](https://www.zhihu.com/equation?tex=c_i) 的条件概率; _m _表示类别数。







 	
  * Wrappers：利用学习算法来评估特征子空间，如Classifier的准确率。优点是Wrappers通常比Filters更准确；缺点是计算量较大。

 	
  * Embedded： 特征选择算法是作为学习算法的部分嵌入其中的，特征选择和训练过程同时进行。常见的有Decision Tree和 Deep Neural Networks。
























* * *





# COMMENT



