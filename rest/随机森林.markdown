---
author: evo
comments: true
date: 2018-04-25 23:51:29+00:00
layout: post
link: http://106.15.37.116/2018/04/26/random-forests/
slug: random-forests
title: 随机森林
wordpress_id: 3876
categories:
- 随想与反思
tags:
- '@todo'
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL





 	
  * 七月在线 机器学习




# TODO





 	
  * a




# MOTIVE





 	
  * 对随机森林进行总结





* * *






# 随机森林算法介绍




## 什么是随机森林算法？


利用多棵树对样本进行训练并预测的一种算法。


## 随机森林与决策树之间的关系与比较


随机森林算法是在决策树的基础上发展出来的。

决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。随机森林就相当于很多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。

对于决策树算法来说，实际上单棵树的泛化能力还是很弱的，而随机森林的泛化能力就比较强了。**为什么？**


## 随机森林在现实中的使用情况


在实际的应用中，随机森林用的非常多，基本上在Kaggle的比赛里面是在首选方法的列表里面。**Kaggle的比赛中有那些首选方法？现在好像lightGBM比较多了。**






# OK，如何构造随机森林？


首先，我们通过两个随机性，来构造不同的次优树：



 	
  * 数据的随机性：我们采样的时候使用的是有放回的随机采样，而且我们只采样60%或者80%，因此这样的话样本是有可能被重复选到的。

 	
  * 特征的随机性：我们随机选择一部分特征来构建决策树，而且并不对树进行裁剪，也就是说这个时候构造的随机树，所使用的特征并不一定是全局最优的。


注：单树采用 CART 树。**什么是CART树？**

通过这两种随机性构造出的次优树，基本上是彼此不同的，这样就提升了系统的多样性，从而可以提升分类性能。**为什么呢？为什么提升了多样性就可以提升分类性能？**

在生成一定数目的次优树之后，森林的输出采用简单多数投票法（针对分类）或单棵树输出结果的简单平均（针对回归）得到。




# 什么是好的随机森林？




## 分类强度和相关度





 	
  * 森林中单棵树的分类强度（Strength）：每棵树的分类强度越大，则随机森林的分类性能越好。

 	
  * 森林中树之间的相关度（Correlation）：树之间的相关度越大，则随机森林的分类型能越差。


也就是说，单棵树的分类强度越大，树之间的相关度越小，则效果越好。**但是这个是怎么实现的？单树足够强大可以是树的深度允许更深。还有吗？**

可见：树不是越多越好的，由于树的个数越多，树之间的相关度也就会越大，所以，森林的分类性反而会降低。 其实，我们可以自己实现看看不同的树在测试集中的分类正确率的曲线。


## OOB错误率


OOB错误率是随机森林的错误率无偏估计

首先，由于树是由可放回的样本构造出来的，那么肯定有些样本不参数与这个树的构建，这些样本相对于这个树来说就是OOB样本，那么我每一个样本，都可以找到对应的不参与构建的那些树，每个样本在这些树中的出错的比值叫做OOB错误率。即：对每个样本，在其所有OOB的单树中错误占比，作为OOB错误率。

因此每个样本的OOB错误率求平均值就是随机森林的错误率的无偏估计 。由于是无偏估计了，所以，随机森林不需要进行交叉验证。**什么是交叉验证？**

实际上如果样本足够多的时候OOB错误率大概就等于产品上线之后的错误率。当然，前提是我的样本足够多。

**这个没明白**


# 随机森林的作用的扩展





 	
  * 根据样本在相同结点出现的比率，判别样本的相似性。比如说我的两个样本在不同的树中总是分到同一个类别下面，这说明我的这两个样本是很相似的。

 	
  * 根据分割特征选择的次数，可以给出特征重要性排序。即如果某个特征经常被用来作为分割，那么这个特征就是很重要的。**这个没有很清楚。要怎么统计呢？**





# 随机森林的优缺点




## 优点：


继承了树的一些优点



 	
  * 数据适应性好，对于连续值和离散值都可以很好分类，而且缺失特征对它影响很小

 	
  * 分类计算快速

 	
  * 能应对一定unblance的数据


增加的一些优点：

 	
  * 提高了泛化能力

 	
  * 随机森林中的树是可以并行生成的，因此可以用分布式的框架同时生成很多的树，再组合再一起。**有相关的实现吗，想看看怎么实现的。怎么分到分布式框架里面的。**




## 缺点：





 	
  * 


随机森林增加树的话还是要重来的重新训练，因为再原来的基础上改造还是比较困难的。**现在还是这样吗？有什么好的对应的方法？**






# 随机森林的算法特点


优点：



 	
  * 几乎不需要输入准备

 	
  * 可实现隐式特征选择

 	
  * 训练速度非常快、其他模型很难超越  **真的吗？**

 	
  * 很难建立一个糟糕的随机森林模型  **这个也是优点，好吧**

 	
  * 有大量优秀、免费以及开源的实现。


缺点：

 	
  * 模型大小、是个很难去解释的黑盒子


适用数据范围：

 	
  * 数值型和标称型







# COMMENT：


**最典型的机器学习算法，所有的，还是要自己手动写一遍，这样才会有代码级的理解。最核心的元素才能够体会到。**



**老师的例子用的是js写的，最好自己用Python写一遍。**
