---
author: evo
comments: true
date: 2018-04-26 00:01:35+00:00
layout: post
link: http://106.15.37.116/2018/04/26/ensemble-learning/
slug: ensemble-learning
title: 集成学习算法
wordpress_id: 3885
categories:
- 随想与反思
tags:
- '@NULL'
- '@todo'
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL





 	
  * 七月在线 机器学习




# TODO





 	
  * a




# MOTIVE





 	
  * **对集成学习算法进行总结，尤其是Xgboost Adaboost 随机森林。**

 	
  * **随机森林能不能与HMM融合在一起？**





* * *





# 







# 什么是集成学习算法？


集成学习算法（ensemble learning），集成学习算法一般来说是必用的。**是这样吗？**






# 有那些集成学习算法？


集成学习算法主要分为两种：



 	
  * Bagging   最流行的是随机森林（Random Forest）是Bagging方法的典型

 	
  * Boosting  当前最流行的版本是AdaBoost，人脸识别里面最经典的那个就是使用的AdaBoost。还有GBDT（也就是Gradient Boosting）


OK，我们以选男友为例：

 	
  * Bagging ：美女选择择偶对象的时候，会先问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友

 	
  * Boosting ：3个帅哥追同一个美女，第1个帅哥失败->(传授经验：姓名、家庭情况) 第2个帅哥失败->(传授经验：兴趣爱好、性格特点) 第3个帅哥成功


也就是说：

Boosting 是把一些表现效果一般（可能仅仅优于随机猜测）的模型通过特定方法进行组合来获得一个表现效果较好的模型。从抽象的角度来看，Boosting算法是借助 convex loss function 在函数空间进行梯度下降的一类算法。**没明白，什么是借助convec loss function 在函数空间进行梯度下降？**


# bagging 和 boosting 区别是什么？





 	
  1. bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。

 	
  2. bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。

 	
  3. bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。





# Bagging的思路：


实际上Bagging的思路可以用在别的地方，比如说，不同类型的分类器的组合

比如说我用随机重复采样的2/3的样本构造100个SVM，然后用这100个分类器去投票，但是实际上这个并不一定能得到很好的结果，为什么呢？因为随机森林有个优势：树之间的相关度会没有那么相关。而如果用SVM来做，那么相关度就比较强，即使你用的是2/3的随机样本。**为什么随机森林的树之间没有那么相关？。。说 树会很好的overfit我的数据，而我的数据每次都是不一样的，所以踩没有那么相关，没想到overfit也是一个优点。再确认下。**

但是，如果你用集成算法集成不同种类的分类器，那么不同的分类器之间相关度就会比较弱，比如一个LR和一个SVM和一个naive bayes。三个方法来投票，效果就会很好






# 集成学习算法思路




## 举个例子


假如有T个朋友，每个人拥有一个预测A股的函数 gi(x)，如何更好的预测股价升降？



 	
  * validation：选择一个炒股成绩最好的gi(x)

 	
  * uniformly：平均每个人预测结果，投票的方式

 	
  * non-uniformly：加权求和每个人的结果，某些人的权重更大

 	
  * canditionally：有条件的加权求和每个的结果，满足条件下某些人的权重更大。


**什么叫有条件的加权求和每个的结果？**





## 分析一下不同的方式


存在函数 g1,g2,...,gT，每个gi(x)函数表示股价升降预测

validation：选择一个炒股成绩最好的gi(x)


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda670e1be2.png)


uniformly：平均每个人预测结果，投票的方式


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda67e83610.png)


non-uniformly：加权求和每个人的结果，某些人的权重更大


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda6867d732.png)


canditionally：有条件的加权求和每个的结果，满足条件下某些人的权重更大


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda68e8264e.png)


最后一种是很灵活的，因为模型变得很复杂，因此用的非常很少。主要的就是 uniformly 和 non-uniformly 这两种，今天讨论的主要就是这两种方法。


## uniformly 的，比如 Uniform blending


**到底 uniformly 与 Uniform blending 是什么关系？属于其中的一种？**

利用函数的多样化，实现G(x)比单个g(x)好

分类应用   ![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda6cd2c349.png)

回归应用  ![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda6d6c1e3e.png)

这种直接的线性，虽然不一定能提高准确度，但是能够提升稳定性，降低 variance。即在训练集上是什么样的成绩在验证机上也是什么样的成绩。


### non-uniformly 的，比如 Linear blending


**到底 non-uniformly 与 Linear blending 是什么关系？属于其中的一种？**

选择一组权重，满足：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda7004fc2a.png)


回归问题，等价于新的特征下的线性回归，唯一的约束条件是alpha非负。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda71eaa724.png)


验证集，训练集


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abda7321fa57.png)


怎么就等于新的特征下的线性回归了？

这个\(\alpha\)既可以在验证集上选，也可以在训练集混合验证集上选。


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae1a5c381931.png)这里没有明白？





## 怎么构造多样性的 g1,g2,...gT


前面讲的是 g1,g2,...gT 都是固定的，都是知道参数的，但是实际应用中实际上不能提前知道 g1,g2,...gT 的，因此一般都是人工自己构造出来：



 	
  * 基于模型

 	
    * 不同的算法。比如 SVM一个，线性回归一个，Naive Bayes一个

 	
    * 不同的参数。超参不同，比如线性回归的惩罚系数，SVM的c




 	
  * 基于数据

 	
    * 随机采样：比如有放回的boostrap的采样。

 	
    * 设置权重，改变样本的概率分布，Adaboost会用的。

 	
    * 随机选择特征，随机森林的思路。





因此，即使算法只有一个，也能构造出多样性的一组 g 。

注意：这种用来混合的 g 一定要有多样性。






# COMMENT：


**需要修正。**
