

值函数近似






前面我们一直假定强化学习任务是在有限状态空间上进行，每个状态可 用一个编号来指代;值函数则是关于有限状态的“表格值函数” (tabular value function),即值函数能表示为一个数组，输入S对应的函数值就是数组元素S的 值，且更改一个状态上的值不会影响其他状态上的值.然而，现实强化学习任务


所面临的状态空间往往是连续的，有无穷多个状态.这该怎么办呢?

一个直接的想法是对状态空间进行离散化，将连续状态空间转化为有限离 散状态空间，然后就能使用前面介绍的方法求解.遗憾的是，如何有效地对状态 空间进行离散化是一个难题,尤其是在对状态空间进行探索之前.

实际上，我们不妨直接对连续状态空伺的值函数进行学习.假定状态空间 为n维实数空间X = R'此时显然无法用表格值函数来记录状态值.先考虑简 单情形，即值函数能表达为状态的线性函数[Busoniu et al., 2010]

Ve{x) = 0Tx ,    (16.32)
其中®为状态向量，0为参数向量.由于此时的值函数难以像有限状态那 样精确记录每个状态的值，因此这样值函数的求解被称为值函数近似(value function approximation).

我们希望通过式(16.32)学得的值函数尽可能近似真实值函数V'近似程 度常用最小二乘误差来度量：

E0 =    [(V-⑷-% ⑻)2]，    (16.33)
其中表示由策略7T所采样而得的状态上的期望.

为了使误差最小化，采用梯度下降法,对误差求负导数

-■gg~ =    2(y ⑻-^(^)) qq
,    (16.34)
于是可得到对于单个样本的更新规则

3 = 0    — Ve{x)) x . z    (16.35)
、我们并不知道策略的真实值函数V'但可借助时序差分学习，基于 P⑷二r + 7P(a/)用当前估计的值函数代替真实值函数，即

3 = 0-}- a(r + 7^0(^) -    x
=0 + a{r + yOTxf — 0Tx) x ,    (16.36)
其中V是下一时刻的状态.

需注意的是，在时序差分学习中需要状态-动作值函数以便获取策略.这里 一种简单的做法是令0作用于表示状态和动作的联合向量上,例如给状态向量 增加一维用于存放动作编号，即将式(16.32)中的$替换为(^a);另一种做法是 用0/1对动作选择进行编码得到向量a = (0;...; 1;... ;0),其中“1”表示该动 作被选择，再将状态向量与其合并得到(％«)，用于替换式(16.32)中的这样 就使得线性近似的对象为状态-动作值函数.

核方法参见第6章.


基于线性值函数近似来替代Sarsa算法中的值函数，即可得到图16.14的 线性值函数近似Sarsa算法.类似地可得到线性值函数近似Q-学习算法.显然， 可以容易地用其他学习方法来代替式(16.32)中的线性学习器，例如通过引入核 方法实现非线性值函数近似.

输入：环境五；

动作空间A;

起始状态怎0;

奖赏折扣7;

更新步长a.

过程：

1： 0 = 0;

2: x = Xq, a = 7r(x) = argmaxa/z 0T(x; a");

3： for f = 1,2,... do

原始策略的e-贪心策略. 式(16.36)更新参数.


4： r，Y二在芯中执行动作a产生的奖赏与转移的状态; 5： a' = 7re(a?/);

6：    0 = 0 + a{r + ^0T{xr\ar) — 0T(ar; a))(a?; a);

7:    7r(x) — argmaxa,z 0T(x; az,);

8:    x — xf，a = af

9： end for

输出：策略TT

图16.14线性值函数近似Sarsa算法










# REF
1. 《机器学习》周志华
