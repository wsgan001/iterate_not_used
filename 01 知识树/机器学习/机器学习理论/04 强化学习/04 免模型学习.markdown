

16.4免模型学习
在现实的强化学习任务中，环境的转移概率、奖赏函数往往很难得知，甚

亦称“无模型学习”


至很难知道环境中一共有多少状态.若学习算法不依赖于环境建模，则称为 “免模型学习”(model-free learning)，这比有模型学习要困难得多.

16.4.1蒙特卡罗强化学习

蒙特卡罗方法参见14.7 节；14.5.1节中使用过马尔 可夫链蒙特卡罗方法.


在免模型情形下，策略迭代算法首先遇到的问题是策略无法评估，这是由 于模型未知而导致无法做全概率展开.此时，只能通过在环境中执行选择的动 作，来观察转移的状态和得到的奖赏.受K摇臂赌博机的启发，一种直接的策 略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏 的近似，这称为蒙特卡罗强化学习.由于采样必须为有限次数，因此该方法更适 合于使用T步累积奖赏的强化学习任务.

另一方面，策略迭代算法估计的是状态值函数V,而最终的策略是通过状 态-动作值函数Q来获得.当模型已知时，从y到q有很简单的转换方法，而 当模型未知时，这也会出现困难.于是，我们将估计对象从V转变为Q，即估计 每一对“状态-动作”的值函数.

此外，在模型未知的情形下，机器只能是从一个起始状态(或起始状态集 合)开始探索环境，而策略迭代算法由于需对每个状态分别进行估计，因此在这 种情形下无法实现.例如探索种瓜的过程只能从播下种子开始，而不能任意选 择种植过程中的一个状态开始.因此，我们只能在探索的过程中逐渐发现各个 状态并估计各状态-动作对的值函数.

综合起来，在模型未知的情形下，我们从起始状态出发，使用某种策略进行 采样，执行该策略r步并获得轨迹

然后，对轨迹中出现的每一对状态-动作，记录其后的奖赏之和，作为该状态-动 作对的一'次累积奖赏采样值.多次采样得到多条轨迹后，将每个状态-动作对的 累积奖赏采样值进行平均，即得到状态-动作值函数的估计.

可以看出，欲较好地获得值函数的估计，就需要多条不同的采样轨迹.然 而，我们的策略有可能是确定性的，即对于某个状态只会输出一个动作，若使用 这样的策略进行采样，则只能得到多条相同的轨迹.这与尺摇臂赌博机的“仅 利用”法面临相同的问题，因此可借鉴探索与利用折中的办法，例如使用e-贪 心法，以e的概率从所有动作中均匀随机选取一个，以1 -e的概率选取当前最 优动作.我们将确定性的策略tt称为“原始策略”，在原始策略上使用e-贪心 法的策略记为

_ .    以概率 1 - e;

(16.20)


IT(X)= <

4中以均匀概率选取的动作，以概率e.

\

假定只有一个最优动作.


对于最大化值函数的原始策略tt = argmaxa<3^，G0，其e-贪心策略7re中，当前 最优动作被选中的概率是1 - e +    而每个非最优动作被选中的概率是

于是，每个动作都有可能被选取,而多次采样将会产生不同的采样轨迹.

与策略迭代算法类似，使用蒙特卡罗方法进行策略评估后，同样要对策 略进行改进.前面在讨论策略改进时利用了式(16.1幻揭示的单调性，通过换 入当前最优动作来改进策略.对于任意原始策略7T,其e-贪心策略7Te仅是将 e的概率均匀分配给所有动作，因此对于最大化值函数的原始策略7rz，同样有 > V^x),于是式(16.16)仍成立，即可以使用同样方法来进行策略

改进.

图16.10给出了上述过程的算法描述，这里被评估与被改进的是同一个策 略，因此称为“同策略”(on-policy)蒙特卡罗强化学习算法.算法中奖赏均值 采用增量式计算，每采样出一条轨迹，就根据该轨迹涉及的所有“状态-动作” 对来对值函数进行更新.

默认均匀概•率选取动作. 采样第s条執迹.

对每一个状态-动作对. 计算轨迹中的累积奖赏. 式(16.2)更新平均奖赏.


输入：环境积 动作空间 起始状态 策略执行步数T.

过程：

1: Q(x, a) = 0, county, a) = 0, a) =


2： for s = 1,2,... do

3：    在E中执行策略7T产生轨迹


根据值函数得到策略.


for f = 0,1，…，T — 1 do R=

n(^. n \ — Q(a^adxcount(gt,ad+fi.

—    count(a:t ,at)+l ，

count (xt, at) = count (a:*, at) + 1 end for

对所有已见状态a::

f \ __ f argmaxoz Q{x,af),    以概率 1 — e;

^a) = \以均匀概率从A中选取命作，以概率e .


10： end for 输出：策略TT


图16.10同策略蒙特卡罗强化学习算法

同策略蒙特卡罗强化学习算法最终产生的是e-贪心策略.然而，引入e胃贪 心是为了便于策略评估，在使用策略时并不需要e-贪心；实际上我们希望改进 的是原始(非贪心：)策略.那么，能否仅在策略评估时引入e-贪心，而在策略改 进时却改进原始策略呢？

这其实是可行的.不妨用两个不同的策略7T和7/来产生采样轨迹，两者的 区别在于每个“状态-动作对”被采样的概率不同.一般的，函数/在概率分布 P下的期望可表达为

E[/] = [ p(x)f(x)Ax ,    (16.21)
Jx
可通过从概率分布p上的采样｛町，来估计/的期望，即

-I m
断水    (16.22)
mi=i
若引入另一个分布g,则函数/在概率分布p下的期望也可等价地写为

E[/] = [ g⑻宇⑷• .    (16.23)
Jx QW

这样基于一个分布的 采样来估计另一个分布 下的期望，称为重要性采 样(importance sampling).


上式可看作在分布9下的期望，因此通过在g上的采样 Xf2, ..”34｝可估计为

軸=    刑).    (16-24)
回到我们的问题上来，使用策略7T的采样轨迹来评估策略7T，实际上就是 对累积奖赏估计期望

-j m
Q(x,a) = —.    (16.25)
i=l

若改用策略7/的采样轨迹来评估策略7T,则仅需对累积奖赏加权，即



(16.26)

其中if和分别表示两个策略产生第€条轨迹的概率.对于给定的一条轨 迹〈吻，a0,ri,...5 xt-1, aT-i?^T,吻》，策略?r产生该轨迹的概率为

T-1

(16.27)


i=Q

虽然这里用到了环境的转移概率P^i+1,但式(16.24)中实际只需两个策略概

率的比值    _

P’    U 丌’(恥，％) •


(16.28)


若7T为确定性策略而屮是7T的e-贪心策略，则始终为1，7vf(xi,ai) 为命或1-€+命，于是就能对策略7T进行评估了.图16.11给出了 “异策 略” (off-policy)蒙特卡罗强化学习算法的描述.

默认均匀概率选取动作. 采样第S条轨迹.

重要性采样系数.

计算修正的累积奖赏.

式(16.2)更新平均奖赏.

根据值函数得到策略.


输入：环境E;

动作空间义；

起始状态工0;

策略执行步数T.

过程：

1： Q(x,a) = 0, count(x, a)

2： for s = 1,2,... do


3:


在E中执行tt的e-贪心策略产生轨迹 < ^o^o,ri,xi,ai,r2,... ,xT-i,aT-i,rT^r >； 1 -e + e/|A|, ai = 7r(x);

Pi =


7

、8 9

10 11


ai / tt ⑻，

V，，，T，T" 1 d° T-l 1 T^t    X Hj=i

n \ — Q(g^，at)xcount(a^,aO+_R. WVH，叫—    count(心，at)+l    ，

count at) = count (心，at) + 1 end for

tt(x) = arg maxo/ Q(x, ar)

end for


for t

R:


输出：策略TT


图16.11异策略蒙特卡罗强化学习算法

16.4.2时序差分学习

蒙特卡罗强化学习算法通过考虑采样轨迹，克服了模型未知给策略估计造 成的困难.此类算法需在完成一个采样轨迹后再更新策略的值估计，而前面介 绍的基于动态规划的策略迭代和值迭代算法在每执行一步策略后就进行值函 数更新.两者相比，蒙特卡罗强化学习算法的效率低得多，这里的主要问题是 蒙特卡罗强化学习算法没有充分利用强化学习任务的MDP结构.时序差分 (Temporal Difference,简称TD)学习则结合了动态规划与蒙特卡罗方法的思 想，能做到更高效的免模型学习.

蒙特卡罗强化学习算法的本质，是通过多次尝试后求平均来作为期望累

积奖赏的近似，但它在求平均时是“批处理式”进行的，即在一个完整的采 样轨迹完成后再对所有的状态-动作对进行更新.实际上这个更新过程能增 量式进行.对于状态-动作对不妨假定基于t个采样已估计出值函数 QU^a) = l    则在得到第f + 1个采样n+1时，类似式(16.3)，有

= Ql(^, a) + 亡 + 丄(n+i —    (16.29)
显然，只需给Qi (x, a)加上增量^f(『t+i - Qt a))即苛•更~*般的，将 替换为系数at+i,则可将增量项写作mi(n+1 -    在实践中通常令

at为一个较小的正数值a，若将展开为每步累积奖赏之和，则可看出 系数之和为1，即令= a不会影响是累积奖赏之和这一性质.更新步长a 越大，则越靠后的累积奖赏越重要.

以7折扣累积奖赏为例，利用动态规划方法且考虑到模型未知时使用状 态-动作值函数更方便，由式(16.10)有

x'ex

=E KU A    (16.30)
xfEX    alEA

通过增量求和可得Q?+X(x,a) = Q^{x, a) + a+    - Qt{x,a)} ,    (16.31)
其中是前一次在状态$执行动作a后转移到的状态，是策略7T在上选 择的动作.

将这几个英文单词的首 字母连起来.


使用式(16.31),每执行一步策略就更新一次值函数估计，于是得到图16.12 的算法.该算法由于每次更新值函数需知道前一步的状态(state)、前一步的动 作(action)、奖赏值(reward)、当前状态(state)、将要拔行的动作(action)，由 此得名为 Sarsa 算法[Rummery and Niranjan，1994].显然，Sarsa 是一个同策 略算法，算法中评估(第6行)、执行(第5行)的均为&贪心策略.

将Sarsa修改为异策略算法，则得到图16.13描述的Q-学习(Q-learning)算 法[Watkins and Dayan, 1992],该算法评估(第6行)的是e-贪心策略,而执行(第 5行)的是原始策略.

默认均匀概率选取动作.


单步执行策略.

原始策略的e-贪心策略. 式(16.31)更新值函数.


输入••环境识

动作空间A;

起始状态卻；

奖赏折扣7;

更新步长a.

过程：

1:    a) = 0, 7r(x, a) = |^^)|;

2: x = Xq^ a = 7r(ic);
3： for t = 1,2,... do

4:    以=在五中执行动作a产生的奖赏与转移的状态;

5： af = 7re(xf);

6： Q(x,a) = Q(x, a) -h a(r + yQ(xf,af) - Q(x,a));

7:    7r(a:) = argmaxa// Q{x^a,r)\

Si x — x Cb —— a

9： end for 输出：策略TT


图 16.12 Sarsa 算法


默认均匀概率选取动作.


单步执行策略.

原始策略.

式(16.31)更新值函数.


输入：环境

动作空间A;

起始状态:T0;

奖赏折扣7;

更新步长Q；.

过程：

1: Q{x^ a) = 0, 7r(a;, a) =    ;

2: x = Xo；

3： for t = 1,2,... do

4:    r，Y =在E中执行动作7re(a:)产生的奖赏与转移的状态;

5： af = 7r(a/);

6： Q{x,a) = Q(x,a) + a(r + yQ(xf,af) - Q(rr，a));

7：    7v(x) = arg maxa〃 Q(x，a");

8： X —— X a —— CL

9： end for 输出：策略TT


图16.13 Q-学习算法
















# REF
1. 《机器学习》周志华
