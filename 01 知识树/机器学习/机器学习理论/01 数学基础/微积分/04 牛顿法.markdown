---
author: evo
comments: true
date: 2018-03-31 02:39:49+00:00
layout: post
link: http://106.15.37.116/2018/03/31/ai-calculus-newton-method/
slug: ai-calculus-newton-method
title:
wordpress_id: 2131
categories:
- 随想与反思
tags:
- '@want_to_know'
---

<!-- more -->

[mathjax]


# REF：






  1. 七月在线 深度学习

********************************************************************************


# 缘由：


对ai涉及到的牛顿法的基本知识进行整理

**牛顿法的推导用的就是泰勒展开式？是的，牛顿法用的是二阶的泰勒级数来逼近，因为是二阶，所以可以很容易找到\(x_0\)附近的使f(x)最小的x，然后再新建一个二阶泰勒级数来逼近，再求一个新的x，以此类推。关于牛顿法怎么用Taylor公式的要补充在这里。**牛顿法，除非你知道目标函数非常好才采用牛顿法。

牛顿法，除非你知道目标函数非常好才采用牛顿法。



\(\widetilde{x}\) 这个就读作 x tilder




# 求某一个损失函数的极小值


**一直想知道牛顿法到底是什么法？**

很多机器学习或者统计的算法最后都转化成一个优化的问题. 也就是求某一个损失函数的极小值的问题, 在本课范围内我们考虑可微分的函数极小值问题。

优化问题：对于一个无穷可微的函数 f(x)，如何寻找他的极小值点.


##  极值点条件






  * 全局极小值: 如果对于任何\(\widetilde{x}\), 都有\(f(x_*)≤f(\widetilde{x})\)，那么 \(x_*\)就是全局极小值点。

  * 局部极小值: 如果存在一个正数 δ 使得，对于任何满足\(|\widetilde{x}-x_*|<δ\)的\(\widetilde{x}\)，都有 \(f(x_*)≤f(\widetilde{x})\)，那么\(x_*\)就是局部极小值点。（方圆 δ 内的极小值点）

  * 不论是全局极小值还是局部极小值一定满足一阶导数/梯度为零，\(f'=0\)或者\(\bigtriangledown f = 0\)




#




#




# 牛顿法


我们本节课利用极值点条件，来介绍牛顿法.


## 牛顿法还是需要很多前提条件的






  * 这种方法只能寻找局部极值

  * 这种方法要求必须给出一个初始点\(x_0\)

  * 数学原理：牛顿法使用二阶逼近

  * 牛顿法对局部凸的函数找到极小值，对局部凹的函数找到极大值，对局部不凸不凹的可能会找到鞍点.

  * 牛顿法要求估计二阶导数。这个要求就比较高


**看来牛顿法的要求还是很多的。**


##  具体怎么做呢？二次逼近




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abef2a06cdbd.png)


**还是没怎么明白牛顿法，而且这个必须要初始点选择的比较好吧？如果用的化怎么选择一个好的初始点？牛顿法就是使用二阶泰勒级数不断逼近极值点吗？**

初始点选的好，就会收敛，选的不好就不会收敛


#




# COMMENT：


**资料和作业还是要做一下的**

参考资料：




  * 数学分析教程，常庚哲，史济怀

  * 简明微积分，龚升

  * 微积分讲义，陈省身


作业

数学分析教程，常庚哲，史济怀 (


  * p142:2,3,7,8;

  * p143:3,4,6;

  * p148:2,3,6;

  * p176:8,11;

  * p210:4,5;

  * p211:6)
