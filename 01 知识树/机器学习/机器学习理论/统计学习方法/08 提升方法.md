##### 第8章提升方法

提升（boosting）方法是一种常用的统计学习方法，应用广泛且有效.在分 类问题中，它通过改变训炼样本的权重，学习多个分类器，并将这些分类器进行 线性组合，提髙分类的性能.

本章首先介绍提升方法的思路和代表性的提升算法AdaBoost；然后通过训练 误差分析探讨AdaBoost为什么能够提高学习精度：并且从前向分步加法模型的

角度解释AdaBoost；最后叙述提升方法更具体的实例-提升树（boosting

tree）. AdaBoost算法是1995年由Freund和Schapire提出的，提升树是2000年 由Friedman等人提出的.

###### 8.1提升方法AdaBoost算法

8.1.1提升方法的基本思路

提升方法基于这样一种思想：对于一•个复杂任务来说，将多个专家的判断进 行适当的综合所得出的判断，要比其中任何一个专家单独的判断好.实际上，就 是“三个臭皮匠顶个诸葛亮”的道理.

历史上，Kearns和Valiant首先提出了“强可学习（strongly leamable ） ”和“弱 可学习（weakly leamable）”的概念.指出：在概率近似正确（probably approximately correct, PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习 算法能够学习它，并且正确率很髙，那么就称这个概念是强可学习的；一个概念， 如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那 么就称这个概念是弱可学习的.非常有趣的是Schapire后来证明强可学习与弱可 学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分 必要条件是这个概念是弱可学习的.

这样一来，问题便成为，在学习中，如果已经发现了 “弱学习算法”，那么 能否将它提升（boost）为“强学习算法”.大家知道，发现弱学习算法通常要比 发现强学习算法容易得多.那么如何具体实施提升，便成为开发提升方法时所要 解决的问题.关于提升方法的研究很多，有很多算法被提出.最具代表性的是 AdaBoost 算法（AdaBoost algorithm）.

对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类 器）要比求精确的分类规则（强分类器）容易得多.提升方法就是从弱学习算法 出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱 分类器，构成一个强分类器.大多数的提升方法都是改变训练数据的概率分布(训 练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分 类器.

这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变训练 数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器.关于第1个 问题，AdaBoost的做法是，提髙那些被前一轮弱分类器错误分类样本的权值，而 降低那些被正确分类样本的权值.这样一来，那些没有得到正确分类的数据，由 于其权值的加大而受到后一轮的弱分类器的更大关注.于是，分类问题被一系列 的弱分类器“分而治之”.至于第2个问题，即弱分类器的组合，AdaBoost采取 加权多数表决的方法.具体地，加大分类误差率小的弱分类器的权值，使其在表 决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小 的作用.

AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里.

8.1*2 AdaBoost 算法

现在叙述AdaBoost算法.假设给定一个二类分类的训练数据集 T =

其中，每个样本点由实例与标记组成.实例七标记=

•V是实例空间，3；是标记集合.AdaBoost利用以下算法，从训练数据中学习一 系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器.

算法 8.1 (AdaBoost)

输入：训练数据集    其中

；V = {-1，+1}:弱学习算法；

输出：最终分类器G(x).

(1)    .初始化训练数据的权值分布

Dx=(wn,-,wu，-, ww), wu -, i = \,2,--,N N

(2)    对m = l，2,…，Af

(a)使用具有权值分布的训练数据集学习，得到基本分类器

Gm (x):X —> {-1，+1}

(b)计算么Cr)在训练数据集上的分类误差率

e«= P(GM = £

(8.1)



(C)计算G„(X)的系数

这里的对数是自然对数.

(d)更新训练数据集的权值分布

Wm+I,i    ⑷)，！■ = 1,2,…，W

这里，是规范化因子

=ZW«/ 呵(-a”yiGm ⑹) 1=1

它使DM+I成为一个概率分布.

(3)构建基本分类器的线性组合

/(x) = JamC7B(x)

得到最终分类器

G(x) = sign(/(x)) = sign

(8.2)

(8.3)

(8.4)

(8.5)

(8.6)

(8.7)



对AdaBoost算法作如下说明：

步骤(1)假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类 器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器^⑶.

步骤(2) AdaBoost反复学习基本分类器，在每一轮m = l，2,…，M顺次地执 行下列操作：

(a)    使用当前分布加权的训练数据集，学习基本分类器

(b)    计算基本分类器在加权训练数据集上的分类误差率：

=p(G«U)*Z)= Z    (8-8)

这里，&表示第m轮中第/个实例的权值，^>#=1.这表明，氏什)在加权的

(=i

训练数据集上的分类误差率是被<?„00误分类样本的权值之和，由此可以看出数 据权值分布仏与基本分类器Gm(x)的分类误差率的关系.

(c)    计算基本分类器0„(*)的系数表示C<)在最终分类器中的重要 性.由式(8.2)可知，当么彡j时，am^0,并且随着么的减小而增大，所以 分类误差率越小的基本分类器在最终分类器中的作用越大.

Cd)更新训练数据的权值分布为下~轮作准备.式(8.4)可以写成:

Gn(Xl) = yi An

由此可知，被基木分类器误分类样本的权值得以扩大，而被正确分类样本 的权值却得以缩小.两相比较，误分类样本的权值被放大e2fl-=-S=—倍.因此，

误分类样本在下一轮学习中起更大的作用.不改变所给的训练数据，而不断改变 训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是 AdaBoost的一个特点.

步骤(3)线性组合/(X)实现A/个基本分类器的加权表决.系数，表示了 基本分类器的重要性，这里，所有&之和并不为1. /C0的符号决定实例 *的类，/(JC)的绝对值表示分类的确信度.利用基本分类器的线性组合构建最终 分类器是AdaBoost的另一特点.

8.1.3 AdaBoost 的例子①

例8.1给定如表8.1所示训练数据.假设弱分类器由;c<v或x>v产生，其 阈值v使该分类器在训练数据集上分类误差率最低.试用AdaBoost算法学习一个 强分类器.

表8.1训练数据表

| 序号    1 | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |      |
| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| X         | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    |
| y         | 1    | 1    | 1    | -1   | -1   | -1   | 1    | 1    | 1    | -1   |

解初始化数据权值分布

wH=0.1, / = 1,2,-,10

对 w = l，

(a)在权值分布为^的训练数据上，阈值v取2.5时分类误差率最低，故基 本分类器为

①例题来源于 ht^)://www.csie.edu.tw。

(b)    G,(x)在训练数据集上的误差率e, = P(G,(xJ*乃)=0.3.

(c)    计算(?办)的系数：a, =ilOgi^- = 0.4236.

2 e,

(d)    更新训练数据的权值分布：

D2=(W21：".，W2P…，W2W)

w2i = ^i-exp^^GiCx,)), » = 1,2,- -,10 zi

D2 = (0.0715,0.0715,0.0715,0.0715,0.0715,0.0715, 0.1666,0.1666,0.1666,0,0715)

7；(«) = 0.4236^(^)

分类器Sign[y；(x)]在训练数据集上有3个误分类点.

对 w = 2，

(a)在权值分布为只的训练数据上，阈值v是8.5时分类误差率最低，基本 分类器为

G!2(x) =

x<8.5

x>8.5

(b) G20c)在训练数据集上的误差率e2=0.2143.

(c )计算 or2 = 0.6496.

(d)更新训练数据权值分布：

D3= (0.0455,0.0455, 0.0455, 0.1667,0.1667,0.1667,

0.1060,0.1060,0.1060,0.0455)

f2(x) = 0.4236G,(x) + 0.6496G2 (x)

分类器sign[/2(x)]在训练数据集上有3个误分类点.

对 = 3，

(a)在权值分布为马的训练数据上，阈值v是5.5时分类误差率最低，基本 分类器为

(b)    G3C0在训练样本集上的误差率& =0.1820.

(c)    计算叫=0.7514.

(d)    更新训练数据的权值分布：

= (0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)

于是得到：

/j(x) = 0.4236G,(x) + 0.6496G2(x) + 0.7514G3(x)

分类器Sign［/3CO］在训练数据集上误分类点个数为0.

于是最终分类器为

G(x) = sign［乂 (*)］ = signlO^eG, CO + 0.6496G2(x) + 0.7514G3 ⑻］    ■

###### 8.2 AdaBoost算法的训练误差分析

AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数 据集上的分类误差率.关于这个问题有下面的定理：

定理8.1 (AdaBoost的训练误差界)AdaBoost算法最终分类器的训练误差

1    N    1    ■哩

—名一Sexp(-y,/(x,)) = nz»

这里，G(x)，/⑻和分别由式(8.7)、式(8.6)和式(8.5)给出.

证明当时，y,f(x,)<0.因而expC-j^/Xx,))彡1.由此直接推导

出前半部分.

后半部分的推导要用到；的定义式(8.5)及式(8.4)的变形：

〜exp(-a„^(?Bt(x/)) = Z„wn+lil

现推导如下:

M

=% e % n 哪(-«»<))

=[ wM exp(-aMyfGM (x,))

###### IP、

这一定理说明，可以在每一轮选取适当的G„使得，最小，从而使训练误差 下降最快.对二类分类问题，有如下结果：

定理8.2 (二类分类问题AdaBoost的训练误差界)

U    M _ M -- < M \

T[Zn= fl[2似l-e„)] = nVO-4^2) < exp -2£(8.10)

m=l    m=l    \ m=l J

这里，

证明由；的定义式(8.5)及式(8.8)得

Z” = £ >%+ exp(-amyiGm(x())

=Z 冰„，〜+ Z wW

= (l-e„)e、+emea"_

=2V^(r-eJ = K    (8.11)

至于不等式

nW-O expf-2^z„2l

则可先由ex和711在点x = 0的泰勒展开式推出不等式^^^^^ < exp(-2zm2), 进而得到.    ■

推论8.1如果存在厂>0,对所有有匕>/，则 1 N

—S !(G(xi) * Z )彡 exp(-2Af/)    (8.12)

A ;=1

这表明在此条件下AdaBoost的训练误差是以指数速率下降的.这一性质当 然是很有吸引力的.

注意，AdaBoost算法不需要知道下界％.这正是Freund与Schapire设计 AdaBoost时所考虑的.与一些早期的提升方法不同，AdaBoost具有适应性，即它 能适应弱分类器各自的训练误差率.这也是它的名称(适应的提升)的由来，Ada 是Adaptive的简写.

###### 8.3 AdaBoost算法的解释

AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模 型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法.

8.3.1前向分步算法

考虑加法模型(additive model)

=    (8.13)

其中，咐;r„)为基函数，4为基函数的参数，足为基函数的系数.显然，式(8.6) 是一个加法模型.

在给定训练数据及损失函数iCv,/co)的条件下，学习加法模型/(X)成为经 验风险极小化即损失函数极小化问题：

啟各£(乃，写狀W«))    (8-14)

通常这是一个复杂的优化问题.前向分步算法(forward stagewise algorithm)求

解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步 只学习一个基函数及其系数，逐步逼近优化目标函数式(8.14)，那么就可以简化 优化的复杂度.具体地，每步只需优化如下损失函数：

N

(乃，州弋;”)    (8.15)

给定训练数据集 r = {(x1,yJ),(Jc2,J,2)»--»(-«j?»y»)} }xteX^Rn , y,^y = 损失函数L(y,/CO)和基函数的集合什⑶7)},学习加法模型/CO的前

向分步算法如下：

算法8.2 (前向分步算法)

输入：训练数据集损失函数Z<y，/(;c));基 函数集译(AZ)};

输出：加法模型/CO.

(1)    初始化/0(x) = O

(2)    对 to = 1，2,…，M

(a)极小化损失函数

\*    N

(此，y„) = arg    + 仰(A; y))    (8.16〉

得到参数此 <b)更新

/mW = /«-iW +    (8.17)

(3)    得到加法模型



(8.18)



这样，前向分步算法将同时求解从m = l到M所有参数足，n,的优化问题简 化为逐次求解各个凡，的优化问题.

8.3.2前向分步算法与AdaBoost

由前向分步算法可以推导出AdaBoost，用定理叙述这一关系.

定理8.3 AdaBoost算法是前向分歩加法算法的特例.这时，模型是由基本

分类器组成的加法模型，损失函数是指数函数.

证明前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法

模型等价于AdaBoost的最终分类器

M

=    (8.19)

由基本分类器及其系数义组成，m =    前向分步算法逐一学习基

函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致.下面证明前 向分步算法的损失函数是指数损失函数(exponential loss function)

厶(J，/W) = exp[-j/(x)]

时，其学习的具体操作等价于AdaBoost算法学习的具体操作.

假设经过m-1轮迭代前向分步算法已经得到:

(x) = f„.2 (x) +    Gm_, (x)

= alGl(x)+- + am_lGm.,(x)

在第m轮迭代得到a。，CO和人Ce) •

Z,W = /«-,(x)+omG!b(x)

目标是使前向分步算法得到的％和使人(x)在训练数据集r上的指数损失 最小，即

= arg rain exp[-y( (/^ (x,) + aG(_x,))}    (8.20)

式(8.20)可以表示为

(«„,GK(xy)= arg mm J；    exp[-少,aG ⑷]    (8.21)

其中，.因为还„,既不依赖a也不依赖于G，所以与最小化 无关.但元，依赖于/^什)，随着每一轮迭代而发生改变.

现证使式(8.21)达到最小的 < 和G⑻就是AdaBoost算法所得到的氏和 G„(x).求解式(8.21)可分两步：

首先，求S(x).对任意a>0,使式(8.21)最小的GC0由下式得到：

###### G*„(x) = argnun^记《/(乃 * G(x.))

其中，exp[-只/„_介)1 •

此分类器GM即为AdaBoost算法的基本分类器GJx)，因为它是使第w轮 加权训练数据分类误差率最小的基本分类器.

之后，求a:.参照式(8.11)，式(8.21)中 StexP 卜只a<7(人)]

=I J；

将已求得的GOO代入式(8.22),对cr求导并使导数为0,即得到使式(8.21)最 小的a.

.1, l-em «« = 2l0g_7^

其中，e„是分类误差率:

N

e”= ——n-=Z    (〜))

lx i=,

i=l

这里的 < 与AdaBoost算法第2(c)步的a„完全一致.

最后来看每--轮样本权值的更新.由

以及 < =，可得

wn+M=wn>j exp[-肌(7„(x)]

这与AdaBoost算法第2(d)步的样本权值的更新，只相差规范化因子，因而 等价.    .

###### 8.4提升树

提升树是以分类树或回归树为基本分类器的提升方法.提升树被认为是统计 学习中性能最好的方法之一.

8.4.1提升树模型

提升方法实际采用加法模型(即基函数的线性组合)与前向分步算法.以决 策树为基函数的提升方法称为提升树(boosting tree).对分类问题决策树是二叉 分类树，对回归问题决策树是二叉回归树.在例8.1中看到的基本分类器;c<v或 x>v,可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决 策树桩(decision stump).提升树模型可以表示为决策树的加法模型：

M

/M(x) = 2r(x;0m)    (8.23)

m=l

其中，表示决策树：为决策树的参数；a/为树的个数.

8.4.2提升树算法

提升树算法采用前向分步算法.首先确定初始提升树/e(x) = 0,第w歩的模

型是

Z,,W=ZB-1W+r(x；©m)    (8.24)

其中，人为当前模型，通过经验风险极小化确定下一棵决策树的参数e。，

em=argmin    (xj + T(x,; ©„))    (8.25)

由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间 的关系很复杂也是如此，所以提升树是一个髙功能的学习算法.

下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数 不同.包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及 用一般损失函数的一般决策问题.

对于二类分类问题，提升树算法只需将AdaBoost算法8.1中的基本分类器限 制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况， 这里不再细述.下面叙述回归问题的提升树.

已知一个训练数据集义为输 入空间，ytey^R, 3；为输出空间.在5.5节中已经讨论了回归树的问题.如 果将输入空间1划分为个互不相交的区域2?,，^，…，心，并且在每个区域上确 定输出的常量Cy,那么树可表示为

j

T(x; ©) = ^ Cjl(x eRj)    (8.26)

/=i

其中，参数© = «代，Cj,(及，c2)，…，况而)｝表示树的区域划分和各区域上的常

数.J是回归树的复杂度即叶结点个数. 回归问题提升树使用以下前向分步算法:

/oW = O

fmW =    W+T(.x;Qm), m =

M

/mW = Z^；©m)

在前向分步算法的第W步，给定当前模型⑻，需求解 0«=arg min    ) + T{xt; 0m ))

得到白„，即第棵树的参数.

当采用平方误差损失函数时，

取7V)) = (y-_/W

其损失变为

吻-人-相-nx;©』2 = [r-r(x;0J]2

这里，

"=广/„-相    (8.27)

是当前模型拟合数据的残差(residual).所以，对回归问题的提升树算法来说， 只需简单地拟合当前模型的残差.这样，算法是相当简单的.现将回归问题的提 升树算法叙述如下.

算法8 J (回归问题的提升树算法)

输入:训练数据集r = {(么乃)，(^[，乃)》•■••，(々，外)}, x,eXQR", y.ey^R； 输出：提升树/^x).

(1)    初始化/flco=o

(2)    对 m = l，2,…，M (a)按式(8.27)计算残差

(b)拟合残差rmi学习一个回归树，得到r(x;0„) (C)更新人⑻⑶+ror;e„)

(3)得到回归问题提升树

，《(x)=：£r(x;e”)    ■

«=i

例8.2已知如表8.2所示的训练数据，x的取值范围为区间［0.5,10.5］，j的 取值范围为区间［5.0,10.0］,学习这个回归问题的提升树模型，考虑只用树桩作为 基函数.

表8.2训练数据表

x,    1    . 2    _ .3    4.5.6    7    .8    9    10

y,    5.56    5.70    5.91    6.40    6.80    7.05    8.卯 8.70    9.00    9.05

解按照算法8J,第I步求乂⑻即回归树7］(x). 苜先通过以下优化问题：



![img](2012.4e2a-62.jpg)



求解训练数据的切分点S:

72, = {x|jf^ ,    7^ = {x | x > s}

容易求得在代，尽内部使平方损失误差达到最小值的C|, 02为

这里％,仏是氏，奂的样本点数.

求训练数据的切分点.根据所给数据，考虑如下切分点：

1.5, 2.5, 3.5, 4.5» 5.5, 6.5, 7.5, 8.5, 9.5 对各切分点，不难求出相应的代，氏，c,, 9及

mfs) = min (J, ~c,)2 +min (yt -c2)z

q    01 x,e«2

例如，当 s = 1.5 时，代={1}，尽={2,3,…，10}, c,=5.56, <^=7.50,

m(s) = min (Z - ci)2 +min    (>>, -c2)z = 0+15.72 = 15.72

Cl »(6«2

现将s及m(s)的计算结果列表如下(见表8.3).

表83计算数据表

| S    | 1.5   | 2.5   | 3.5  | 4.5  | 5.5  | 6.5  | 7,5  | 8.5   | 9.5   |
| ---- | ----- | ----- | ---- | ---- | ---- | ---- | ---- | ----- | ----- |
| W⑻   | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |

由表8.3可知，当■? = 6.5时w⑷达到最小值，此时代={1,2,-,6},^= {7,8,9,10}, c,=6.24, c2=8.91,所以回归树7JCO为

16.24, 18.91, /(x) = 7；(x)

x<6.5

x^6.5



用/(x)拟合训练数据的残差见表8.4,



表中 ^=_y,-7IU)，



jc, .1    2    3    456    7    8    9    10

r2,    -0.68    -0.54    -0.33    0—16    0.56    0.81    -0.01    -0.21    0.09    0.14

用/(X)拟合训练数据的平方损失误差：

10





###### L(,y,fSx))=E U - Z (巧))2 = 1.93 /-I

第2步求r2co.方法与求—样，只是拟合的数据是表8.4的残差.可 以得到：

r2(x)=



-0.52,

0.22,



x<3.5

x>3.5



/2W = ZW + r2(x)

| 5.72, | x<3.5     |
| ----- | --------- |
| 6.46, | 3.5^x<6.5 |
| 9.13, | x 衾 6.5  |

用/2C0拟合训练数据的平方损失误差是

###### 10

i(y，Z00)=Z(z-/2(x())J =0.79

M

继续求得



r3(工)=•

r4(x)=

r5(x)=-

W=-

| 0.15,  | x<6.5    |
| ------ | -------- |
| -0.22, | x 多 6.5 |
| -0.16, | x<4.5    |
| 0.11,  | x 衾 4.5 |
| 0.07,  | x<6.5    |
| -0.11, | x^6.S    |
| -0.15, | x<2.5    |
| 0.04,  | x^2.5    |

L(y,/3(x)) = 0.47,

L(y,/4(x)) = 0.30,

L(y,/s(x)) = 0.23,



/6（x）=/s（x）+r6（x）=rt（x）+-+r5（x）+r6（x）

| 5.63， | x<2.5         |
| ------ | ------------- |
| 5.82， | 2.5    <3.5   |
| 6.56， | 3.5 冬x<4.5   |
| 6.83， | 4.5 彡 x <6.5 |
| 8.95， | x^6.5         |

用拟合训练数据的平方损失误差是

^Cv>/6(x)) = EU -乂⑷)2 =0.17

i=i

假设此时已满足误差要求，那么/(x) = f6(x)即为所求提升树.

8.4.3梯度提升

提升树利用加法模型与前向分歩算法实现学习的优化过程.当损失函数是平 方损失和指数损失函数时，每一步优化是很简单的.但对一般损失函数而言，往 往每一步优化并不那么容易.针对这一问题，Freidman提出了梯度提升(gradient boosting)算法.这是利用最速下降法的近似方法，其关键是利用损失函数的负 梯度在当前模型的值

L 彻 J/^w

作为回归问题提升树算法中的残差的近似值，拟合一个回归树.

算法8.4 (梯度提升算法)

输入：训练数据集7 =供，乃)，(々，)，…，(〜，JW)}' e A* c R",乃e y £ R ; 损失函数£(y，/(x)):

输出：回归树/(x).

(1)    初始化

N

ZoW = arg min ^£(>»pc)

C i=l

(2)    对 w = l，2,…，Af

(a)对f = l，2，".，7V,计算

7 -

L彻」紙^

(b)    对rmi拟合一个回归树，得到第m棵树的叶结点区域； = 1,2,--,J

(c)    对/ = 1,2,…，J,计算

^=argmin iU，/m-,(x,) + c)

c栌〜

(d)更新厶00=人_相+玄〜，(妊〜)

(3)得到回归树

/W = Af(x)=    R^)    ■

m=1 J=i

算法第1步初始化，估计使损失函数极小化的常数值，它是只有一个根结点 的树.第2⑻步计算损失函数的负梯度在当前模型的值，将它作为残差的估计.对 于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近 似值.第2(b)步估计回归树叶结点区域，以拟合残差的近似值.第2(c)步利用线 性搜索估计叶结点区域的值，使损失函数极小化.第2(d)步更新回归树.第3步 得到输出的最终模型/(X).

###### 本章概要

\1.    提升方法是将弱学习算法提升为强学习算法的统计学习方法.在分类学 习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器(弱 分类器)，并将这些基本分类器线性组合，构成一个强分类器.代表性的提升方 法是AdaBoost算法.

AdaBoost模型是弱分类器的线性组合：

M

###### 乂 (X) =

m«»I

\2.    AdaBoost算法的特点是通过迭代每次学习~个基本分类器.每次迭代中， 提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的 权值.最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差 率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值.

\3.    AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练 数据集上的分类误差率，这说明了它作为提升方法的有效性.

\4.    AdaBoost算法的--个解释是该算法实际是前向分步算法的一个实现.在 这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法.

每一步中极小化损失函数

(A,，Z„)=吨辦    邱(W))

5.提升树是以分类树或回归树为基本分类器的提升方法.提升树被认为是 统计学习中最有效的方法之一.

###### 继续阅读

提升方法的介绍可参见文献[1,2]. PAC学习可参见文献[3].强可学习与弱可学 习的关系可参见文献[4].关于AdaBoost的最初论文是文献[5].关于AdaBoost的 前向分步加法模型解释参见文献[6],提升树与梯度提升可参见文献[6,7]. AdaBoost 只是用于二类分类，Schapire与Singer将它扩展到多类分类问题AdaBoost与 逻辑斯谛回归的关系也有相关研究[9].

###### 习-题

8.1某公司招聘职员考査身体、业务能力、发展潜力这3项.身体分为合格1、 不合格0两级，业务能力和发展潜力分为上1、中2、下3三级.分类为合 格1、不合格-1两类.已知10个人的数据，如下表所示.假设弱分类器为 决策树桩.试用AdaBoost算法学习一个强分类器.

应聘人员情况数据表

| 1         | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 身体    0 | 0    | 1    | 1    | 1    | 0    | 1    | 1    | 1    | 0    |
| 业务    1 | 3    | 2    | 1    | 2    | 1    | 1    | 1    | 3    | 2    |
| 潜力    3 | 1    | 2    | 3    | 3    | 2    | 2    | 1    | 1    | 1    |
| 分类    1 | -1   | -I   | -1   | -1   | -1   | 1    | 1    | -I   | -I   |

8.2比较支持向量机、AdaBoost,逻辑斯话回归模型的学习策略与算法.

###### 参考文献

Freund Y, Schapire RE. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence, 1999, 14(5): 771-780

Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning: Data Mining,

Inference, and Prediction. Springer-Verlag, 2001 (中译本：统计学习基础-数据挖掘、推

理与预测.范明，柴玉梅，昝红英，等译.北京：电子工业出版社，2004)

Valiant LG A theory of the leamable. Communications of the ACM, 1984,27(11): 1134-1142 Schapire R. The strength of weak leamability. Machine Learning, 1990,5(2): 197-227 Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an

PQQZ 'iBomof Sunuesq

auiqOTj^ saaoBKip ireuiSaia pnc jsoogepv "noissajSai opsiSoq x -ra^ras *a sji^tps *K sutijoo [g] 9H-L6Z ：(€)££ ‘6661 ‘Supieaq amqocw

•snopofpaid pojBj-Mnapijnoo 8msn sunpuoSjB Supsooq paAojduq •人 jaSojs ‘3H andeqas [8] (S)6Z *1002

*sopsijBjs jo siEuay aaiqoBm Supsooq inajpaig e :aopennxoiddB aoijoaiy XpasjQ f nrnnpsuj [/,] LOP~L£€ ：83 ‘0002 'sopspBisjo S[buuv (suoissnosip qjiM)

Snpsooq jo msia jcopspBis b :uoissmSw opsi3o[ SAijippy raanqsqij, *i apscn *f ueaipsuj [9] L£~ZZ *S66l *W6 PA

•aonaps jaindaioQ in sw)n amjoaq Xioain Suiareaq icaopejndiuo^) .3叩sooq oj nopeai[dde
