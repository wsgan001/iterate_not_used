
贝叶斯网

贝叶斯网(Bayesian network)亦称“信念网” (belief network),它借助有向无环图(Directed Acyclic Graph,简称DAG)来刻画属性之间曲依赖关系，并使用条件概率表(Conditional Probability Table,简称CPT)来描述属性的联合概率分布.

具体来说，一个贝叶斯网 B 由结构 G 和参数 $\Theta$  两部分构成，即 $B =\langle G,\Theta \rangle$ 。网络结构 G 是一个有向无环图，其每个结点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来；参数 $\Theta$ 定量描述这种依赖关系， 假设属性 $x_i$ 在 G 中的父结点集为 $\pi_x$ ，则 $\Theta$ 包含了每个属性的条件概率表 $\theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$ 。


作为一个例子，图 7.2 给出了西瓜问题的一种贝叶斯网结构和属性 “根蒂” 的条件概率表.从图中网络结构可看出，“色泽”直接依赖于“好瓜”和 “甜度”，而“根蒂”则直接依赖于“甜度”；进一步从条件概率表能得到 “根蒂”对“甜度”量化依赖关系，如 $P(根蒂二硬挺|甜度二高)=0.1$ 等.


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/HEEDJH0IL8.png?imageslim)


# 1 结构

贝叶斯网结构有效地表达了属性间的条件独立性.给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是 $B=\legal G,\Theta \rangle$ 将属性 $x_1,x_2,\cdots ,x_d$ 的联合概率分布定义为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/2ckLDcdJb4.png?imageslim)

以图7.2为例，联合概率分布定义为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/IkekBFg9ii.png?imageslim)


显然，$x_3$ 和 $x_4$ 在给定 $x_1$ 的取值时独立， $x_4$ 和 $x_5$ 在给定 $x_2$ 的取值时独立，分别简记为 $x_3\perp x_4 | x_1$ 和 $x_4\perp x_5 | x_2$.

图7.3 显示出贝叶斯网中三个变量之间的典型依赖关系，其中前两种在 式(7.26)中已有所体现•

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/lFEKa52KcD.png?imageslim)



在 “同父” (common parent)结构中，给定父结点 $x_1$ 的取值，则 $x_3$ 与 $x_4$  条件独立.在“顺序”结构中，给定 x 的值，则 $y$ 与 $z$ 条件独立.V 型结构(V-structure)亦称“冲撞”结构，给定子结点 $x_4$ 的取值, $x_1$ 与 $x_2$ 必不独立；奇妙的是，若 $x_4$ 的取值完全未知，则 V 型结构下 $x_1$ 与 $x_2$ 却是相互独立的.我们做 一个简单的验证：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/96Bhd9L08k.png?imageslim)


这样的独立性称为 “边际独立性” (marginal independence),记为 $x_1 \upvDash x_2$

事实上，一个变量取值的确定与否，能对另两个变量间的独立性发生影响, 这个现象并非 V 型结构所特有。例如在同父结构中，条件独立性 $x_3\perp x_4 | x_1$ 成立，但若 $x_1$ 的取值未知，则 $x_3$ 和 $x_4$ 就不独立，即  $x_3 \upvDash x_4$ 不成立；在顺序结构中  $y\perp z | x$ ，但 $y \upvDash z$ 不成立.


为了分析有向图中变量间的条件独立性，可使用“有向分离”(D-separation).我们先把有向图转变为一个无向图：

- 找出有向图中的所有 V 型结构，在V型结构的两个父结点之间加上一条无向边；
- 将所有有向边改为无向边.



由此产生的无向图称为“道德图”(moral graph)(也有译为“端正图”),令父结点相连的过程称为 “道德化”(moralization) [Cowell et al., 1999].

基于道德图能直观、迅速地找到变量间的条件独立性.假定道德图中有变量 $x$ ,$y$ 和变量集合 $z = \{z_i\}$ ,若变量$x$ 和 $y$ 能在图上被 z 分开，即从道德图中将变量集合 z 去除后，x 和y 分属两个连通分支，则称变量x和y被Z有向分离， $x\perp y | z$成立。

例如，图7.2所对应的道德图如图7.4所示，从图中能容易地找 出所有的条件独立关系：$x_3\perp x_4 | x_1$，$x_4\perp x_5 | x_2$ $x_3\perp x_2 | x_1$，$x_3\perp x_5 | x_1$，$x_3\perp x_5 | x_2$等.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/Emac6J8EFm.png?imageslim)

# 2. 学习


若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需通过对训练样本“计数”，估计出每个结点的条件概率表即可.

但在 现实应用中我们往往并不知晓网络结构，于是，贝叶斯网学习的首要任务就是 根据训练数据集来找出结构最“恰当”的贝叶斯网.

“评分搜索”是求解这一 问题的常用办法.具体来说，我们先定义一个评分函数(score function),以此来 评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优 的贝叶斯网.显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归 纳偏好.

常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时 编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需 的字节长度.

对贝叶斯网学习而言，模型就是一个贝叶斯网，同时，每个贝叶斯网描述了一个在训练数据上的概率分布，自有一套编码机制能使那些经常出现的样本有更短的编码.于是，我们应选择那个综合编码长度(包括描述网络 和编碍数据)最短的贝叶斯网，这就是“最小描述长度” (Minimal Description Length,简称MDL)准则.


给定训练集 $D=\{x_1,x_2,\cdots x_m\}$ ,贝叶斯网 $B=\langle G,\Theta \rangle$  在 D 上的评分函数可写为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/GajfDidhdL.png?imageslim)

其中， $|B|$ 是贝叶斯网的参数个数;$f(\theta)$ 表示描述每个参数 $\theta$ 所需的字节数;而

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/D8FCA4Ad7d.png?imageslim)

是贝叶斯网 B 的对数似然.显然，式(7.28)的第一项是计算编码贝叶斯网 B 所需的字节数，第二项是计算 B 所对应的概率分布 $P_B$ 需多少字节来描述 D 。于是，学习任务就转化为一个优化任务，即寻找一个贝叶斯网 B 使评分函数 $s(B|D)$ 最小.

若 $f(\theta)=1$ ,即每个参数用 1 字节描述，则得到 AIC (Akaike Information Criterion)评分函数

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/eeB6i876BL.png?imageslim)


若 $f(\theta)=\frac{1}{2}log\,m$ ,即每个参数用 $\frac{1}{2}log\,m$字节描述，则得到BIC (Bayesian Information Criterion)评分函数

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/GEEKlDAim7.png?imageslim)

显然，若 $f(\theta)=0$ ,即不计算对网络进行编码的长度，则评分函数退化为负对数似然，相应的，学习任务退化为极大似然估计.

不难发现，若贝叶斯网 $B=\langle G,\Theta\rangle$ 的网络结构 G 固定，则评分函数 $s(B|D)$ 的第一项为常数.此时,最小化 $s(B|D)$ 等价于对参数 $\Theta$ 的极大似然 估计.由式(7.29)和(7.26)可知，参数 $\theta_{x_i|\pi_i}$ 能直接在训练数据 D 上通过经验估计获得，即

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/Kia90gEaea.png?imageslim)


其中 $\hat{P}_D(\cdot )$ 是 D 上的经验分布.因此，为了最小化评分函数 $s(B|D)$ ，只需对网 络结构进行搜索，而候选结构的最优参数可直接在训练集上计算得到.


不幸的是，从所有可能的网络结构空间搜索最优贝叶斯网结构是一个 NP 难问题，难以快速求解。有两种常用的策略能在有限时间内求得近似解：

- 第一 种是贪心法,例如从某个网络结构出发，每次调整一条边(增加、删除或调整方 向)，直到评分函数值不再降低为止;
- 第二种是通过给网络结构施加约束来削减 搜索空间，例如将网络结构限定为树形结构等.

# 3.推断


贝叶斯网训练好之后就能用来回答“查询”(query)，即通过一些属性变量的观测值来推测其他属性变量的取值.例如在西瓜问题中，若我们观测到西瓜 色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何.这样通过已知变量观测值来推测待查询变量的过程称为 “推断”(inference)，已知变量观 测值称为 “证据” (evidence).


最理想的是直接根据贝叶斯网定义的联合概率分布来精确计算后验概率, 不幸的是，这样的“精确推断”已被证明是NP难的[Cooper, 1990];换言之，当网络结点较多、连接稠密时,难以进行精确推断，此时需借助“近似推断”， 通过降低精度要求，在有限时间内求得近似解。

在现实应用中，贝叶斯网的近似 推断常使用吉布斯采样(Gibbs sampling)来完成，这是一种随机采样方法，我们来看看它是如何工作的.

令 $Q=\{Q_1,Q_2,\cdots ,Q_n\}$ 表示待查询变量， $E=\{E_1,E_2,\cdots E_k\}$ 为证据变量，已知其取值为 $e = \{e_1,e_2,\cdots e_k\}$ 。目标是计算后验概率 $P(Q = q | E = e)$ , 其中 $q=\{q_1,q_2,\cdots ,q_n\}$ 是待查询变量的一组取值.以西瓜问题为例，待查询变量为 $Q=\{好瓜，甜度\}$ ，证据变量为 $E=\{色泽，敲声，根蒂\} 且已知其取值为 $e = \{青绿，浊响，蜷缩\}$ ，查询的目标值是 $q = \{是，高\}，即这是好瓜且甜度高的概率有多大.

如图 7.5 所示，吉布斯采样算法先隨机产生一个与证据 $E=e$  一致的样本 $q^0$ 作为初始点，然后每步从当前样本出发产生下一个样本。具体来说，在第 t 次采样中，算法先假设 $q^t=q^{t-1}$ ，然后对非证据变量逐个进行采样改变其取值，采样概率根据贝叶斯网 B 和其他变量的当前取值(即 Z=z )计算获得.假定经过 T 次采样得到的与 q —致的样本共有 $n_q$ 个，则可近似估算出后验概率

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/CJAfbij8a4.png?imageslim)


实质上，吉布斯采样是在贝叶斯网所有变量的联合状态空间与证据 $E = e$ 一致的子空间中进行“随机漫步” (random walk).每一步仅依赖于前一步 的状态，这是一个“马尔可夫链” (Markov chain).在一定条件下，无论从 什么初始状态开始，马尔可夫链第t步的状态分布在 $t\rightarrow \infty$ 时必收敛于一个平稳分布(stationary distribution);对于吉布斯采样来说，这个分布恰好是 $P(Q|E=e)$ 。因此，在 T 很大时，吉布斯采样相当于根据 $P(Q |E=e)$ 采样， 从而保证了式(7.33)收敛于 $P(Q =q|E=e)。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/4KGAIC68l4.png?imageslim)

需注意的是，由于马尔可夫链通常需很长时间才能趋于平稳分布，因此吉布斯采样算法的收敛速度较慢.此外，若贝叶斯网中存在极端概率 “0” 或 “1”，则不能保证马尔可夫链存在平稳分布，此时吉布斯采样会给出错误的估计结果.










# REF

1. 《机器学习》周志华
