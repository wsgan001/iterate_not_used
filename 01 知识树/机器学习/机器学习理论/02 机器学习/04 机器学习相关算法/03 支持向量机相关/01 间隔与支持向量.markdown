
# 出发点，想用一个划分超平面来区分样本


给定的训练样本集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\},y_i\in \{-1,+1\}$ ，分类学习最基本的想法就是基于训练集 D 在样本空间中找到一个划分超平面，将不同类别的样本分开。但是能将训练样本分开的划分超平面可能有很多，如下图所示，我们哪一个最好呢？


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/JaDCGAjhfh.png?imageslim)


直观上看，我们应该去找位于两类训练样本的所谓 “正中间” 的划分超平面，因为这样的超平面看起来对训练样本局部扰动的 “容忍” 性最好。也就是说，这样的分类结果应该是最鲁棒的，泛化能力最强的。

到这里，我们就想问了，什么是所谓的 “正中间”？怎么衡量？怎么求出这个 “正中间”？


# 先看一下划分超平面是什么，怎么定义


我们先看下对于这个划分超平面的描述。

实际上，在样本空间中，划分超平面可通过如下线性方程来描述：

$$w^Tx+b=0$$

其中 $mathbb{w}=(w_1;w_2;\cdots w_d)$ 为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。显然，一个划分超平面是可以被法向量 w 和位移 b 确定的，我们可以记为 $(w,b)$ 。那么样本空间中任意点 x 到超平面 $(w,b)$ 的距离可写为：

$$r=\frac{|w^Tx+b|}{||w||}$$


# 那么能够成功区分样本的超平面是什么样子的？


假设超平面 (w,b) 能将训练样本正确分类，即对于 $x_i,y_i)\in D$ ，若 $y_i=+1$，则有 $w^Tx_i+b>0$ ；若 $y_i=-1$ ，则有 $w^Tx_i+b<0$ 。令：

$$\left\{\begin{matrix} w^Tx_i+b\geqslant +1&y_i=+1 \\ w^Tx_i+b\leqslant -1&y_i=-1 \end{matrix}\right.$$

可见，对于上面这个式子来说，距离超平面最近的这几个训练样本点使得等号成立。由于每个样本点对应一个特征向量，因此，这几个使得取等号的样本点就称为支持向量 (support vector)。从下图可以看出就是这三个 被画了圆圈的点。 <span style="color:red;">嗯，对于这个support 应该怎么理解呢？</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/h9jKJD4J15.png?imageslim)


然后，我们定义两个异类支持向量到超平面的距离的和 $\gamma=\frac{2}{||w||}$ 为间隔(margin)。


# 我们想找到这个所谓的最中间的超平面


OK，这时候，我们想找到一个最中间的超平面，也就是想要找到具有最大间隔 (maximum margin) 的划分超平面，也就是想找到这样：

$$\begin{align*} &\underset{w,b}{max}\; \frac{2}{||w||}\\ & s.t.\; y_i(w^Tx_i+b)\geqslant 1,i=1,2,\cdots ,m \end{align*}$$

即在满足 $y_i(w^Tx_i+b)\geqslant 1$ 的条件下，使得 $\gamma$ 最大的 $w$ 和 $b$ 。

也就是说：

$$\begin{align*} &\underset{w,b}{min}\; \frac{1}{2}||w||^2\\ & s.t.\; y_i(w^Tx_i+b)\geqslant 1,i=1,2,\cdots ,m \end{align*}$$

这个，就是支持向量机 (Support Vector Machine，简称 SVM ) 的基本型。

注：上面这个式子看起来这个最小仅与 $w$ 有关, 但是事实上 $b$ 通过约束隐式地影响着 $w$ 的取值，进而对间隔产生影响。<span style="color:red;">嗯，是的</span>





# REF
1. 《机器学习》周志华
