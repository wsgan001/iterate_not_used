---
author: evo
comments: true
date: 2018-05-10 14:05:25+00:00
layout: post
link: http://106.15.37.116/2018/05/10/rf-sample1/
slug: rf-sample1
title: 
wordpress_id: 5530
categories:
- 随想与反思
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL






  1. [第7章 集成方法 ensemble method](http://ml.apachecn.org/mlia/ensemble-random-tree-adaboost/#_3)




# TODO






  * aaa




# MOTIVE






  * aaa





* * *






# 项目说明


给你一些声纳信号数据，来区分声纳的类型。


# 项目数据


[download id="5546"]


# 完整代码




    from random import seed, randrange, random
    import copy
    import math


​    
    # 导入csv文件
    def load_data_set(file_name):
        datas = []
        with open(file_name, 'r') as fr:
            for line in fr.readlines():
                if not line:
                    continue
                line_arr = []
                for featrue in line.split(','):
                    str_f = featrue.strip()  # 移除空格
                    try:
                        # 这里不能用str_f.isdigits()来判断是不是数值，因为小数点会被判定为 false
                        n = float(str_f)
                        line_arr.append(n)
                    except:
                        if str_f == "R":  # 分类标签
                            line_arr.append(1)
                        else:
                            line_arr.append(0)
                datas.append(line_arr)
        return datas


​    
    # 交叉采样 这个采样的方式的名字叫什么？忘记了
    # 数据集dataset分成n_flods份，每次都是从dataset中抽取一个dataset/n_folds大小的list，为了用于交叉验证
    # 抽出一个list的时候是又放回的还是无放回的？
    def cross_validation_split(dataset, n_folds):
        dataset_split = list()
        fold_size = len(dataset) / n_folds
        for i in range(n_folds):
            dataset_copy = copy.deepcopy(dataset)
            fold = list()
            while len(fold) < fold_size:
                index = randrange(len(dataset_copy))
                # 在采样出一个fold的时候，到底使用的是有放回的采样还是无放回的采样？
                fold.append(dataset_copy.pop(index))  # 无放回的方式
                # fold.append(dataset_copy[index])  # 有放回的方式
            dataset_split.append(fold)
        return dataset_split


​    
    # 尝试根据这一行的这个特征的特征值来划分 dataset
    def get_split_result_with_feature_value(feature_index, feature_value, dataset):
        # 根据每个样本对应的这个特征的特征值大于或者小于这个值，来划分为两个列表
        left, right = list(), list()
        for row in dataset:
            if row[feature_index] < feature_value:
                left.append(row)
            else:
                right.append(row)
        return left, right


​    
    # 为这一次的分割计算基尼指数
    def calc_gini_index(groups, labels):  # 个人理解：计算代价，分类越准确，则 gini 越小
        gini = 0.0
        for label in labels:  # labels = [0, 1]
            for group in groups:  # groups = (left, right)
                size = len(group)
                if size == 0:
                    # 说明左子树或者右子树没有
                    continue
                proportion = [row[-1] for row in group].count(label) / float(size)
                gini += (proportion * (1.0 - proportion))  # 个人理解：计算代价，分类越准确，则 gini 越小
        # 左右两边的数量越一样，说明数据区分度不高，gini系数越大
        return gini


​    
    # 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）
    def get_current_features_split_result(dataset, n_features):
        # 获得标记的set
        labels = list(set(row[-1] for row in dataset))  # class_values =[0, 1]
    
        # b_index：最优的分类特征。b_value：分类特征值 。b_score：这里使用的是基尼系数。 b_groups：分类结果 ？
        b_index, b_value, b_score, b_groups = 999, 999, 999, None
    
        # 开始选择出 n_features 个特征
        ifeatures = list()
        while len(ifeatures) < n_features:
            index = randrange(len(dataset[0]) - 1)
            if index not in ifeatures:
                ifeatures.append(index)
        # 从这些特征中找到一个最优的分法
        for index in ifeatures:  # 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性
            for row in dataset:
                # 尝试根据这一行的这个特征的特征值来划分dataset
                groups = get_split_result_with_feature_value(index, row[index], dataset)  # groups=(left, right)
                gini = calc_gini_index(groups, labels)
                if gini < b_score:
                    b_index, b_value, b_score, b_groups \
                        = index, row[index], gini, groups
                    # 将此时这些特征中分类最好的特征的index及其值返回，同时返回分类的左右类
        return {'index': b_index, 'value': b_value, 'groups': b_groups}


​    
    # 输出group中出现次数较多的标签
    def to_terminal(group):
        outcomes = [row[-1] for row in group]
        # 输出 group 中出现次数较多的标签
        # max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准 max这种用法要总结下。
        return max(set(outcomes), key=outcomes.count)


​    
    # 为一个结点创建子分割器，递归分类，直到分类结束
    # max_depth = 10, min_size = 1,
    def split(node, max_depth, min_size, n_features, current_depth):
        left, right = node['groups']
        del (node['groups'])
        # 左边或者右边已经没有了
        if not left or not right:
            # 就不用分了 设定这时候的 node 的左和右是同一种 label
            node['left'] = node['right'] = to_terminal(left + right)
            return
        # 检查是不是超过了最大深度
        if current_depth >= max_depth:
            # 如果超过了，但是还是没有结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合
            node['left'], node['right'] = to_terminal(left), to_terminal(right)
            return
        # 处理左边的
        if len(left) <= min_size:
            # 左边的只有一个样本
            node['left'] = to_terminal(left)
        else:
            # 这个时候 node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，
            # 也就是说 node是一个多层字典
            node['left'] = get_current_features_split_result(left, n_features)
            # 这个地方不明白了，难道每次递归的时候，n_features也要重新选择出来吗？不是使用的开始随机选出来的那些吗？
            # 这个n_features 的确是要重新选择出来的，我试过把features一开始选择出来，然后后面就用这些feature，但是效果基本维持在60% 上不去了
            split(node['left'], max_depth, min_size, n_features, current_depth + 1)  # 递归，depth+1计算递归层数
        # 处理右边的
        if len(right) <= min_size:
            # 右边的只有一个样本
            node['right'] = to_terminal(right)
        else:
            node['right'] = get_current_features_split_result(right, n_features)
            split(node['right'], max_depth, min_size, n_features, current_depth + 1)


​    
    # 创建一个决策树
    # max_depth       决策树深度不能太深，不然容易导致过拟合 为什么？
    # min_size        叶子节点的大小
    def build_tree(train, max_depth, min_size, n_features):
        # 进行第一次划分
        root = get_current_features_split_result(train, n_features)
        # 对左右两边的数据 进行递归的调用 按照split里面的算法来看，之前用过的feature仍然是可能被选择的，是这样吗？
        split(root, max_depth, min_size, n_features, current_depth=1)
        return root


​    
    # 简单投票法判断出该行所属分类
    def predict(tree, row):  # 预测模型分类结果
        if row[tree['index']] < tree['value']:
            if isinstance(tree['left'], dict):
                return predict(tree['left'], row)
            else:
                return tree['left']
        else:
            if isinstance(tree['right'], dict):
                return predict(tree['right'], row)
            else:
                return tree['right']


​    
    # 使用一批树，对一行的结果进行预测
    def bagging_predict(trees, row):
        predictions = [predict(tree, row) for tree in trees]
        # 返回结果中出现次数最多的结果，相当于投票
        return max(set(predictions), key=predictions.count)


​    
    # 可放回的重复采样，创建数据集的随机子样本
    def sub_sample(dataset, ratio):
        sample = list()
        n_sample = round(len(dataset) * ratio)  # round() 方法返回浮点数x的四舍五入值。
        while len(sample) < n_sample:
            # ，此则自助采样法。从而保证每棵决策树训练集的差异性
            index = randrange(len(dataset))
            sample.append(dataset[index])
        return sample


​    
    # 随机森林算法
    # train           训练数据集
    # test            测试数据集
    # max_depth       决策树深度不能太深，不然容易导致过拟合
    # min_size        叶子节点的大小
    # sample_size     训练数据集的样本比例
    # n_trees         决策树的个数
    # n_features      选取的特征的个数
    def evaluate_rf_effect(train, test, max_depth, min_size, sample_ratio, tree_num, n_features):
        trees = list()
        for i in range(tree_num):
            # 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性
            sample = sub_sample(train, sample_ratio)
            # 创建一个决策树
            tree = build_tree(sample, max_depth, min_size, n_features)
            trees.append(tree)
        # OK，现在已经得到这些树了，我们使用这些树，对每一行进行预测
        predictions = [bagging_predict(trees, row) for row in test]
        return predictions


​    
    # 根据预测值和实际值，计算精确度
    def calc_accuracy(actual, predicted):
        correct = 0
        for i in range(len(actual)):
            if actual[i] == predicted[i]:
                correct += 1
        return correct / float(len(actual)) * 100.0


​    
​    
    if __name__ == '__main__':
        # 加载数据
        dataset = load_data_set('sonar-all-data.txt')
        # 准备样本数据
        # 将数据集采样为 n_folds 份，其中一个 fold 作为测试集，其余作为训练集，这样可以有 n_folds 套训练和测试样本
        # 遍历整个 folds ，实现交叉验证
        # 数据可以重复重复抽取，每一次 list 的元素是无重复的
        n_folds = 5  # 准备将样本数据分成5份，来进行交叉验证
        folds = cross_validation_split(dataset, n_folds)
        train_sets = []
        test_sets = []
        for fold in folds:
            train_set = list(folds)
            train_set.remove(fold)
            train_set = copy.deepcopy(sum(train_set, []))  # 这也可以，将4个folds合并成一个集合
            test_set = copy.deepcopy(fold)
            train_sets.append(train_set)
            test_sets.append(test_set)
    
        max_depth = 15  # 调参（自己修改） #决策树深度不能太深，不然容易导致过拟合
        min_size = 1  # 决策树的叶子节点最少的元素数量
        sample_ratio = 0.8  # 做决策树时候的样本的比例
        n_features = int(
            math.sqrt(len(dataset[0]) - 1))  # 需要自己调整，准确性与多样性之间的权衡，为什么n_features 要等于 int(sqrt((dataset[0])-1))？ 有什么说法吗？
        print(n_features)
        for n_trees in [1, 5, 10, 20]:  # 理论上树是越多越好   , 5,10, 20
            scores = list()
            for i in range(n_folds):
                predicted = evaluate_rf_effect(train_sets[i], test_sets[i],
                                               max_depth, min_size, sample_ratio, n_trees, n_features)
                actual = [row[-1] for row in test_sets[i]]
                print(predicted)
                print(actual)
                # 计算随机森林的预测结果的正确率
                accuracy = calc_accuracy(actual, predicted)
                scores.append(accuracy)
            seed(1)
            print('Trees: %d' % n_trees)
            print('Scores: %s' % scores)
            print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))



输出如下：


    7
    [1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]
    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]
    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
    [1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
    [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
    [0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1]
    [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    Trees: 1
    Scores: [57.14285714285714, 64.28571428571429, 57.14285714285714, 54.761904761904766, 73.80952380952381]
    Mean Accuracy: 61.429%
    [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]
    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]
    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
    [1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]
    [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
    [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]
    [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
    [1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
    [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    Trees: 5
    Scores: [73.80952380952381, 73.80952380952381, 61.904761904761905, 78.57142857142857, 80.95238095238095]
    Mean Accuracy: 73.810%
    [0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1]
    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
    [0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
    [1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]
    [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
    [1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]
    [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
    [0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    Trees: 10
    Scores: [80.95238095238095, 78.57142857142857, 61.904761904761905, 78.57142857142857, 88.09523809523809]
    Mean Accuracy: 77.619%
    [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]
    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
    [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]
    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
    [1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]
    [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
    [1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]
    [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
    [1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
    Trees: 20
    Scores: [85.71428571428571, 78.57142857142857, 69.04761904761905, 83.33333333333334, 88.09523809523809]
    Mean Accuracy: 80.952%


嗯，还是有几个问题要说一下的：


## 几个参数的说明与调整






  * max_depth 我现在调到15，调到20的时候反而没有15的时候效果好，**那么实际中到底怎么确定哪个参数好呢？**

  * sample_ratio 我用的是0.8，也就是说每棵树在生成的时候是根据train中的80%的样本生成的。

  * n_features 这里我用的是 math.sqrt(len(dataset[0]) - 1)) **没明白为什么要用这个？**

  * **min_size 会有设置为不为1 的时候吗？**




## 对于一颗树来说，每次split的时候都要重新选择 n_features 个 feature吗？


之前，我以为，随机森林的每棵树在选择特征集的时候，只是在开始的时候选择一些特征，然后后面的split就用这些特征，但是这个代码看下来好像不是这样，而是每次的split，都是从完整特征集里面重新选出n_features个特征。**为什么要这样做呢？**

不过，我试了下，如果只是开始的时候选一些特征，然后后面就用这些，那么最后的accuracy基本上就是60%左右，而且上不去了，如果像上面这样的话，就可以随着tree_num的增加一直增加accuracy。**嗯，但是还是不清楚，这样做有什么道理呢？**


## 判断一个string是不是一个数的时候，怎么判断？


之前程序里用的是 a.isdigits() 来判断 a 里面是不是每个字符都是数字，但是这样对于 0.02 这样的float 会判断为False，因为有小数点。因此这里直接用 try 来进行转换。


## list里面的item也是list的时候，怎么拷贝？


这个，用 import copy    b=copy.deepcopy(a) 就可以，其它的比如[:] ，list(a) 什么的都不能将里面的list也进行拷贝。


## 标签的名字应该怎么取？用 class_value 还是用 labels？


**这个一直想知道，我这里用的是labels，但是到底应该用什么？**







* * *





# COMMENT



