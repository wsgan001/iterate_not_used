


# ORIGINAL






  1. 《解析卷积神经网络》魏秀参




# TODO






  * aaa





* * *





# INTRODUCTION






  * aaa













卷积神经网

尽管卷

的效果

备的应 亿3千 （FLoati

大的存

用。






















































自然语言处理等领域均取得了出类拔萃

得诸多实际应用（特别是基于嵌入式设

L6网络［74］为例，其参数数量达到了 1 :储空间，需要进行309亿次浮点运算1

才能完成一张图像的识别任务。如此巨

了深度网络在移动端等小型设备上的应


















移到云端，但对于一些高实时性计算场 宽、延迟和全时可用性均面临着严峻的挑战，从而无法替 些场景下的设备往往并不具备超高的计算性能。因此，对 言，尽管深度学习带来了巨大的性能提升，却因计算瓶颈










:研究表明，深度神经网络面临着严峻的过参数化（over-


积层和全连接层的浮点运算为准，不包含其他计算开销。本书将一次向量相乘视作两次浮

（乘法与加法），但在部分文献中以向量相乘作为基本浮点操作，因此在数值上可能存在2


parameterization）-模型内部参数存在着巨大的冗余。如Denil等人［15］发

现，只给定很小一部分的参数子集（约全部参数量的5%），便能完整地重构出 剩余的参数，从而揭示了模型压缩的可行性。需要注意的是，这种冗余在模型

训练阶段是十分必要的。因为深度神经网络面临的是一个极其复杂的非凸优化

问题，对于现有的基于梯度下降的优化算法而言，这种参数上的冗余保证了网

络能够收敛到一个比较好的最优值［16,39］。因而在一定程度上，网络越深，参

数越多，模型越复杂，其最终的效果也往往越好。

鉴于此，神经网络的压缩2逐渐成为当下深度学习领域的热门研究课题。研

究者们提出了各种新颖的算法，在追求模型高准确度的同时，尽可能地降低其

复杂度，以期达到性能与开销上的平衡。

总体而言，绝大多数的压缩算法，均旨在将一个庞大而复杂的预训练模型 （pre-trained model）转化为一个精简的小模型。当然，也有研究人员试图设计

出更加紧凑的网络结构，通过对新的小模型进行训练来获得精简模型。从严格

意义上来讲，这种算法不属于网络压缩的范畴，但本着减小模型复杂度的最终

目的，我们也将其归纳到本章的介绍内容中来。

按照压缩过程对网络结构的破坏程度，我们将模型压缩技术分为“前端压 缩”与“后端压缩”两部分。所谓“前端压缩”，是指不改变原网络结构的压缩 技术，主要包括知识蒸馏、紧凑的模型结构设计以及滤波器（filter）层面的剪 枝等；而“后端压缩”则包括低秩近似、未加限制的剪枝、参数量化以及二值 网络等，其目标在于尽可能地减少模型大小，因而会对原始网络结构造成极大 程度的改造。其中，由于“前端压缩”未改变原有的网络结构，仅仅只是在原 模型的基础上减少了网络的层数或者滤波器的个数，其最终的模型可完美适配 现有的深度学习库，如Cafe ［47］等。相比之下，“后端压缩”为了追求极致的 压缩比，不得不对原有的网络结构进行改造，如对参数进行量化表示等，而这 样的改造往往是不可逆的。同时，为了获得理想的压缩效果，必须开发相配套 的运行库，甚至是专门的硬件设备，其最终的结果往往是一种压缩技术对应于 一套运行库，从而带来了巨大的维护成本。

本书所提及的压缩，不仅仅指体积上的压缩，也包括时间上的压缩，其最终目的在于减少模型

当然，上述两种压缩策略并不存在绝对的好坏，各种方法均有其各自的适应

场景。同时，两种压缩技术可以相互结合，将“前端压缩”的输出作为“后端

压缩”的输入，能够在最大程度上减少模型的复杂度。因此，本章所介绍的这

两大类压缩技术，实际上是一种相互补充的关系。有理由相信，“前端压缩”与

后端压缩”的结合能够开启深度模型走向“精简之美”的大门。

4.1低秩近似
卷积神经网络的基本计算模式是进行卷积运算。具体实现上，卷积操作由矩阵 相乘完成，如Caffe ［47］中的“im2col”操作。不过通常情况下，权重矩阵往往

稠密且巨大，从而带来计算和存储上的巨大开销。为解决这种情况的一种直观

想法是，若能将该稠密矩阵由若干个小规模矩阵近似重构出来，那么便能有效

降低存储和计算开销。由于这类算法大多采用低秩近似的技术来重构权重矩阵，

我们将其归类为低秩近似算法。

例如，给定权重矩阵W G Rmxn，若能将其表示为若干个低秩矩阵的 组合，即W = m=i aiMi，其中风G Rmxn为低秩矩阵，其秩为ri,并 满足ri《min（m,n）,则每一个低秩矩阵都可分解为小规模矩阵的乘积, Mi = GiHT,其中，GiG Rmxri, Hi G Rnxr、当n的取值很小时，便能大

幅降低总体的存储和计算开销。

基于以上想法，Sindhwani等人［75］提出使用结构化矩阵来进行低秩分解的 算法。结构化矩阵是一系列拥有特殊结构的矩阵，如Toeplitz矩阵，该矩阵的 特点是任意一条平行于主对角线的直线上的元素都相同。他们使用Toeplitz矩 阵来近似重构原权重矩阵：W = aiTiT-1十a2T3T-1T5 ,此情况下W和T 为方阵。而每一个Toeplitz矩阵T都可以通过置换操作（displacement action, 如使用Sylvester替换算子）转化为一个非常低秩（例如秩小于等于2）的矩阵。 但该低秩矩阵与原矩阵并不存在直接的等价性,为了保证两者之间的等价性, 还需借助于一定的数学工具（如Krylov分解）以达到使用低秩矩阵来重构原结 构化矩阵的目的,从而减少了存储开销。计算方面,得益于其特殊的结构,可 使用快速傅里叶变换以实现计算上的加速。最终,这样一个与寻常矩阵相乘截

4.2.剪枝与稀疏约束

然不同的计算过程，在部分小数据集上能够达到2~3倍的压缩效果，而最终 的精度甚至能超过压缩之前的网络。

另外一种比较简便的做法是直接使用矩阵分解来降低权重矩阵的参数。如

Denton等人［16］提出使用奇异值分解(Singular Value Decomposition,简称 SVD)分解来重构全连接层的权重。其基本思路是先对权重矩阵进行SVD分 解：w = usvT,其中Ue Rmxm, Se Rmxn, v e Rnxn。根据奇异值矩阵 s中的数值分布情况，可选择保留下前k个最大项。于是，可通过两个矩阵相 乘的形式来重构原矩阵，即W«(U§)VT。其中，ue Rmxk, s e Rkxk，两


者的乘积作为第一个矩阵的权重。▽ e Rk: 对于一个三阶张量(卷积层的一个滤波器) 如对于一个秩为1的3阶张量w e Rmx ||W - a ® P ® yIIf，其中 a e Rm, P e R 上述思想可轻松拓展至秩为 K 的情况：给 得(a,P,Y),重复K次更新计算以减少重构 最终的近似矩阵由这若干向量外积的结果相
























W(
























，利用同样的算法获 卜 Wk — a ® P ® y。


(4+1)

结合一些其他技术，利用矩阵分解能够将卷积层压缩2~3倍，全连接层压缩 5〜13倍，速度提升2倍左右，而精度损失则被控制在了 1%之内。

低秩近似算法在中小型网络模型上取得了很不错的效果，但其超参数量与网 络层数呈线性变化趋势［16,81］,随着网络层数的增加与模型复杂度的提升，其 搜索空间会急剧增大［86］。当面对大型神经网络模型时，是否仍能通过近似算 法来重构参数矩阵，并使得性能下降保持在一个可接受范围内？最终的答案还 是有待商榷的。



剪枝，作为模型压缩领域中的一种经典技术，已经被广泛运用到各种算法的后 处理中，如著名的C4.5决策树算法［70］。通过剪枝处理，在减小模型复杂度 的同时，还能有效防止过拟合，提升模型泛化性。剪枝操作可类比于生物学上 大脑神经突触数量的变化情况。很多哺乳动物在幼年时，其脑神经突触的数量 便已达到顶峰，随着大脑发育的成熟，突触数量会随之下降。类似的，在神经 网络的初始化训练中，我们需要一定冗余度的参数数量来保证模型的可塑性与 “容量”(capacity),而在完成训练之后，则可以通过剪枝操作来移除这些冗余 参数，使得模型更加成熟。

早在1990年，LeCun等人［55］便已尝试将剪枝运用到神经网络的处理中 来，通过移除一些不重要的权重，能够有效加快网络的速度，提升泛化性能。他 们提出了一种名为“最佳脑损伤” (Optimal Brain Damage, OBD)的方法来 对神经网络进行剪枝。随后，更多剪枝算法在神经网络中的成功应用也再次证 明了其有效性［5,33］。然而，这类算法大多基于二阶梯度来计算各权重的重要 程度，处理小规模网络尚可，当面对现代大规模的深度神经网络时，便显得捉 襟见肘了。

进人到深度学习时代后，如何对大型深度神经网络进行高效的剪枝，成为了

一个重要的研究课题。研究人员提出了各种有效的剪枝方案，尽管各算法的具

体细节不尽相同，但所采用的基本框架却是相似的。给定一个预训练好的网络

模型，常用的剪枝算法一般都遵从如下的操作流程：

1.    衡量神经元的重要程度——这也是剪枝算法中最重要的核心步骤。根据剪 枝粒度(granularity)的不同，神经元的定义可以是一个权重连接，也可 以是整个滤波器。衡量其重要程度的方法也是多种多样，从一些基本的启 发式算法，到基于梯度的方案，其计算复杂度与最终的效果也是各有千秋。

2.    移除掉一部分不重要的神经元。根据上一步的衡量结果，剪除掉部分神经 元。这里可以根据某个阈值来判断神经元是否可以被剪除，也可以按重要 程度排序，剪除掉一定比例的神经元。一般而言，后者比前者更加简便， 灵活性也更高。

3.    对网络进行微调。由于剪枝操作会不可避免地影响网络的精度，为防止对 分类性能造成过大的破坏，需要对剪枝后的模型进行微调。对于大规模图 像数据集(如ImageNet ［73］)而言，微调会占用大量的计算资源。因此,

4.2.剪枝与稀疏约束

对网络微调到什么程度,也是一件需要斟酌的事情。

4.返回第1步，进行下一轮剪枝。

基于如上循环剪枝框架，Han等人［30］提出了一个简单而有效的策略。 他们首先将低于某个阈值的权重连接全部剪除。 他们认为如果某个连接 (connectivity)的权重值过低，则意味着该连接并不十分重要，因而可以被移 除。之后对剪枝后的网络进行微调以完成参数更新。如此反复迭代,直到在性 能和规模上达到较好的平衡。最终,在保持网络分类精度不下降的情况下,可 以将参数数量减少9 ~ 11倍。在实际操作中，还可以借助li或者I2正则化, 以促使网络的权重趋向于零。

该方法的不足之处在于,剪枝后的网络是非结构化的,即被剪除的网络连接 在分布上没有任何连续性。这种随机稀疏的结构，导致了 CPU高速缓存(CPU cache)与内存之间的频繁切换，从而制约了实际的加速效果。另一方面，由于 网络结构的改变,使得剪枝之后的网络模型极度依赖于专门的运行库,甚至需 要借助于特殊的硬件设备,才能达到理论上的加速比,严重制约了剪枝后模型 的通用性。

基于此，也有学者尝试将剪枝的粒度提到滤波器级别［56,68］,即直接丢弃整 个滤波器。这样一来,模型的速度和大小均能得到有效的提升,而剪枝后网络 的通用性也不会受到任何影响。这类算法的核心在于如何衡量滤波器的重要程 度,通过移除掉“不重要”的滤波器来减少对模型准确度的破坏。其中,最简 单的一种策略是基于滤波器权重本身的统计量,如分别计算每个滤波器的 li 或 12值，将相应数值的大小来作为重要程度的衡量标准。这类算法以［56］为代表, Li等人将每个滤波器权重的绝对值相加作为最终分值：si = £ |W(i,:,:, :)|。基 于权重本身统计信息的评价标准,很大程度上是出于小权重值滤波器对于网络 的贡献相对较小的假设,虽然简单易行,却与网络的输出没有直接关系。在很 多情况下,小权重值对于损失函数也能起到非常重要的影响。当采用较大的压 缩率时,直接丢弃这些权重将会对网络的准确度造成十分严重的破坏,从而很 难恢复到原先的性能。

因此,由数据驱动的剪枝似乎是更合理的方案。最简单的一种策略是根据网

络输出中每一个通道(channel)的稀疏度来判断相应滤波器的重要程度［42］。 其出发点在于，如果某一个滤波器的输出几乎全部为0,那么该滤波器便是冗 余的，移除掉这样的滤波器不会带来很大的性能损失。但从本质上而言，这种 方法仍属启发式算法，只能根据实验效果来评价其好坏。如何对剪枝操作进行 形式化描述与推理，以得到一个更加理论化的选择标准，成为了下一步亟待解 决的问题。对此，Molchanov等人［68］给出的方案是计算每一个滤波器对于损 失函数(loss function)的影响程度。如果某一滤波器的移除不会带来很大的损 失变化，那么自然可以安全地移除该滤波器。但直接计算损失函数的代价过于 庞大，为此，Molchanov等人使用Taylor展开式来近似表示损失函数的变化, 以便衡量每一个滤波器的重要程度。

与此同时，利用稀疏约束来对网络进行剪枝也成为了一个重要的研究方向。 稀疏约束与直接剪枝在效果上有着异曲同工之妙，其思路是在网络的优化目标 中加入权重的稀疏正则项，使得训练时网络的部分权重趋向于 0，而这些 0 值 元素正是剪枝的对象。因此，稀疏约束可以被视作为动态的剪枝。相对于剪枝

的循环反复操作，稀疏约束的优点显而易见：只需进行一遍训练，便能达到网

络剪枝的目的。这种思想也被运用到Han等人［30］的剪枝算法之中，即利用 li,l正则化来促使权重趋向于0。

针对非结构化稀疏网络的缺陷，也有学者提出结构化的稀疏训练策略，有效 提升了网络的实际加速效果，降低了模型对于软、硬件的依赖程度［53,86］。结 构化稀疏约束可以视作是连接级别(connectivity level)的剪枝与滤波器级别 (filter level)的剪枝之间的一种平衡。连接级别的剪枝粒度太细，剪枝之后带来 的非结构化稀疏网络很难在实际应用中得到广泛使用。而滤波器层面的剪枝粒 度则太过粗放，很容易造成精度的大幅降低，同时保留下来的滤波器内部还存 在着一定的冗余。而结构化的稀疏训练方法以滤波器、通道(channel)、网络深 度作为约束对象，将其添加到损失函数的约束项中，可以促使这些对象的数值 趋向于0。例如，Wen等人［86］定义了如下的损失函数：

L / Ni    \    L / Ci    \

E(W) = Ed(W)+ An    £ IIwn!):,:,:||s + Ac • £ £ IIW:(C),:,:III小

l=1 ni=1    l=1 ci=1

(4+2)


表4+1:不同剪枝算法在ImageNet数据集［73］上的性能比较。其中，“参数数 量”和浮点运算次数“FLOPS”汇报了相对原始模型的压缩比例。


方法

网络模型

Top-1精度

Top-5精度

八肀,、r" M

FLOPs

备注

参数数量

Han 等人[30]

VGG-16

+0.16%

+0.44%

13x

5x

随机稀疏的结构难以应用

APoZ [42]

VGG-16

+ 1.81%

+ 1.25%

2.70X

Mix

只减少了参数数量

Taylor-1 [68]

VGG-16

-

-1.44%

1x

2.68x

关注卷积层的剪枝来加快速度

Taylor-2 [68]

VGG-16

-

-3.94%

Mix

3.86x

Weight sum [56]

ResNet-34

-1.06%

-

1.12x

1.32x

ResNet网络冗余度低，更难剪枝

Lebedev 等人[53]

Alex-Net

-1.43%

-

3.23x

3.2x

依赖于特定运行库

SSL [86]

Alex-Net

-2.03%

-

-〜

3.1x

依赖于特定运行库

2


其中， ED(W) 表示原来的损失函数， L 表示 表示的是群Lasso。公式4.2在原损失函数［










约束，促使这些对象整体趋向于0。由于结构化约束改变了网络结构(所得到 的每一层的权重不再是一个完整的张量)，在实际应用时，仍然需要修改现在的 运行库以便支持新的稀疏结构。


总体而言，剪枝是一项有效减小模型复杂度的通用压缩技术，其关键之处在 于如何衡量个别权重对于整体模型的重要程度。在这个问题上，各种权重选择 策略也是众说纷纭、莫衷一是，尤其是对于深度学习而言，几乎不可能从理论 上确保某一选择策略是最优的。另一方面，由于剪枝操作对网络结构的破坏程 度极小，这种良好的特性往往被当做网络压缩过程的前端处理。将剪枝与其他 后端压缩技术相结合，能够达到网络模型的最大程度压缩。最后，表4.1总结了 以上几种剪枝方案在ImageNet数据集上的效果。


相比于剪枝操作，参数量化则是一种常用的后端压缩技术。所谓“量化”，是

指从权重中归纳出若干“代表”，由这些“代表”来表示某一类权重的具体数


值。“代表”被存储在码本(codebook)之中，而原权重矩阵只需记录各自“代 表”的索引即可,从而极大地降低了存储开销。这种思想可类比于经典的词包 模型(bag-of-words model,如图 3.2)。

其中，最简单也是最基本的一种量化算法便是标量量化(scalar quantization)。 该算法的基本思路是，对于每一个权重矩阵WG Rmxn，首先将其转化为向量 形式：WG R1xmn。之后对该权重向量的元素进行k个簇的聚类，这可借助于 经典的k-均值(k-means)聚类算法快速完成：

argmjn    ||wi-cj||2.    (4.3)

ij

如此一来，只需将k个聚类中心(Cj,标量)存储在码本之中便可，而原权 重矩阵则只负责记录各自聚类中心在码本中的索引。如果不考虑码本的存储开 销，该算法能将存储空间减少为原来的log2(k)/32。基于k-均值算法的标量量 化尽管简单，在很多应用中却非常有效。Gong等人［28］对比了不同的参数量 化方法,发现即便采用最简单的标量量化算法,也能在保持网络性能不受显著 影响的情况下，将模型大小减少8至16倍。其不足之处在于，当压缩率比较大 时很容易造成分类精度的大幅下降。

在文献［29］中，Han等人便采用了标量量化的思想。如图4.1所示，对于当 前权重矩阵，首先对所有的权重值进行聚类，取k = 4,可获得4个聚类中心, 并将其存储在码本之中,而原矩阵只负责记录相应的索引。对于4个聚类中心 而言,只需 2 个比特位即可,从而极大地降低了存储开销。由于量化会在一定 程度上降低网络的精度，为了弥补性能上的损失，Han等人借鉴了网络微调思 想,利用后续层的回传梯度对当前的码本进行更新,以降低泛化误差。其具体 过程如图4.1所示，首先根据索引矩阵获得每一个聚类中心所对应的梯度值。将 这些梯度值相加,作为每一个聚类中心的梯度,最后利用梯度下降对原码本中 存储的聚类中心进行更新。当然,这种算法是近似的梯度下降,其效果十分有 限,只能在一定程度上缓解量化所带来的精度损失。

为了避免标量量化能力有限的弊端,也有很多算法考虑结构化的向量量化方 法。其中最常用的一种算法是乘积量化(Product Quantization, PQ)。该算法 的基本思路是先将向量空间划分为若干个不相交的子空间,之后依次对每个子

4.3.参数量化

原权重矩阵

32位浮点数)



-0.03

-0.01

0.03

0.02

-0.01

0.01

-0.02

0.12

-0.01

0.02

0.04

0.01

-0.07

-0.02

0.01

-0.02






图4+1:参数量化与码本微调示意图。上：对权重矩阵进行k均值聚类，得到量 化索引与码本；下：使用回传梯度对码本进行更新 空间执行量化操作。即先按照列方向(行方向亦可)将权重矩阵W划分为s 个子矩阵：WiG Rmx(n/s),之后对的每一行进行聚类：

(4+4)

其中，wZ G R1x(n/s)表示子矩阵的第z行，cj为其对应的聚类中心。最 后，依据标量量化的流程，将原权重矩阵转化为码本的索引矩阵。相对于标 量量化而言，乘积量化考虑了更多空间结构信息，具有更高的精度和鲁棒性。 但由于码本中存储的是向量，所占用的存储空间不可忽略，因此其压缩率为

(32 mn )

Wu等人［90］以此为基础，设计了一种通用的网络量化算法：QCNN (quantized CNN)。由于乘积量化只考虑了网络权重本身的信息，与输人输出无 直接关联。这很容易造成量化误差很低，但网络的分类性能却很差的情况。为 此，Wu等人认为，最小化每一层网络输出的重构误差，比最小化该层参数的

化误差更加有效，即考虑如下优化问题：

min

D(s)},{B(s)}


El|On-E(D(s)B(s))TI(s)|F,


(4+5)


其中，In与On分别表示第n张图片在某一层的输人与输出，s表示当前量化 的第 s 个子矩阵空间， D(s) 与 B(s) 为其所对应的码本与索引，即由两者的乘 积来近似表示该子矩阵的权重。对于当前欲量化的子空间 s 而言，其优化目标 在于求得新的码本D(s)与索引B(s)使得重构误差最小化，即：

arg D^sJ^ |On -(




(4+6)


对于当前的子空间 s 而言，上式前两项为固定值。因此，可固定索引 B(s) 来优 化码本D(s),这可通过最小二乘法来实现。同理也可固定D(s)来求B(s)。如 此循环迭代每一个子空间，直到最终的重构误差达到最小。在实际操作中，由 于网络是逐层量化的，对当前层完成量化操作之后，势必会使得精度有所下降。 这可通过微调后续若干层网络来弥补损失，使得网络性能下降尽可能地小。对 于VGG-16而言，该方法能将模型FLOP减少4.06倍，体积减少20.34倍，而 网络的top-5精度损失仅为0+58%。

以上所介绍的基于聚类的参数量化算法，其本质思想是将多个权重映射到

同一个数值，从而实现权重共享，降低存储开销的目的。权重共享是一项十分

经典的研究课题，除了用聚类中心来代替该聚类簇的策略外，也有研究人员考

虑使用哈希技术(hashing)来达到这一目的。在文献［6］中，Chen等人提出 了 HashedNets算法来实现网络权重共享。该算法有两个关键性的元素：码本 cl与哈希函数hl(i,j)。首先，选择一个合适的哈希函数，该函数能够将第I层 的权重位置(i,j)映射到一个码本索引：Wj = chi(ij)。第I层(i,j)位置上的 权重值由码本中所对应的数值来表示。即，所有被映射到同一个哈希桶(hash bucket)中的权重共享同一个参数值。而整个网络的训练过程与标准的神经网 络大致相同，只是增加了权重共享的限制。

综合来看，参数量化作为一种常用的后端压缩技术，能够以很小的性能损失

实现模型体积的大幅下降。其不足之处在于，量化后的网络是“固定”的，很

难再对其做任何改变。另一方面，这一类方法的通用性较差，往往是一种量化

方法对应于一套专门的运行库，造成了较大的维护成本。

4.4.二值网络

4.4二值网络
二值网络可以被视为量化方法的一种极端情况：所有参数的取值只能是±1。正 是这种极端的设定,使得二值网络能够获得极大的压缩效益。首先,在普通的 神经网络中,一个参数是由单精度浮点数来表示的,参数的二值化能将存储开 销降低为原来的 1/32。其次,如果中间结果也能二值化的话,那么所有的运算 仅靠位操作便可完成。借助于同或门(XNOR gate)等逻辑门元件便能快速完 成所有的计算。而这一优点是其余压缩方法所不能比拟的。深度神经网络的一 大诟病就在于其巨大的计算代价,如果能够获得高准确度的二值网络,那么便 可摆脱对GPU等高性能计算设备的依赖。

事实上,二值网络并非网络压缩的特定产物,其历史最早可追溯到人工神经 网络的诞生之初。早在I943年，神经网络的先驱Warren McCulloch和Walter Pitts ［65］两人基于数学和阈值逻辑算法提出的人工神经元模型便是二值网络的 雏形。纵观其发展史,缺乏有效的训练算法一直是困扰二值网络的最大障碍。 即便是在几年前，二值网络也只能在手写数字识别(MNIST)等小型数据集上 取得一定的准确度,距离真正的可实用性还有很大的距离。直到近两年,二值 网络在研究上取得了可观的进展,才再次引发了人们的关注。

现有的神经网络大多基于梯度下降来训练,但二值网络的权重只有±1,无 法直接计算梯度信息，也无法进行权重更新。为了解决这个问题，Courbariaux 等人［11］提出了二值连接(binary connect)算法。该算法退而求其次，采用单 精度与二值相结合的方式来训练二值神经网络：网络的前向与反向回传是二值 的,而权重的更新则是对单精度权重进行的,从而促使网络能够收敛到一个比 较满意的最优值。在完成训练之后,所有的权重将被二值化,从而获得二值网 络体积小、运算快的优点。

网络二值化首先需要解决两个基本问题：

1.如何对权重进行二值化？权重二值化，通常有两种选择：一是直接根据权 重的正负进行二值化：xb = sign(x); 一是进行随机的二值化，即对每一 个权重,以一定的概率取 +1。在实际过程中,随机数的产生会非常耗时, 因此,第一种策略更加实用。

2.如何计算二值权重的梯度？由于二值权重的梯度为0,无法进行参数更 新。为了解决这个问题，需要对符号函数进行放松，即用Htanh(x)= max(-1,min(1,x))来代替sign(x)。当x在区间［-1,1］时，存在梯度值 1，否则梯度为 0。

在模型的训练过程中，存在着两种类型的权重，一是原始的单精度权重，二 是由该单精度权重得到的二值权重。在前向过程中，首先对单精度权重进行二 值化，由二值权重与输人进行卷积运算(实际上只涉及加法)，获得该层的输出。 在反向更新时，则根据放松后的符号函数，计算相应的梯度值，并根据该梯度 值对单精度的权重进行参数更新。由于单精度权重发生了变化，所对应的二值 权重也会有所改变，从而有效解决了二值网络训练困难的问题。在MNIST3与 CIFAR-104等小型数据集上，该算法能够取得与单精度网络相当的准确度，甚 至在部分数据集上超过了单精度的网络模型。这是因为二值化对权重和激活值 添加了噪声，这些噪声具有一定的正则化作用，能够防止模型过拟合。

但二值连接算法只对权重进行了二值化，网络的中间输出值仍然是单精度 的。于是，Hubara等人［44］对此进行了改进，使得权重与中间值同时完成二值 化，其整体思路与二值连接大体相同。与二值连接相比，由于其中间结果也是 二值化的，可借助于同或门等逻辑门元件快速完成计算，而精度损失则保持在 了 0.5% 之内。 (

更进一步，Rastegari等人［71］提出用单精度对角阵与二值矩阵之积来近似 表示原矩阵的算法，以提升二值网络的分类性能，弥补纯二值网络在精度上的 弱势。该算法将原卷积运算分解为如下过程：

I W    (I B)a,

(4+7)






RCXWinXhin为该层的输人张量，WG RCXWXh为该层的一个滤波器， (W) G {+1,-1}cXwXh为该滤波器所对应的二值权重。Rastegari等人 靠二值运算，很难到达原单精度卷积运算的效果。因此，他们使用额 "单精度放缩因子a G R+来对该二值滤波器卷积后的结果进行放缩。

Lttp://yann.lecun.com/exdb/mnist/

Lttps://www.cs.toronto.edu/~kriz/cifar.html

4.5.知识蒸馏

而关于a的取值，则可根据优化目标：

min ||W - aB||2,    (4.8)

得到a=n||W||ii。整个网络的训练过程与上述两个算法大体相同，所不同之 处在于梯度的计算过程还考虑了 a 的影响。从严格意义上来讲，该网络并不是 纯粹的二值网络，每个滤波器还保留了一个单精度的缩放因子。但正是这个额 外的单精度数值，有效降低了重构误差，并首次在ImageNet数据集上取得了 与Alex-Net [52]相当的精度。此外，如果同时对输人与权重都进行二值化，则 可进一步提升运行速度，但网络性能会受到明显的影响，在ImageNet上其分 类精度降低了 12+6%。

尽管二值网络取得了一定的技术突破，但距离真正的可实用性之间还有很长

一段路要走。但我们有理由相信，随着技术的进步与研究的深人，其未来的发

展前景将会更加美好。

4.5知识蒸馏
对于监督学习而言，监督信息的丰富程度在模型的训练过程中起着至关重要的 作用。同样复杂度的模型，给定的监督信息越丰富，训练效果也越好。正如我 们在本章开篇所言，参数的冗余能够在一定程度上保证网络收敛到一个较好的 最优值。那么，在不改变模型复杂度的情况下，通过增加监督信息的丰富程度， 是否也能带来性能上的提升呢？答案是肯定的。正是本着这样的思想，“知识蒸 馏” (knowledge distillation)应运而生。

所谓“知识蒸馏”，其实是迁移学习(transfer learning)的一种，其最终目 的是将一个庞大而复杂的模型所学到的知识，通过一定的技术手段迁移到精简 的小模型上，使得小模型能够获得与大模型相近的性能。这两种不同规模的网 络，分别扮演着“学生”和“老师”的角色：如果完全让“学生”(小模型)自 学的话，往往收效甚微；但若能经过一个“老师”(大模型)的指导，学习的过 程便能事半功倍，“学生”甚至有可能超越“老师”。

在知识蒸馏的框架中，有两个基本要素起着决定性的作用：一是何谓“知

识”,即如何提取模型中的知识；一是如何“蒸馏”,即如何完成知识转移的任

务。

Jimmy等人［1］认为，Softmax层的输人与类别标签相比，包含了更加丰 富的监督信息，可以被视作网络中知识的有效概括。Softmax的计算过程为: Pk = ezk/Ej- ezj,其中输人Zj被称为“logits”，使用logits来代替类别标签对 小模型进行训练，可以获得更好的训练效果。他们将小模型的训练问题，转化 为一个回归问题：

L(W, p)=

(4+9)


以促使小模型的输出尽可能地接近大模型的logits。在实验中，他们选择浅层的 小模型来“模仿”深层的大模型(或者多个模型的集成)。然而，为了达到和大 模型相似的精度,小模型中隐藏层的宽度要足够大,因而参数总量并未明显减 少,其效果十分有限。

与此同时，Hinton等人［40］则认为Softmax层的输出会是一种更好的选择, 它包含了每个类别的预测概率,可以被认为是一种“软标签”。通常意义的类别 标签，只给出一个类别的信息，各类之间没有任何关联。而对于Softmax的预 测概率而言,除了该样本的类别归属之外,还包含了不同类别之间的相似信息： 两个类别之间的预测概率越接近,这两类越相似。因此,“软标签”比类别标签 包含了更多的信息。为了获得更好的“软标签”,他们使用了一个超参数来控制 预测概率的平滑程度,即：

(zi/T)

(4+10)


Mzj/T) +

其中，T被称为“温度”，其值通常为1。T的取值越大，所预测的概率分布通 常越平滑。为了获得更高的预测精度，还可使用普通的类别标签来对“软标签” 进行修正。最终的损失函数由两部分构成：第一项是由小模型的预测结果与大 模型的“软标签”所构成的交叉摘(cross entropy);第二项为预测结果与普通 类别标签的交叉熵。两者之间的重要程度可通过一定的权重进行调节。在实际 应用中，T的具体取值会影响最终的效果，一般而言，较大的T能够获得较高 的准确度。当 T 的取值比较恰当时,小模型能够取得与大模型相近的性能,但

4.6.紧凑的网络结构

减少了参数数量，同时训练速度也得到了提升。 “软标签”的不足之处在于，温度 T 的取值不易确定，而 T 对小模型的训练

结果有着较大的影响。另一方面，当数据集的类别比较多时（如人脸识别中的 数万个类别），即“软标签”的维度比较高时，模型的训练变得难以收敛［63］。 针对人脸识别数据集类别维度高的特点，Luo ［63］等人认为，可以使用Softmax 前一层网络的输出来指导小模型的训练。这是因为，Softmax以该层输出为基 础进行预测计算，具有相当的信息量，却拥有更加紧凑的维度（相对于人脸识 别中Softmax层的数万维度而言）。但相比于“软标签”，前一层的输出包含更 多的噪声与无关信息。因此，Luo等人设计了一个算法来对神经元进行选择, 以去除这些无关维度，使得最终保留的维度更加紧凑与高效。该算法的主要思 想是，保留那些满足如下两点要求的特征维度：一是该维度的特征须具有足够 强的区分度；二是不同维度之间的相关性须尽可能低。使用经过选择后的输出 特征来对小模型进行训练，能够获得更好的分类性能，甚至可以超过大模型的 精度。

总体而言，知识蒸馏作为前向压缩算法的一种补充，可以用来更好地指导小

规模网络的训练。但该方法目前的效果还十分有限，与主流的剪枝、量化等技

术相比，存在一定的距离，需要未来研究工作的深人。

4.6紧凑的网络结构
以上所介绍的各种方法，在模型压缩方面均取得了十分优异的效果，能够有效

降低神经网络的复杂度。其实，我们迫切需要大模型的有效压缩策略，很大一

部分原因是出自小模型的训练效果很难令人满意的无奈。但直接训练小模型真

的没法取得很好的精度吗？似乎也不尽然。研究人员设计出了许多更加紧凑的

网络结构，将这些新颖的结构运用到神经网络的设计中来，能够使得模型在规

诚然，网络结构的设计是一门实验推动的研究，需要很大的技巧性。但通过 研究一些比较成熟的设计思想，可以启迪更多可能的研究工作。为了追求更少 的模型参数，Iandola等人［45］设计了一种名为“Fire Module”的基本单元，并

基于这种结构单元提出了 SqueezeNet。“Fire Module”的基本结构如图4.2a所

示，该结构主要分为两部分：




ethods, on 3the othe






(a) Fire Module。

图4.2:两种紧凑网络结构中所采用的的基本模型单元。

1.    “挤压”特征维度的大小对于模型容量有着较大的影响，维度不够高时, 模型的表示能力便会受到限制。但高维的特征会直接导致卷积层参数的急 剧增加。为追求模型容量与参数的平衡，可使用1x1的卷积来对输入特 征进行降维。同时，1 x 1的卷积可综合多个通道的信息，得到更加紧凑 的输入特征，从而保证了模型的泛化性；

2.    “扩张”常见网络模型的卷积层通常由若干个3 x 3的卷积核构成，占用 了大量的计算资源。这里为了减少网络参数，同时也为了综合多种空间结 构信息，使用了部分1x1的卷积来代替3 x 3的卷积。为了使得不同卷 积核的输出能够拼接成一个完整的输出，需要对3 x 3的卷积输入配置合 适的填充(padding)像素；

以该基本单元为基础所搭建的SqueezeNet具有良好的性能，在ImageNet 上能够达到Alex-Net的分类精度，而其模型大小仅仅为4.8MB,将这样规模的 模型部署到手机等嵌入式设备中将变得不再困难。

另一方面，为了弥补小网络在无监督领域自适应(domain adaptation)任 务上的不足，Wu等人［88］认为，增加网络中特征的多样性是解决问题的关键。 他们参考了 GoogLeNet ［80］的设计思想，提出使用多个分支分别捕捉不同层次

4.7.小结

的图像特征，以达到扩充小模型特征多样性的目的。其基本结构如图4.2b所示， 每一条分支都首先用1x 1的卷积对输人特征做降维处理。在这一方面，其设计 理念与SqueezeNet基本相同。三条分支分别为普通的卷积操作(convolution)、 扩张卷积(dilated convolution)与反卷积(deconvolution)。使用扩张卷积的目 的是为了使用较少参数获得较大感受野，用反卷积则是为了重构输人特征，提 供与其他两条分支截然不同的卷积特征。最后，为了减少参数，对每一条分支 均使用了分组卷积。三条分支在最后汇总，拼接为新的张量，作为下一层的输 人。最终的效果是以4+1M的参数数量实现了 GoogLeNet的精度，并且在领域 自适应问题上取得了良好的性能表现。

直接训练一个性能良好的小规模网络模型固然十分吸引人，然而其结构设计

并不是一件容易的事情，这在很大程度上依赖于设计者本身的经验与技巧。另

一方面，随着参数数量的降低，网络的泛化性是否还能得到保障？当一个紧凑

的小规模网络被运用到迁移学习，或者是检测、分割等其他任务上时，其性能

表现究竟如何，也未可知。

4.7小结
§ 本章从“前端压缩”与“后端压缩”两个角度分别介绍了网络模型压缩技 术中的若干算法，这些算法有着各自不同的应用领域与压缩效果；

§ 低秩近似、剪枝与参数量化作为常用的三种压缩技术，已经具备了较为明 朗的应用前景；其他压缩技术，如二值网络、知识蒸馏等尚处于发展阶段。 不过随着深度学习相关技术的迅猛发展，我们有理由相信，将深度学习技 术应用到更便捷的嵌人式设备中将变得不再遥远。











* * *





# COMMENT



