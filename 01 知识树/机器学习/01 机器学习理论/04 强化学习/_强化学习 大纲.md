

# 强化学习介绍

1. [强化学习介绍](http://106.15.37.116/2018/05/17/rl-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%bb%8b%e7%bb%8d/)

2. [强化学习的发展趋势](http://106.15.37.116/2018/05/17/rl-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%8f%91%e5%b1%95%e8%b6%8b%e5%8a%bf/)



# 基础知识

1. [术语和数学符号](http://106.15.37.116/2018/05/17/rl-%e6%95%b0%e5%ad%a6%e7%ac%a6%e5%8f%b7/)

2. 马尔科夫决策过程    [有限马尔科夫决策过程](http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/)

3. 基于模型的动态规划方法    [动态规划](http://106.15.37.116/2018/05/17/5880/)



# 强化学习方法

1. 基于值函数的强化学习方法

     1. 基于蒙特卡罗的强化学习方法

          2. 基于时间差分的强化学习方法

          3. 基于值函数逼近的强化学习方法

1. 基于直接策略搜索的强化学习方法

 2. 基于策略梯度的强化学习方法

             2. 基于置信域策略优化的强化学习方法

             3. 基于确定策略搜索的强化学习方法

             4. 基于引导策略搜索的强化学习方法

1. 强化学系研究及前沿

 2. 逆向强化学习

             2. 组合策略梯度和值函数方法

             3. 值迭代网络

             4. 基于模型的强化学习方法：PILCO及其扩展







机器学习书上的。

1. [任务与奖赏](http://106.15.37.116/2018/05/24/rl-%e4%bb%bb%e5%8a%a1%e4%b8%8e%e5%a5%96%e8%b5%8f/)

2. [K-摇臂赌博机](http://106.15.37.116/2018/05/24/rl-k%e6%91%87%e8%87%82%e8%b5%8c%e5%8d%9a%e6%9c%ba/)

3. [有模型学习](http://106.15.37.116/2018/05/24/rl-%e6%9c%89%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0/)

4. [免模型学习](http://106.15.37.116/2018/05/24/rl-%e5%85%8d%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0/)

5. [值函数近似](http://106.15.37.116/2018/05/24/rl-%e5%80%bc%e5%87%bd%e6%95%b0%e8%bf%91%e4%bc%bc/)

6. [模仿学习](http://106.15.37.116/2018/05/24/rl-%e6%a8%a1%e4%bb%bf%e5%ad%a6%e4%b9%a0/)





1. [多臂老虎机问题](http://106.15.37.116/2018/05/17/5872/)

2. 

3. [ 蒙特卡洛方法(Monte Carlo Methods) ](http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/)

4. [时序差分学习(Temporal-Difference Learning)](http://106.15.37.116/2018/05/17/rl-%e6%97%b6%e5%ba%8f%e5%b7%ae%e5%88%86%e5%ad%a6%e4%b9%a0temporal-difference-learning/)

5. [规划式方法和学习式方法](http://106.15.37.116/2018/05/17/rl-%e8%a7%84%e5%88%92%e5%bc%8f%e6%96%b9%e6%b3%95%e5%92%8c%e5%ad%a6%e4%b9%a0%e5%bc%8f%e6%96%b9%e6%b3%95/)

6. [on-policy预测的近似方法](http://106.15.37.116/2018/05/17/rl-on-policy-%e9%a2%84%e6%b5%8b%e7%9a%84%e8%bf%91%e4%bc%bc%e6%96%b9%e6%b3%95/)

7. [on-policy控制的近似方法](http://106.15.37.116/2018/05/17/rl-on-policy%e6%8e%a7%e5%88%b6%e7%9a%84%e8%bf%91%e4%bc%bc%e6%96%b9%e6%b3%95/)

8. [off-policy的近似方法](http://106.15.37.116/2018/05/17/rl-off-policy%e7%9a%84%e8%bf%91%e4%bc%bc%e6%96%b9%e6%b3%95/)

9. [资格痕迹(Eligibility Traces)](http://106.15.37.116/2018/05/17/rl-%e8%b5%84%e6%a0%bc%e7%97%95%e8%bf%b9eligibility-traces/)

10. [策略梯度方法(Policy Gradient Methods)](http://106.15.37.116/2018/05/17/rl-%e7%ad%96%e7%95%a5%e6%a2%af%e5%ba%a6%e6%96%b9%e6%b3%95policy-gradient-methods/)

11. [心理学](http://106.15.37.116/2018/05/17/rl-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e8%af%bb%e4%b9%a6%e7%ac%94%e8%ae%b0-14-%e5%bf%83%e7%90%86%e5%ad%a6/)

12. [强化学习总结](http://106.15.37.116/2018/05/17/rl-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e6%80%bb%e7%bb%93/)





- 强化学习和DQN



强化学习与深度学习的结合



# 需要补充的







# 需要消化的

课程资源：



- David Silver 2015年的经典课程Teaching ，
- 加州大学伯克利分校2017年Levine, Finn, Schulman的课程 CS 294 Deep Reinforcement Learning, Spring 2017 ，
- 卡内基梅隆大学的2017 春季课程Deep RL and Control 。

