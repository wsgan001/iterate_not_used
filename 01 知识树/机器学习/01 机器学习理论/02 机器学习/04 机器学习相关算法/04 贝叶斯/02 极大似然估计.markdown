

7.2极大似然估计

估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计.具体地，记关于类别 c 的类条件概率为 $P(x|c)$ ，假设 $P(x|c)$ 具有确定的形式并且被参数向量 $\theta_c$ 唯一确定， 则我们的任务就是利用训练集 D 估计参数 $\theta_c$ 。为明确起见，我们将 $P(x|c)$ 记为 $P(x|\theta_c)$ .


事实上，概率模型的训练过程就是参数估计(parameter estimation)过程. 对于参数估计，统计学界的两个学派分别提供了不同的解决方案：

- 频率主义学派(Prequentist)认为参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值；

- 贝叶斯学派(Bayesian)则认为参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布.

本节介绍源自频率主义学派的极大似然估计(Maximum Likelihood Estimation,简称MLE),这是根据数据采样来估计概率分布参数的经典方法.

令 $D_c$ 表示训练集 D 中第 c 类样本组成的集合,假设这些样本是独立同分布的，则参数 $\theta_c$ 对于数据集 $D_c$ 的似然是

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/6JbDJg7cAE.png?imageslim)

对 $\theta_c$ 进行极大似然估计,就是去寻找能最大化似然 $P(D_c|\theta_c)$ 的参数值 $\hat{\theta}_c$ ，直 观上看，极大似然估计是试图在 $\theta_c$ 所有可能的取值中，找到一个能使数据出现的“可能性”最大的值.

式(7.9)中的连乘操作易造成下溢，通常使用对数似然(log-likelihood)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/5kibJale1D.png?imageslim)

此时参数 $\theta_c$ 的极大似然估计 $\hat{\theta}_c$ 为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/mLAE1hcfcI.png?imageslim)


例如，在连续属性情形下，假设概率密度函数 $p(x|c)~\mathcal{N}(\mu_c,\rho_c^2)$ 则参数  $\mu_c$ 和 $\rho_c^2$ 的极大似然估计为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/Cj4g8mF7Ji.png?imageslim)

也就是说，通过极大似然法得到的正态分布均值就是样本均值，方差就是 $(x-\hat{\mu}_c)(x-\hat{\mu}_c)^T$  的均值,这显然是一个符合直觉的结果.在离散属性情形下， 也可通过类似的方式估计类条件概率.

需注意的是，这种参数化的方法虽能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布.在现实应用中，欲做出能较好地接近潜在真实分布的假设，往往需在一 定程度上利用关于应用任务本身的经验知识，否则若仅凭“猜测”来假设概率 分布形式，很可能产生误导性的结果.<span style="color:red;">嗯。</span>



# REF

1. 《机器学习》周志华
