



低维嵌入


上一节的讨论是基于一个重要假设：任意测试样本 x 附近任意小的 $\delta$ 各距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样” (dense sample).然而，这个假设在现实任务中通常很难满足，<span style="color:red;">是的。</span>

例如若 $\delta = 0.001$ ，仅考虑单个属性，则仅需 1000 个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近 0.001 距离范围内总能找到一个训练样本，此时最近邻分类器的错误率不超过贝叶斯最优分类器的错误率的两倍。

然而，这仅是属性维数为 1 的情形，若有更多的属性，则情况会发生 显著变化.例如假定属性维数为20,若要求样本满足密采样条件，则至少需 $(10^3)^{20}=10^{60}$ 个样本。现实应用中属性维数经常成千上万，要满足密采样条件所需的样本数目是无法达到的天文数字。此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大的麻烦，例如当维数很高时甚至连计算内积都不再容易<span style="color:red;">为什么？</span>.


事实上，在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难” (curse of dimensionality).

缓解维数灾难的一个重要途径是降维(dimension reduction),亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间” (subspace),在这个子空间中样本密度大幅提高，距离计算也变得更为容 易。

为什么能进行降维？这是因为在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维 “嵌入” (embedding)。图10.2给出了一个直观的例子.原始高维 空间中的样本点，在这个低维嵌入子空间中更容易进行学习.<span style="color:red;">没看懂这个图，什么是低维嵌入？</span>


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/5L0l17ka9h.png?imageslim)


若要求原始空间中样本之间的距离在低维空间中得以保持，如图10.2所 示，即得到“多维缩放”(Multiple Dimensional Scaling,简称 MDS) 这样一种经典的降维方法。

下面做一个简单的介绍.

假定 m 个样本在原始空间的距离矩阵为 $D\in\mathbb{R}^{m\times m}$ , 其第i行j列的元素 $dist_{ij}$ 为样本 $x_i$ 到 $x_j$ 的距离.我们的目标是获得样本在 $d'$ 维空间的表示 $Z\in \mathbb{R}^{d'\times m}$ ,$d'\leq d$ ,且任意两个样本在 $d'$ 维空间中的欧氏距离等于原始空间中的距离，即  $||z_i-z_j||=dist_{ij}$ 。<span style="color:red;">嗯，这个倒是。</span>


令 $B=Z^TZ\in \mathbb{R}^{m\times m}$ ，其中 B 为降维后样本的内积矩阵，$b_{ij}=z_i^{T}z_j$ ，有

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/hJk6JahE3a.png?imageslim)


为便于讨论，令降维后的样本 Z 被中心化，即 $\sum_{i=1}^{m}z_i=0$ 。显然,矩阵 $B$ 的行与列之和均为零，即 $\sum_{i=1}^{m}b_{ij}=\sum_{j=1}^{m}b_{ij}$ 。易知

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/3F3jk626GF.png?imageslim)

其中 $tr(\cdot )$ 表示矩阵的迹(trace)，$tr(B) =\sum_{i=1}^{m}||z_i||^2$ 。令

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/fLb2BH22iD.png?imageslim)


由式(10.3)和式(10.4)~(10.9)可得

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/5A7G041K2I.png?imageslim)

由此即可通过降维前后保持不变的距离矩阵 D 求取内积矩阵 B .

对矩阵 B 做特征值分解(eigenvalue decomposition)，$B=V\Lambda V^T$ ,其中 $\Lambda =diag(\lambda_1,\lambda_2,\cdots ,\lambda_d)$ 为特征值构成的对角矩阵，$\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_d$ ，$\Lambda$ 为特征向量矩阵.假定其中有 $d^*$ 个非零特征值，它们构成对角矩阵 $\Lambda_*=diag(\lambda_1,\lambda_2,\cdots ,\lambda_{d^*})$，令 $V_*$ 表示相应的特征向量矩阵，则Z可表达为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/3ccILH06di.png?imageslim)

在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近，而不必严格相等.此时可取 $d'\ll d$ 个最大特征值构成对角矩阵 $\widetilde{\Lambda}=diag(\lambda_1,\lambda_2,\cdots ,\lambda_{d'})$,令 $\widetilde{V}$ 表示相应的特征向量矩阵，则 Z 可表达为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/7DjgCKHgjI.png?imageslim)

图10.3给出了 MDS算法的描述.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/2LB3C5BjCj.png?imageslim)


一般来说，欲获得低维子空间，最简单的是对原始高维空间进行线性变换. 给定d维空间中的样本 $X=(x_1,x_2,\cdots ,x_m)\in \mathbb{R}^{d\times m}$ ,变换之后得到 $d'\leq d$ 维 空间中的样本，

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/LH2HhkA6CF.png?imageslim)

其中 $W\in\mathbb{R}^{d\times d'}$ 是变换矩阵， $Z\in \mathbb{R}^{d'\times m}$ 是样本在新空间中的表达.

变换矩阵 W 可视为 $d'$ 个d 维基向量，$z_i=W^Tx_i$ 是第 i 个样本与 $d'$ 个基向量分别做内积而得到的 $d'$ 维属性向量.换言之，$z_i$ 是原属性向量 $z_i$ 在新 坐标系$\{w_1,w_2,\cdots ,w_{d'}\}$ 中的坐标向量.若 $w_i$ 与 $w_j$  ($i\neq j$)正交，则新坐标系是一个正交坐标系，此时 $W$ 为正交变换.显然，新空间中的属性是原空间中 属性的线性组合.

基于线性变换来进行降维的方法称为线性降维方法，它们都符合式(10.13)的基本形式，不同之处是对低维子空间的性质有不同的要求，相当于对 W 施加了不同的约束.在下一节我们将会看到，若要求低维子空间对样本具有最大可分性，则将得到一种极为常用的线性降维方法.

对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高 则认为降维起到了作用.若将维数降至二维或三维，则可通过可视化技术来直观地判断降维效果.<span style="color:red;">嗯。</span>






# REF
1. 《机器学习》周志华
