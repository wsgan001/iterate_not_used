---
author: evo
comments: true
date: 2018-05-23 11:36:50+00:00
layout: post
link: http://106.15.37.116/2018/05/23/ml-%e7%bb%8f%e9%aa%8c%e8%af%af%e5%b7%ae%e4%b8%8e%e8%bf%87%e6%8b%9f%e5%90%88/
slug: ml-%e7%bb%8f%e9%aa%8c%e8%af%af%e5%b7%ae%e4%b8%8e%e8%bf%87%e6%8b%9f%e5%90%88
title: 
wordpress_id: 6343
categories:
- 人工智能学习
tags:
- Machine Learning
---

<!-- more -->

[mathjax]

**注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。**


# ORIGINAL






  1. 《机器学习》周志华




# TODO






  * **并没有很详细的说，看看后面要怎么整合，或者拆分**





* * *





# INTRODUCTION






  * aaa





# 误差




## 错误率、精度


比如说有 m 个样本，其中有 a 个样本分类错误。那么：




  * 错误率 (error rate)：分类错误的样本数占样本总数的比例：E=a/m

  * 精度 (accuracy) ：精度=1 - 错误率：  1 - a/m




## 误差、经验误差、泛化误差


更一般的：




  * 误差：学习器的实际预测输出与样本的真实输出之间的差异。（**这里所说的“误差”均指误差期望，什么意思？**）

  * 训练误差 (training error) 或 经验误差 (empirical error)：学习器在训练集上的误差

  * 泛化误差 (generalization error) ：在新样本上的误差


显然，我们希望得到泛化误差小的学习器。

然而，我们事先并不知道新样本是什么样，因此我们实际能做的只能是努力使经验误差最小化。

在很多情况下，我们是可以学到一个经验误差很小、在训练集上表现很好的学习器的。甚至这个学习器可以对所有训练样本都分类正确，即分类错误率为零，分类精度为100%的学习器，那么这个是不是我们想要的学习器呢？遗憾的是，这样的学习器在多数情况下都不好。


# 过拟合




## 过拟合与欠拟合


我们实际希望的，是在新样本上能表现得很好的学习器。

为了达到这个 目的，应该从训练样本中尽可能学出适用于所有潜在样本的 “普遍规律”，这样才能在遇到新样本时做出正确的判别。

然而，当学习器把训练样本学得 “太好” 了的时候，很可能会把训练样本自身的一些特点作为所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。**嗯。**

这种现象在机器学习中称为 “过拟合”(overfitting)。与 “过拟合” 相对的是 “欠拟合”(underfitting)，这是指对训练样本的一般性质尚未学好。

下图是一个过拟合和欠拟合的一个直观的类比：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b054ed1f3b87.png)





## 过拟合出现的原因


实际上，有多种因素可能导致过拟合，其中最常见的情况就是：由于学习能力过于强大, 以至于把训练样本所包含的不太一般的特性都学到了。（学习能力是否“过于强大”，是由学习算法和数据内涵共同决定的。） **这个只是一个原因吧。**


## 过拟合与欠拟合的解决


对于欠拟合问题来说，由于欠拟合通常是由于学习能力低下而造成的，因此比较容易克服。例如在决策树学习中扩展分支、在神经网络学习中增加训练轮数等。

但是对于过拟合问题就很麻烦了，实际上，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一 些针对过拟合的措施。而且，我们要知道，过拟合是无法彻底避免的，一些针对过拟合的措施只是 “缓解”，或者说减小过拟合的风险。**为什么不能彻底避免呢？下面的原因还是有些没明白？**

关于过拟合为什么不能彻底避免，原因如下：机器学习面临的问题通常是 NP 难甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性地证明了 “P=NP” ，因此，只要相信 “P ≠ NP” ，过拟合就不可避免。**到底什么是NP难？一直想知道。什么是在多项式时间内运行完成？**









* * *





# COMMENT



