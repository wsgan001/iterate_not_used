TODO


  * **一直想知道这些指标对于多分类问题怎么办？**

  * **聚类的性能度量参见第 9章  这里每写，看来这种性能度量还是要拆开，根据是对应的分类问题还是聚类问题分成两张。**

  * **很多东西还是没有理解，要仔细看下。**



* * *

[TOC]






# 性能度量


当模型从训练集上训练出来，然后在测试集测试之后，我们就要衡量一下这个模型的泛化能力了，这就是性能度量 (performance measure) 。






## 任务需求的重要性


性能度量是反映了任务需求的，因此，在对比不同模型的能力的时候，使用不同的性能度量往往会得到不同的评判结果。这就意味着，模型的 好坏 是相对的，什么样的模型是好的，不仅取决于我们的算法和数据，还取决于任务需求。



在预测任务中，给定样本集\D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}\) ，其中 \(y_i\) 是示例 \(x_i\) 的真实标记。要评估学习器的性能，就要把学习器预测结果 \(f(x)\) 与真实标记进行比较。**什么是预测任务？**

回归任务中最常用的性能度量就是 “均方误差”(mean squared error)：**这个回归任务中的性能度量还是抽出来单独总结吧**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b05751a58cc9.png)


更一般的，对于数据分布\(\mathcal{D}\) 和概率密度函数 \(p(\cdot )\) ，均方误差可描述为


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057547e3ec2.png)


下面，我们主要介绍分类任务中常用的性能度量。




# 错误率与精度


错误率和精度，是分类任务中最常用的两种性能度量， 既适用于二分类任务，也适用于多分类任务。

错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例。

对样例集 D，分类错误率定义为：**D 不是拆开成 S 和 T 了吗？为什么还要放在 D 中看？**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0575df618b9.png)


精度定义为：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b05760e10755.png)


更一般的，对于数据分布 \(\mathcal{D}\) 和概率密度函数 \(p(\cdot )\) ，错误率与精度可分别描述为：**这个地方也是等于和不等于吗？要求这么严格吗？要确认下。**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b05765caaf20.png)




![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057668001bd.png)





# 准确率、召回率与 F1




## 为什么需要准确率和召回率？


错误率和精度虽然常用，但是并不能满足所有任务需求。

比如：

假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率只能衡量了有多少比例的瓜被判别错误，但是如果我们关心的是 “挑出的西瓜中有多少比例是好瓜” 或者 “所有好瓜中有多少比例被挑了出来”，那么错误率就不够用了，这时我们就需要使用其他的性能度量。

类似的需求在信息检索、Web搜索等应用中其实经常出现，比如：在信息检索 中，我们经常会关心 “检索出的信息中有多少比例是用户感兴趣的” ，“用户感兴趣的信息中有多少被检索出来了”。这些是错误率表达不出来的。

OK，因此，我们就有了：查准率 ( 准确率 ，precision ) 与 查全率 ( 召回率，recall) 。**注意：这里是准确率与上面介绍的精度是区别的。**


## 以二分类问题为例


我们以二分类问题为例：

我们可以将样例根据其真实类别与学习器预测类别的组合划分为：




  * 真正例 (true positive)    个数：TP

  * 假正例 (false positive)   个数：FP

  * 真反例 (true negative)  个数：TN

  * 假反例 (false negative)  个数：FN


显然：TP + FP + TN + FN =样例总数 。

分类结果的 “混淆矩阵” (confusion matrix) 如下表所示：**为什么叫混淆矩阵呢？**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0578668db37.png)


OK，那么查准率 P 与查全率 R 的定义为：

![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b05789be6ac1.png)


## 准确率与召回率的关系


查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

例如：




  * 如果我们希望将好瓜尽可能多地选出来， 那么可以通过增加选瓜的数量来实现，如果将所有西瓜都选上，那么所有的好瓜也必然都被选上了，但是这样的查准率就会较低。

  * 如果我们希望选出的瓜中好瓜比例尽可能的高，那么我们就可以只挑选最有把握的瓜，但是这样就难免会漏掉不少好瓜，使得查全率较低。


通常只有在一些简单任务中，才可能使得查全率和查准率都很高。**嗯，好吧 看来还是有可能都高的。**


## P-R 曲线


在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为 “最可能” 是正例的样本，排在最后的则是学习器认为 “最不可能” 是正例的样本。那么按照这个顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。我们以查准率为纵轴、查全率为横轴作图，就得到 了查准率-查全率曲线，简称 “P-R曲线” ，显示该曲线的图称为 “P-R图” 。**没明白，什么叫逐个把样本作为正例进行预测？还是有点不清楚，这个图到底是怎么画出来的？对于一个固定的模型来说，准去率和召回率不应该是一个大概比较固定的数吗？怎么会是一条线呢？**

P-R 曲线与平衡点示意图：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0579a440a3c.png)


为绘图方便和美观，示意图显示出单调平滑曲线，但现实任务中的 P-R 曲线常是非单调、不平滑的， 在很多局部有上下波动。**P-R 曲线是怎么画出来的？为什么会有波动？**

P-R 图直观的显示出学习器在样本总体上的查全率、查准率。在进行比较时：




  * 如果一个学习器的 P-R 曲线被另一个学习器的曲线完全 “包住” ，则可断言后者的性能优于前者，例如上图中学习器 A 的性能优于学习器 C  。

  * 如果两个 学习器的P-R曲线发生了交叉，例如上图中的 A 与 B，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较。


然而，很多情形下，人们往往仍希望把学习器 A 与 B 比出个高低，这时一个比较合理的判据是比较 P-R 曲线下面积的大小，它在一定程度上表征了学习器在查准率和查全率上取得相对 “双高” 的比例。但这个面积值不太容易估算，那么怎么办呢？**为什么比较曲线下的面积大小是合理的？**

我们可以看下 “平衡点” (Break-Event Point，简称BEP )，平衡点是 “查准率=查全率” 时的取值，例如上图中学习器 C 的 BEP 是0.64，基于 BEP 的比较，我们可以认为学习器 A 优于 B。


## F1


但是 BEP 还是过于简单了些，我们更常用的是 F1 度量。F1 是基于查准率与查全率的调和平均。**为什么使用调和平均？书上说，调和平均与算数平均 \(\frac{P+R}{2}\) 和集合平均 \(\sqrt{P\times R}\) 相比，更重视较小值，为什么要重视较小值？**

**有个问题之前就想知道：F1 与 BEP 或者 P-R 的面积会有冲突吗？冲突的时候以哪个为准？**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057db93c6ae.png)




![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057cb9baf1a.png)





## $F_\beta$


在一些应用中，对查准率和查全率的重视程度是有所不同的，比如：




  * 在商品推荐系统中，为了尽可能的少打扰用户，我们更希望推荐内容是用户感兴趣的，这个时候查准率就更重要。

  * 而在逃犯信息检索系统中，我们更希望尽可能的少漏掉逃犯，因此这个时候查全率更加重要。


F1 度量的一般形式 \(F_\beta\) ，能够让我们表达出对查准率/查全率的不同偏好，它是基于查准率与查全率的加权调和平均：**之前从来不知道还有  \(F_\beta\) ，有可能看到过，但是忘记了。**


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057e1e9394c.png)




![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b057d73ebb14.png)


其中 \(\beta >0\) 度量了查全率对查准率的相对重要性：




  * \(\beta =1\) 时退化为标准的 F1

  * \(\beta >1\) 时查全率有更大影响

  * \(\beta <1\) 时查准率有更大影响




## 多个二分类混淆矩阵怎么比较？


很多时候我们有多个二分类混淆矩阵，比如说：




  * 进行了多次训练/测试，每次得到一个混淆矩阵

  * 或者是在多个数据集上进行了训练/测试，我们希望能够估计算法的 “全局” 性能；

  * 或者是执行多分类任务，每两两类别的组合都对应一个混淆矩阵。**这也可以？**

  * ……


总之，我们希望在 n 个二分类混淆矩阵上综合考察查准率和查全率。怎么办呢？

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率， 记为 \((P_1,R_1),(P_2,R_2),\cdots ,(P_n,R_n)\) ，再计算平均值，这样就得到了 “宏查准率”（macro-P）、“宏查全率”（macro-R）,以及相应的 “宏F1” （macro-F1）:


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b05820c9c088.png)


我们还可以将各混淆矩阵的对应元素进行平均，得到 TP、FP、TN、FN 的平均值，分别记为\(\overline{TP}\) 、\(\overline{FP}\) 、\(\overline{TN}\) 、\(overline{FN}\) ，然后，我们再基于这些平均值计算出 “微查准率”（micro-P）、“微查全率”（micro-R）和“微F1” （micro-F1）:


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0583012d328.png)


**？到底用哪个？到底怎么评价？**




# ROC 与 AUC


**没有很明白？**

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值 (threshold) 进行比较，如果大于阈值则分为正类，否则为反类。

例如：神经网络在一般情形下是对每个测试样本预测出一个 [0.0,1.0] 之间的实值， 然后将这个值与0.5 进行比较，大于 0.5 则判为正例，否则为反例。这个实值或概率预测结果的好坏，直接决定了学习器的泛化能力。

实际上，根据这个实值或概率预测结果，我们可以将测试样本进行排序，“最可能” 是正例的排在最前面， “最不可能” 是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个 “截断点” (cut point) 将样本分为两部分，前一部分判作正例，后一部分则判作反例。 在不同的应用任务中，我们可根据任务需求来采用不同的截断点，例如若我们更重视“查准率”，则可选择排序中靠前的位置进行截断；若更重视 “查全率”，则可选择靠后的位置进行截断。

因此，排序本身的质量好坏，体现了综合考虑学习器在不同任务下的 “期望泛化性能” 的好坏，或者说， “一般情况下” 泛化性能的好坏。

ROC 曲线则是从这个角度出发来研究学习器泛化性能 的有力工具。 ROC 全称是 “受试者工作特征” (Receiver Operating Characteristic) 曲线，它源于二战中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域。

P-R曲线相似，我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、级坐标作图，就得到了 “ROC曲线”。

与P-R曲线使用查准率、查全率为纵、横轴不同，ROC 曲线的纵轴是 “真正例率” (True Positive Rate,简称 TPR)，横轴是 “假正例率” (False Positive Rate，简称FPR)，两者定义如下：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b063347b6cda.png)


ROC 曲线的图称为 “ROC图” ，下图为ROC曲线与AUC示意图：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b063371989b9.png)


显然， 对角线对应于 “随机猜测” 模型，而点 (0, 1) 则对应于将所有正例排在所有反例之前的 “理想模型”。

现实任务中通常是利用有限个测试样例来绘制 ROC 图，此时仅能获得有限个(真正例率，假正例率)坐标对，无法产生上图 (a) 中的光滑 ROC 曲线，只能 绘制出如上图 (b) 中所示的近似 ROC 曲线。绘图过程很简单：给定 \(m^+\) 个正例和 \(m^-\) 个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大， 即把所有样例均预测为反例，此时真正例率和假正例率均为 0，在坐标 (0,0) 处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为 \(x,y)\)，当前若为真正例，则对应标记点的坐标为 \((x,y+\frac{1}{m^+})\) ，当前若为假正例，则对应标记点的坐标为 \((x+\frac{1}{m^-},y)\)  ，然后用线段连接相邻点即得。

进行学习器的比较时，与 P-R 图相似，：




  * 若一个学习器的 ROC 曲线被另一 个学习器的曲线完全 “包住”，则可断言后者的性能优于前者；

  * 若两个学习器 的ROC曲线发生交叉，则难以一般性地断言两者孰优孰劣，此时如果一定要进行比较，则较为合理的判据是比较ROC曲线下的面积，即 AUC (Area Under ROC Curve)。


从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得。假定 ROC 曲线是由坐标为 \(\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}\) 的点按序连接而形成 \((x_1=0,x_m=1)\) ，则 AUC 可估算为

\[AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot (y_i+y_{i+1})\]

形式化地看，AUC 考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定 \(m^+\) 个正例和 \(m^-\) 个反例，令 \(D^+\) 和 \(D^-\) 分别表示正、反例集合， 则排序 “损失” (loss) 定义为：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0635858df7b.png)


即考虑每一对正、反例，若正例的预测值小于反例，则记一个 “罚分” ，若相 等，则记 0.5 个 “罚分”。容易看出，\(\mathcal{l}_{rank}\) 对应的是 ROC 曲线之上的面积：若 一个正例在 ROC 曲线上对应标记点的坐标为\((x,y)\) ，则 x 恰是排序在其之前的 反例所占的比例，即假正例率。因此有：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b063637ee538.png)





# 代价敏感错误率与代价曲线


**没有明白**

在现实任务中常会遇到这样的情况：不同类型的错误所造成的后果不同。

例如：




  * 在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者, 看起来都是犯了 “一次错误”，但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机。

  * 再比如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故。


为权衡不同类型错误所造成的不同损失，可为错误赋予 “非均等代价” (unequal cost)。

以二分类任务为例，我们可根据任务的领域知识设定一个 “代价矩阵” (cost matrix)，如下图所示：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0636f7bce02.png)


其中 \(cost_{ij}\) 表示将第 i 类样本预测为第 j 类样本的代价。一般来说， \(cost_{ii}=0\) ；若将第 0 类判别为第 1 类所造成的损失更 大，则 \(cost_{01}>cost_{10}\) ；损失程度相差越大， \(cost_{01}\) 与 \(cost_{10}\) 值的差别越大。一般情况下，重要的是代价比值而非绝对值，例 如 \(cost_{01}：cost_{10}=5：1\)  与 50 : 10 所起的效果相当。

回顾前面介绍的一些性能度量可看出，它们大都隐式地假设了均等代价，**嗯，是的。** 例如错误率与精度小节中所定义的错误率是直接计算 “错误次数”，并没有考虑不同错误会造成不同的后果。在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化 “总体代价”(total cost)。若将上面这个代价矩阵中的第 0 类作为正 类、第 1 类作为反类，令 \(D^+\) 与 \(D^-\) 分别代表样例集 D 的正例子集和反例子，则 “代价敏感” (cost-sensitive) 错误率为：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b06386ec9b3c.png)类似的，可给出基于分布定义的代价敏感错误率，以及其他一些性能度量如精度的代价敏感版本。若令 \(cost_{ij}\) 中的 i、j 取值不限于0、1，则可定义出多分类任务的代价敏感性能度量。**怎么定义？**


在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而 “代价曲线” (cost curve) 则可达到该目的。代价曲线图的横轴是取值为 [0,1] 的正例概率代价：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0638d31bf77.png)


其中 p 是样例为正例的概率；纵轴是取值为 [0,1] 的归一化代价：（规范化 (normalization)是将不同变化范围的值映射到相同的固定范围中，常见的是 [0,1] ，此时亦称为 “归一化”）


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0639003804d.png)


其中 FPR 是假正例率，FNR = 1-FPK 是假反例率。

代价曲线 的绘制很简单：ROC曲线上每一点对应了代价平面上的一条线段，设 ROC 曲线上点的坐标为 (TPR,FPR)， 则可相应计算出FNR，然后在代价平面上绘制 一条从 (0,FPR) 到 (1,FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 ROC 曲线上的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价。

下图即为代价曲线与期望总体代价：


![](http://106.15.37.116/wp-content/uploads/2018/05/img_5b0639a41ccc7.png)









* * *





# COMMENT


ROC曲线在二十世纪八十年代后期被引入机器学习［Spackman, 1989］, AUC则是从九十年代中期起在机器学习领域广为使用［Bradley, 1997］,但利用

ROC曲线下面积来评价模型期望性能的做法在医疗检测中早已有之[Hanley and McNeil, 1983]. [Hand and Till，2001]将 ROC 曲线从二分类任务推广到多 分类任务.[Fawcett, 2006]综述了 ROC曲线的用途.
[Drummond and Holte, 2006]发明了代价曲线.需说明的是，机器学习过 程涉及许多类型的代价，除了误分类代价，还有测试代价、标记代价、属性代 价等，即便仅考虑误分类代价，仍可进一步划分为基于类别的误分类代价以及 基于样本的误分类代价.代价敏感学习(cost-sensitive learning) [Elkan, 2001; Zhou and Liu，2006]专门研究非均等代价下的学习.





# REF

1. 《机器学习》周志华

