





# REF
  1. 《深度学习》Ian Goodfellow




# TODO






  * aaa



* * *




# INTRODUCTION






  * aaa
















第二章 线性代数
线性代数作为数学的一个分支，广泛应用于科学和工程中。然而，因为线性代数

主要是面向连续数学，而非离散数学，所以很多计算机科学家很少接触它。掌握好线

性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算

法而言。因此，在开始介绍深度学习之前，我们集中探讨一些必备的线性代数知识。

如果你已经很熟悉线性代数，那么可以轻松地跳过本章。如果你已经了解这些概 念，但是需要一份索引表来回顾一些重要公式，那么我们推荐 The Matrix Cookbook

（Petersen and Pedersen, 2006）。如果你没有接触过线性代数，那么本章将告诉你本

书所需的线性代数知识，不过我们仍然非常建议你参考其他专门讲解线性代数的文

献，例如 Shilov （1977）。最后，本章略去了很多重要但是对于理解深度学习非必需 的线性代数知识。

2.1 标量、向量、矩阵和张量
学习线性代数，会涉及以下几类数学概念：

•标量（scalar）: —个标量就是一个单独的数，它不同于线性代数中研究的其他 大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小 写的变量名称。当我们介绍标量时，会明确它们是哪种类型的数。比如，在定 义实数标量时，我们可能会说“令S e R表示一条线的斜率”；在定义自然数标 量时，我们可能会说“令n e N表示元素的数目''。

• 向量（ vector ） ：一个向量是一列数。这些数是有序排列的。通过次序中的索 引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比 如a:。向量中的元素可以通过带脚标的斜体表示。向量的第一个元素是2；1,

第二个元素是T2,等等。我们也会注明存储在向量中的元素是什么类型的。如 果每个元素都属于R，并且该向量有n个元素，那么该向量属于实数集R的 n次笛卡尔乘积构成的集合，记为Rn。当需要明确表示向量中的元素时，我们

会将元素排列成一个方括号包围的纵列：

x1
x2
(2.1)


x=

xn
我们可以把向量看作空间中的点，每个元素是不同坐标轴上的坐标。

有时我们需要索引向量中的一些元素。在这种情况下，我们定义一个包含这些 元素索引的集合，然后将该集合写在脚标处。比如，指定A，23和2%，我们定 义集合S =仏3,6｝，然后写作勒。我们用符号一表示集合的补集中的索引。 比如 x-1 表示 x 中除21 外的所有元素， x-S 表示 x 中除 21， 23， 26 外所有元 素构成的向量。

•矩阵（matrix）:矩阵是一个二维数组，其中的每一个元素被两个索引（而非 一个）所确定。我们通常会赋予矩阵粗体的大写变量名称，比如4。如果一个 实数矩阵高度为m，宽度为n，那么我们说4 e Rmxn。我们在表示矩阵中的 元素时，通常以不加粗的斜体形式使用其名称，索引用逗号间隔。比如， A1,1 表示 4 左上的元素， Am,n 表示 4 右下的元素。我们通过用 “:'' 表示水平坐

标，以表示垂直坐标中的所有元素。比如，4i,:表示4中垂直坐标i上的一 横排元素。这也被称为4的第i行（row）。同样地，4:,i表示4的第i列 （column）。当我们需要明确表示矩阵中的元素时，我们将它们写在用方括号括

起来的数组中：    [

A1,1 A1,2 A2,1 A2,2

(2.2)


有时我们需要矩阵值表达式的索引，而不是单个元素。在这种情况下，我们在

表达式后面接下标，但不必将矩阵的变量名称小写化。比如， f（4）i,j 表示函数 f 作用在 4 上输出的矩阵的第 i 行第 j 列元素。

•张量（tensor）:在某些情况下，我们会讨论坐标超过两维的数组。一般地，一 个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。我们使用 字体A来表示张量“A''。张量A中坐标为（ij, k）的元素记作Aij,k。

转置(transpose)是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像， 这条从左上角到右下角的对角线被称为主对角线(main diagonal)。图2.1显示了这 个操作。我们将矩阵4的转置表示为，定义如下

(AT)i,j = Aj,i.    (2.3)

向量可以看作只有一列的矩阵。对应地，向量的转置可以看作是只有一行的矩

阵。有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其

变为标准的列向量，来定义一个向量，比如 x=[x1,x2,x3]T.

标量可以看作是只有一个元素的矩阵。因此，标量的转置等于它本身， a=aT。


A2,1

A2,2


A3,1

A3,2


图 2.1: 矩阵的转置可以看成以主对角线为轴的一个镜像

只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对应位置 的兀素相加，比如C=A + B，其中Cij = Aij + Bij。

标量和矩阵相乘，或是和矩阵相加时，我们只需将其与矩阵的每个元素相乘或 相加，比如乃=a • B + c，其中Dij = a • Bij + c。

在深度学习中，我们也使用一些不那么常规的符号。我们允许矩阵和向量相 加，产生另一个矩阵：C= A + 6,其中Gj = Aij + ~。换言之，向量6和矩阵 A 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 b 复制 到每一行而生成的矩阵。这种隐式地复制向量 6 到很多位置的方式，被称为广播 broadcasting)。

2.2 矩阵和向量相乘
矩阵乘法是矩阵运算中最重要的操作之一。两个矩阵 A 和 B 的矩阵乘积 (matrix product)是第三个矩阵C。为了使乘法定义良好，矩阵A的列数必须和矩 阵B的行数相等。如果矩阵A的形状是rn x n，矩阵B的形状是n x p，那么矩阵

C的形状是mxp。我们可以通过将两个或多个矩阵并列放置以书写矩阵乘法，例如

C = AB .    (2.4)

具体地，该乘法操作定义为

Ci,j =    Ai,kBk,j .    (2.5)

k

需要注意的是，两个矩阵的标准乘积不是指两个矩阵中对应元素的乘积。不过 那样的矩阵操作确实是存在的，被称为元素对应乘积(element-wise product)或 者 Hadamard 乘积(Hadamard product )，记为 A 0 B。

两个相同维数的向量a和y的点积(dot product)可看作是矩阵乘积aTy。我 们可以把矩阵乘积C= AB中计算Cij的步骤看作是A的第z‘行和B的第j列之

间的点积。

矩阵乘积运算有许多有用的性质，从而使矩阵的数学分析更加方便。比如，矩

阵乘积服从分配律：

利用两个向量点积的结果是标量，标量转置是自身的事实，我们可以证明式(2.8)：

aTy = (aTy)T = yTa.    (2.10)

由于本书的重点不是线性代数，我们并不试图展示矩阵乘积的所有重要性质

但读者应该知道矩阵乘积还有很多有用的性质。

现在我们已经知道了足够多的线性代数符号，可以表达下列线性方程组：

4x = b    (2.11)

其中4 e Rmxn是一个已知矩阵，6 e Rm是一个已知向量，z e Rn是一个我们要 求解的未知向量。向量的每一个元素都是未知的。矩阵4的每一行和6中对

应的元素构成一个约束。我们可以把式(2.11)重写为

41,:x=b1

(2.12)

42,:x=b2

(2.13)

(2.14)

4 m, : x = bm

(2.15)

或者，更明确地，写作

41,1x1 + 41,2x2 + •

• • 41,nxn = b1

(2.16)

42,1X1 + 42,2X2 + •

■■ 42,nxn = b2

(2.17)

(2.18)

4m,1x1 + 4m,2X2 + • •

• 4m,nXn = bm.

(2.19)

矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示

2.3 单位矩阵和逆矩阵
线性代数提供了被称为矩阵逆(matrix inversion )的强大工具。对于大多数矩 阵 4，我们都能通过矩阵逆解析地求解式 (2.11) 。

为了描述矩阵逆，我们首先需要定义单位矩阵(identity matrix )的概念。任意 向量和单位矩阵相乘，都不会改变。我们将保持 n 维向量不变的单位矩阵记作 In。

形式上， In e Rnxn，

'ix e R™, Inx = x.    (2.20)

单位矩阵的结构很简单：所有沿主对角线的元素都是 1，而所有其他位置的元素都是 0。如图2.2所示。

100

010

001

图 2.2: 单位矩阵的一个样例：这是 I3。

矩阵A的矩阵逆(matrix inversion )记作A-1，其定义的矩阵满足如下条件

A-1A = In.    (2.21)

现在我们可以通过以下步骤求解式(2.11)：

Ax = 6    (2.22)

A-1Ax=A-16    (2.23)

Inx = A-16    (2.24)

x=A-16.    (2.25)

当然，这取决于我们能否找到一个逆矩阵A-1。在接下来的章节中，我们会讨 论逆矩阵 A-1 存在的条件。

当逆矩阵 A-1 存在时，有几种不同的算法都能找到它的闭解形式。理论上，相 同的逆矩阵可用于多次求解不同向量6的方程。然而，逆矩阵A-1主要是作为理论 工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵 A-1 在数

字计算机上只能表现出有限的精度，有效使用向量 6的算法通常可以得到更精确的 x。

2.4 线性相关和生成子空间
如果逆矩阵 A-1 存在，那么式(2.11)肯定对于每一个向量 6 恰好存在一个解。 但是，对于方程组而言，对于向量 6 的某些值，有可能不存在解，或者存在无限多 个解。存在多于一个解但是少于无限多个解的情况是不可能发生的；因为如果 x 和 y 都是某方程组的解，则

z = ax +(1 — a)y

(2.26)


(其中a取任意实数)也是该方程组的解。

为了分析方程有多少个解，我们可以将A的列向量看作从原点(origin)(元素 都是零的向量)出发的不同方向，确定有多少种方法可以到达向量6。在这个观点 下，向量a中的每个元素表示我们应该沿着这些方向走多远，即表示我们需要沿 着第 z 个向量的方向走多远：

Aa =    xiA:,i.    (2.27)

i

一般而言，这种操作被称为线性组合(linear combination )。形式上，一组向量的线 性组合，是指每个向量乘以对应标量系数之后的和，即：

civ(i).    (2.28)

i

一组向量的生成子空间( span )是原始向量线性组合后所能抵达的点的集合。

确定Aa= 6是否有解相当于确定向量6是否在A列向量的生成子空间中。这 个特殊的生成子空间被称为A的列空间(column space )或者A的值域(range)。

为了使方程Aa = 6对于任意向量6 e Rm都存在解，我们要求A的列空间构 成整个R™。如果Rm中的某个点不在A的列空间中，那么该点对应的6会使得 该方程没有解。矩阵 A 的列空间是整个 Rm 的要求，意味着 A 至少有 m 列，即 n > m。否则，A列空间的维数会小于m。例如，假设A是一个3 x 2的矩阵。目 标6是3维的，但是a只有2维。所以无论如何修改a的值，也只能描绘出R3空 间中的二维平面。当且仅当向量6在该二维平面中时，该方程有解。

不等式 n > m 仅是方程对每一点都有解的必要条件。这不是一个充分条件，因 为有些列向量可能是冗余的。假设有一个R2x2中的矩阵，它的两个列向量是相同 的。那么它的列空间和它的一个列向量作为矩阵的列空间是一样的。换言之，虽然 该矩阵有2列，但是它的列空间仍然只是一条线，不能涵盖整个R2空间。

正式地说，这种冗余被称为线性相关(linear dependence )。如果一组向量中 的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为线性无关 (linearly independent )。如果某个向量是一组向量中某些向量的线性组合，那么我 们将这个向量加入这组向量后不会增加这组向量的生成子空间。这意味着，如果一 个矩阵的列空间涵盖整个Rm，那么该矩阵必须包含至少一组m个线性无关的向量。 这是式(2.11)对于每一个向量6的取值都有解的充分必要条件。值得注意的是，这 个条件是说该向量集恰好有 m 个线性无关的列向量，而不是至少 m 个。不存在一 个m维向量的集合具有多于m个彼此线性不相关的列向量，但是一个有多于m个 列向量的矩阵有可能拥有不止一个大小为m的线性无关向量集。

要想使矩阵可逆，我们还需要保证式(2.11)对于每一个 6值至多有一个解。为 此，我们需要确保该矩阵至多有 m 个列向量。否则，该方程会有不止一个解。

综上所述，这意味着该矩阵必须是一个方阵(square)，即m = n，并且所有列 向量都是线性无关的。一个列向量线性相关的方阵被称为奇异的(singular)。

如果矩阵 4 不是一个方阵或者是一个奇异的方阵，该方程仍然可能有解。但是 我们不能使用矩阵逆去求解。

目前为止，我们已经讨论了逆矩阵左乘。我们也可以定义逆矩阵右乘：

44-1 = I.    (2.29)

对于方阵而言，它的左逆和右逆是相等的。

2.5 范数
有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数 (norm )的函数衡量向量大小。形式上，Lp范数定义如下

11< =



(2.30)

其中 peR，p> 1。

范数(包括Lp范数)是将向量映射到非负值的函数。直观上来说，向量x的 范数衡量从原点到点x的距离。更严格地说，范数是满足下列性质的任意函数：

• f (x) = 0 今 x = 0

•    f (x + y) < f (x) + f (y)(三角不等式(triangle inequality ))

•    Va e R, f (ax) = |a|f (x)

当p = 2时，L2范数被称为欧几里得范数(Euclidean norm )。它表示从原点 出发到向量x确定的点的欧几里得距离。L2范数在机器学习中出现地十分频繁，经

常简化表示为||x||，略去了下标2。平方L2范数也经常用来衡量向量的大小，可以

简单地通过点积 xTx 计算。

平方 L2 范数在数学和计算上都比 L2 范数本身更方便。例如，平方 L2 范数对 x 中每个兀素的导数只取决于对应的兀素，而 L2 范数对每个兀素的导数却和整个向 量相关。但是在很多情况下，平方 L2 范数也可能不受欢迎，因为它在原点附近增长

得十分缓慢。在某些机器学习应用中，区分恰好是零的兀素和非零但值很小的兀素

是很重要的。在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的 数学形式的函数： L1 范数。 L1 范数可以简化如下：

(2.31)


| x| 1 =    |xi|.

当机器学习问题中零和非零兀素之间的差异非常重要时，通常会使用 L1 范数。每当 x中某个元素从0增加e，对应的L1范数也会增加e。

有时候我们会统计向量中非零兀素的个数来衡量向量的大小。有些作者将这种 函数称为 “L0 范数''，但是这个术语在数学意义上是不对的。向量的非零元素的数目 不是范数，因为对向量缩放a倍不会改变该向量非零元素的数目。因此，L1范数经 常作为表示非零元素数目的替代函数。

另外一个经常在机器学习中出现的范数是L-范数，也被称为最大范数(max norm )。这个范数表示向量中具有最大幅值的元素的绝对值：

(2.32)


| x|    = max |xi|.

有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使

用 Frobenius 范数( Frobenius norm)

| A|


(2.33)

其类似于向量的 L2 范数。

两个向量的点积(dot product)可以用范数来表示。具体地

(2.34)


xTy= llx|2 Ily|l2cos(9

其中0表示x和y之间的夹角

2.6 特殊类型的矩阵和向量
有些特殊类型的矩阵和向量是特别有用的。

对角矩阵(diagonal matrix)只在主对角线上含有非零元素，其他位置都是零。 形式上，矩阵乃是对角矩阵，当且仅当对于所有的i = j，Dij = 0。我们已经看到 过一个对角矩阵：单位矩阵，对角元素全部是 1。我们用 diag(v) 表示一个对角元素 由向量r中元素给定的对角方阵。对角矩阵受到关注的部分原因是对角矩阵的乘法 计算很高效。计算乘法diag(r)a，我们只需要将a中的每个元素巧放大％倍。换 言之，diag(T)a = r 0 a。计算对角方阵的逆矩阵也很高效。对角方阵的逆矩阵存在， 当且仅当对角元素都是非零值，在这种情况下， diag(r)-1=diag([1/v1,...,1/vn]T)。 在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法；但通过将一 些矩阵限制为对角矩阵，我们可以得到计算代价较低的(并且简明扼要的)算法。

不是所有的对角矩阵都是方阵。长方形的矩阵也有可能是对角矩阵。非方阵的 对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。对于一个长方形对 角矩阵D而言，乘法Da会涉及到a中每个元素的缩放，如果D是瘦长型矩阵， 那么在缩放后的末尾添加一些零；如果 D 是胖宽型矩阵，那么在缩放后去掉最后一 些元素。

对称(symmetric。矩阵是转置和自己相等的矩阵：

A = AT.    (2.35)

当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常会出现。例如，如 果A是一个距离度量矩阵，Aij表示点i到点j的距离，那么Aij = Aj,i，因为距 离函数是对称的。

单位向量 (unit vector。是具有单位范数(unit norm。的向量：

||4 = 1.    (2.36)

如果 aTy= 0，那么向量 a 和向量 y 互相正交( orthogonal )。如果两个向量都 有非零范数，那么这两个向量之间的夹角是90度。在Rn中，至多有n个范数非 零向量互相正交。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是 标准正交( orthonormal)。

正交矩阵(orthogonal matrix)是指行向量和列向量是分别标准正交的方阵：

4t4 = 44t = I.    (2.37)

这意味着

4-1 = 4t,    (2.38)

所以正交矩阵受到关注是因为求逆计算代价小。我们需要注意正交矩阵的定义。违

反直觉的是，正交矩阵的行向量不仅是正交的，还是标准正交的。对于行向量或列

向量互相正交但不是标准正交的矩阵，没有对应的专有术语。

2.7 特征分解
许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性而

更好地理解，这些属性是通用的，而不是由我们选择表示它们的方式产生的。

例如，整数可以分解为质因数。我们可以用十进制或二进制等不同方式表示整 数12,但是12 = 2 x 2 x 3永远是对的。从这个表示中我们可以获得一些有用的信 息，比如 12不能被5整除，或者12的倍数可以被3 整除。

正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以通过分

解矩阵来发现矩阵表示成数组元素时不明显的函数性质。

特征分解(eigendecomposition )是使用最广的矩阵分解之一，即我们将矩阵分

解成一组特征向量和特征值。

方阵4的特征向量(eigenvector)是指与4相乘后相当于对该向量进行缩放 的非零向量 v ：

4v = Av.    (2.39)

标量A被称为这个特征向量对应的特征值(eigenvalue)。(类似地，我们也可以 定义左特征向量(left eigenvector) vT4 = AvT，但是通常我们更关注右特征向量

right eigenvector))。

如果v是4的特征向量，那么任何缩放后的向量sv (s e R，s = 0)也是4的 特征向量。此外，sv和v有相同的特征值。基于这个原因，通常我们只考虑单位特 征向量。

假设矩阵 4 有 n 个线性无关的特征向量 {v(1),...,v(n)}， 对应着特征值 {Ai,...,An}。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量:

V = [v⑴，...，v⑷].类似地，我们也可以将特征值连接成一个向量A = [Ai,..., An]T 因此义的特征分解(eigendecomposition )可以记作

A= Vdiag(A)V-1.    (2.40)

我们已经看到了构建具有特定特征值和特征向量的矩阵，能够使我们在目标方 向上延伸空间。然而，我们也常常希望将矩阵分解(decompose )成特征值和特征向 量。这样可以帮助我们分析矩阵的特定性质，就像质因数分解有助于我们理解整数。

不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下，特征分解存

在，但是会涉及复数而非实数。幸运的是，在本书中，我们通常只需要分解一类有

简单分解的矩阵。具体来讲，每个实对称矩阵都可以分解成实特征向量和实特征值：

A = QAQt.    (2.41)

其中Q是A的特征向量组成的正交矩阵，A是对角矩阵。特征值Ay对应的特征 向量是矩阵Q的第i列，记作Q:,i。因为Q是正交矩阵，我们可以将A看作沿方 向v(i)延展人倍的空间。如图2.3所示的例子。

虽然任意一个实对称矩阵 A 都有特征分解，但是特征分解可能并不唯一。如果 两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间 中，任意一组正交向量都是该特征值对应的特征向量。因此，我们可以等价地从这 些特征向量中构成Q作为替代。按照惯例，我们通常按降序排列A的元素。在该 约定下，特征分解唯一当且仅当所有的特征值都是唯一的。

矩阵的特征分解给了我们很多关于矩阵的有用信息。矩阵是奇异的当且仅当含 有零特征值。实对称矩阵的特征分解也可以用于优化二次方程f(x) = xTAx，其中 限制||x||2 = 1。当x等于A的某个特征向量时，f将返回对应的特征值。在限制条 件下，函数 f 的最大值是最大特征值，最小值是最小特征值。

所有特征值都是正数的矩阵被称为正定( positive definite )；所有特征值都是非 负数的矩阵被称为半正定(positive semidefinite )。同样地，所有特征值都是负数的 矩阵被称为负定(negative definite);所有特征值都是非正数的矩阵被称为半负定 (negative semidefinite )。半正定矩阵受到关注是因为它们保证Vx，xTAx > 0。此夕卜， 正定矩阵还保证xTAx =0 x = 0。

Effect of eigenvectors and eigenvalues


x0    x00

图 2.3: 特征向量和特征值的作用效果。特征向量和特征值的作用效果的一个实例。在这里，矩阵

A有两个标准正交的特征向量，对应特征值为M的r⑴以及对应特征值为^的r⑶。f左J我 们画出了所有的单位向量《e R2的集合，构成一个单位圆。f右J我们画出了所有的A«点的集 合。通过观察A拉伸单位圆的方式，我们可以看到它将r(i)方向的空间拉伸了 Ai倍。

2.8 奇异值分解
在第2.7节，我们探讨了如何将矩阵分解成特征向量和特征值。还有另一种分解 矩阵的方法，被称为奇异值分解(singular value decomposition, SVD )，将矩阵分 解为奇异向量(singular vector )和奇异值(singular value )。通过奇异值分解，我 们会得到一些与特征分解相同类型的信息。然而，奇异值分解有更广泛的应用。每 个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没 有特征分解，这时我们只能使用奇异值分解。

回想一下，我们使用特征分解去分析矩阵 A 时，得到特征向量构成的矩阵 V 和特征值构成的向量A，我们可以重新将A写作

奇异值分解是类似的，只不过这回我们将矩阵 4 分解成三个矩阵的乘积：

4 = UDVT.    (2.43)

假设4是一个m x n的矩阵，那么U是一个m x m的矩阵，D是一个m x n 的矩阵，V是一个n x n矩阵。

这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵U和V都定义为正交 矩阵，而矩阵 D 定义为对角矩阵。注意，矩阵 D 不一定是方阵。

对角矩阵D对角线上的元素被称为矩阵4的奇异值(singular value)。矩阵 U的列向量被称为左奇异向量(left singular vector )，矩阵V的列向量被称右奇异 向量( right singular vector)。

事实上，我们可以用与 4 相关的特征分解去解释4 的奇异值分解。 4 的左奇 异向量(left singular vector )是44T的特征向量。4的右奇异向量(right singular vector)是4t4的特征向量。4的非零奇异值是4T4特征值的平方根，同时也是 44T 特征值的平方根。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。我们将在下一节中

探讨。





2.9 Moore-Penrose 伪逆
对于非方矩阵而言，其逆矩阵没有定义。假设在下面的问题中，我们希望通过 矩阵 4 的左逆 B 来求解线性方程，

4x = y

(2.44)


等式两边左乘左逆 B 后，我们得到

x = By.    (2.45)

取决于问题的形式，我们可能无法设计一个唯一的映射将 4 映射到 B。

如果矩阵 4 的行数大于列数，那么上述方程可能没有解。如果矩阵 4 的行数 小于列数，那么上述矩阵可能有多个解。

Moore-Penrose 伪逆( Moore-Penrose pseudoinverse )使我们在这类问题上 取得了一定的进展。矩阵 A 的伪逆定义为：

A+ = lim (ATA + aI)-1AT.    (2.46)

a\0

计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：

A+ = VD+UT.    (2.47)

其中，矩阵R D和V是矩阵A奇异值分解后得到的矩阵。对角矩阵D的伪逆 D+ 是其非零元素取倒数之后再转置得到的。

当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一 种。特别地，x= A+y是方程所有可行解中欧几里得范数||x||2最小的一个。

当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 x 使得Ax和y的欧几里得距离||Ax- y||2最小。

2.10 迹运算
迹运算返回的是矩阵对角元素的和：

Tr(A) =    Ai,i.    (2.48)

i

迹运算因为很多原因而有用。若不使用求和符号，有些矩阵运算很难描述，而通过矩

阵乘法和迹运算符号可以清楚地表示。例如，迹运算提供了另一种描述矩阵Frobenius 范数的方式：    _

||A||f = y Tr(AAT).    (2.49)

用迹运算表示表达式，我们可以使用很多有用的等式巧妙地处理表达式。例如

迹运算在转置运算下是不变的：

Tr(A) = Tr(AT).    (2.50)

多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相

乘的迹是相同的。当然，我们需要考虑挪动之后矩阵乘积依然定义良好：

或者更一般地，

n    n-1

Tr( F(i)) = Tr(F(n)    F(i)).    (2.52)

i=1    i=1

即使循环置换后矩阵乘积得到的矩阵形状变了，迹运算的结果依然不变。例如，假

设矩阵A e Rmxn，矩阵B e Rnxm，我们可以得到

Tr(AB) = Tr(BA)    (2.53)

尽管 AB e Rmxm 和 BA e Rnxn。

另一个有用的事实是标量在迹运算后仍然是它自己： a=Tr(a)。

2.11 行列式
行列式，记作det(A)，是一个将方阵A映射到实数的函数。行列式等于矩阵特 征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小 了多少。如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的 体积。如果行列式是 1，那么这个转换保持空间体积不变。

2.12 实例：主成分分析
主成分分析(principal components analysis, PCA )是一个简单的机器学习算 法，可以通过基础的线性代数知识推导。

假设在Rn空间中我们有m个点｛a⑴,...，a(m)｝，我们希望对这些点进行有损

压缩。有损压缩表示我们使用更少的内存，但损失一些精度去存储这些点。我们希

望损失的精度尽可能少。

一种编码这些点的方式是用低维表示。对于每个点a(i) eRn，会有一个对应的 编码向量c⑴e R1。如果l比n小，那么我们便使用了更少的内存来存储原来的数 据。我们希望找到一个编码函数，根据输人返回编码，f(a) = c；我们也希望找到一 个解码函数，给定编码重构输人，a «g(f (a))。

PCA 由我们选择的解码函数而定。具体地，为了简化解码器，我们使用矩阵乘 法将编码映射回Rn，即g(c) = Dc，其中D e Rnxl是定义解码的矩阵。

目前为止所描述的问题，可能会有多个解。因为如果我们按比例地缩小所有点

对应的编码向量Ci，那么我们只需按比例放大D:,i，即可保持结果不变。为了使问 题有唯一解，我们限制 D 中所有列向量都有单位范数。

计算这个解码器的最优编码可能是一个困难的问题。为了使编码问题简单一些 PCA限制D的列向量彼此正交(注意，除非l = n，否则严格意义上D不是一个 正交矩阵)。

为了将这个基本想法变为我们能够实现的算法，首先我们需要明确如何根据每 一个输人得到一个最优编码c\ 一种方法是最小化原始输人向量和重构向量 g(c^)之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使 用 L2 范数：

c* = argmin \\x — g(c)\\2.    (2.54)

我们可以用平方L2范数替代L2范数，因为两者在相同的值c上取得最小值。

这是因为 L2 范数是非负的，

并且平方运算在非负值上是单调递增的

。

c* = arg min \ x - g(c)\ 22 .

c2

(2.55)

该最小化函数可以简化成

(x-g(c))T(x-g(c))

(2.56)

(式(2.30)中 L2 范数的定义)

= xT x

- xTg(c) - g(c)Tx+g(c)Tg(c)

(2.57)

(分配律)

xTx- 2xTg(c) +g(c)Tg(c)

(2.58)

(因为标量 g(c)Tx 的转置等于自己)

因为第一项xTx不依赖于c，所以我们可以忽略它，得到如下的优化目标:

c* = argmin - 2xTg(c) + g(c)Tg(c).    (2.59)

c
更进一步，我们代人 g(c) 的定义：

c* = arg min - 2xTDc + cTDTDc    (2.60)

c
= argmin - 2xTDc + cTIlc


(2.61)


(矩阵 D 的正交性和单位范数约束)

= arg min - 2xTDc + cTc    (2.62)

c
我们可以通过向量微积分来求解这个最优化问题(如果你不清楚怎么做，请参

考第4.3节)

▽c(-2xTDc+ cTc) = 0    (2.63)

-2DTx+2c=0    (2.64)

c = DTx.    (2.65)

这使得算法很高效：最优编码 x 只需要一个矩阵-向量乘法操作。为了编码向量 我们使用编码函数：

f(x) = DTx.    (2.66)

进一步使用矩阵乘法，我们也可以定义PCA重构操作：

r(x) = g(f(x)) = DDTx.    (2.67)

接下来，我们需要挑选编码矩阵D。要做到这一点，我们回顾最小化输人和 重构之间 L2 距离的这个想法。因为用相同的矩阵 D 对所有点进行解码，我们不 能再孤立地看待每个点。反之，我们必须最小化所有维数和所有点上的误差矩阵 的 Frobenius 范数：

D* = arg min

D

(x(i) _ r(x⑴)j) subject


to DT D = Il.


(2.68)


为了推导用于寻求D*的算法，我们首先考虑l = 1的情况。在这种情况下，D 是一个单一向量d。将式(2.67)代人式(2.68)，简化D为d，问题简化为

d* = arg min Z||x⑴—ddTx叫| subject to ||d||


1.


(2.69)


上述公式是直接代人得到的，但不是文体表述最舒服的方式。在上述公式中，我 们将标量dTx(i)放在向量d的右边。将该标量放在左边的写法更为传统。于是我们 通常写作

d* = arg min ^^||x⑷一dTx⑷d|| subject to ||d||2 = 1,    (2.70)

或者，考虑到标量的转置和自身相等，我们也可以写作

d* = arg min X ||a(i) — a(i)Tdd||

subject to || d|^ = 1.


(2.71)


d
读者应该对这些重排写法慢慢熟悉起来。

此时，使用单一矩阵来重述问题，比将问题写成求和形式更有帮助。这有助于 我们使用更紧凑的符号。将表示各点的向量堆叠成一个矩阵，记为Xe Rmxn，其中 Xi,: = a(i)T。原问题可以重新表述为：

d* = arg min 11X — XddT | subject to dTd = 1.

暂时不考虑约束，我们可以将Frobenius范数简化成下面的形式：

2


(2.72)


(式(2.49) )


X—XddT

ddT T X—XddT


arg min

dF

arg min Tr

d

(2.73)

(2.74)


arg min Tr

d

X—XTXddT—ddTXTX+ddTXTXd


(2.75)


= arg min Tr(XTX) — Tr(XTXddT) — Tr(ddTXTX) + Tr(ddTXTXddT) (2.76)





d
=argmin —Tr(XTXddT)—Tr(ddTXTX)+Tr(ddTXTXddT)    (2.77)

d
(因为与 d 无关的项不影响 argmin)

= argmin — 2Tr(XTXddT) + Tr(ddTXTXddT)    (2.78)

d
(因为循环改变迹运算中相乘矩阵的顺序不影响结果，如式(2.52)所示)

= arg min — 2Tr(XTXddT) + Tr(XTXddTddT)    (2.79)

d
(再次使用上述性质)

此时，我们再来考虑约束条件:

arg min —2Tr(XTXddT)+Tr(XTXddTddT) subject to dTd= 1    (2.80)

d
= arg min -2Tr(XTXddT)+Tr(XTXddT) subject todTd=1    (2.81)

d
(因为约束条件)

= arg min - Tr(XTXddT) subject to dTd = 1    (2.82)

d
= arg max Tr(XTXddT) subject to dTd = 1    (2.83)

d
= arg max Tr(dTXTXd) subject to dTd = 1.    (2.84)

d
这个优化问题可以通过特征分解来求解。具体来讲，最优的d是xTx最大特 征值对应的特征向量。

以上推导特定于 l =1的情况，仅得到了第一个主成分。更一般地，当我们希望

得到主成分的基时，矩阵D由前l个最大的特征值对应的特征向量组成。这个结论

可以通过归纳法证明，我们建议将此证明作为练习。

线性代数是理解深度学习所必须掌握的基础数学学科之一。另一门在机器学习

中无处不在的重要数学学科是概率论，我们将在下一章探讨。







* * *




# COMMENT
