
# REF
1. 《解析卷积神经网络》魏秀参




# TODO






  * aaa



* * *




# INTRODUCTION






  * aaa













参数设定和网络训练



模块、

模型训

巧,如

11.1

搭建整



11.1.1输人数据像素大小


各部件选定后即可开始搭建网络模型和

设计过程中的超参数设定技巧和训练技

络优化策略的选择等。


网络结构相关的各项超参数：输入图像


使用卷积神经网络处理图像问题时,对不同输入图像为得到同规格输出,同

时便于GPU设备并行，会统一将图像压缩到2n大小，一些经典案例如: CIFAR-10 ［51］数据的 32 x 32 像素，STL 数据集［9］的 96 x 96 像素，ImageNet 数据集［73］常用的224 x 224像素。另外，若不考虑硬件设备限制（通常是 GPU显存大小），更高分辨率图像作为输人数据（如448 x 448、672 x 672等）


11.1.网络超参数设定

一般均有助于网络性能的提升，特别是对基于注意力模型（attention model）的

深度网络提升更为显著。不过，高分辨率图像会增加模型计算消耗而导致网络

整体训练时间延长。此外，需指出的是，由于一般卷积神经网络采用全连接层

作为最后分类层，若直接改变原始网络模型的输入图像分辨率，会导致原始模

型卷积层的最终输出无法输入全连接层的状况，此时须重新改变全连接层输入

滤波器的大小或重新指定其他相关参数。

11.1.2卷积层参数的设定

卷积层的超参数主要包括卷积核大小、卷积操作的步长和卷积核个数。关于卷

积核大小，如第3+1 + 1节所述，小卷积核相比大卷积核有两项优势：

1.    增强网络容量和模型复杂度；

2.    减少卷积参数个数。

因此，实践中推荐使用3 x 3及5 x 5这样的小卷积核，其对应卷积操作步长建 议设为1。

此外，卷积操作前还可搭配填充操作（padding）。该操作有两层功效：

1.    可充分利用和处理输人图像（或输人数据）的边缘信息（如图11.1所示）；

2.    搭配合适的卷积层参数可保持输出与输人同等大小，而避免随着网络深度 增加，输人大小的急剧减小。

例如当卷积核大小为3 x 3、步长为1时，可将输人数据上下左右各填充1单 位大小的黑色像素（值为0,故该方法也被称为“zeros-padding”），便可保持输 出结果与原输人同等大小，此时p =1;当卷积核为5 x 5、步长为1时，可指 定p = 2,也可保持输出与输人等大。泛化来讲，对卷积核大小fxf、步长为 1的卷积操作，当P = （f - 1）/2时，便可维持输出与原输人等大。

最后，为了硬件字节级存储管理的方便，卷积核个数通常设置为2的次幂, 如64, 128, 512和1024等等。这样的设定有利于硬件计算过程中划分数据矩 阵和参数矩阵，尤其在利用显卡计算时更为明显。

1

X1

2

3

x1

4

5

6

7

8

9

0

x0

x1

x0

9

8

7

6

5

x1

x0

x1

4

3

2

1

0

1

2

3

4

5

(a)未做填充操作


0

X1

0

x0

0

x1

0

0

0

0

0

x0

1

x1

2

X0

3

4

5

0

0

x1

6

x0

7

x1

8

9

0

0

0

9

8

7

6

5

0

0

4

3

2

1

0

0

0

1

2

3

4

5

0

0

0

0

0

0

0

0




图11.1：填充(padding)操作示例：该例为向输人数据四周填充0像素(右图

中灰色区域)。

11.1.3汇合层参数的设定

同卷积核大小类似,汇合层的核大小一般也设为较小的值,如 2x2, 3x3 等。

常用的参数设定为2 x 2、汇合步长为2。在此设定下，输出结果大小仅为输人 数据长宽大小的四分之一，也就是说输人数据中有75%的响应值(activation values)被丢弃，这也就起到了 “下采样”的作用。为了不丢弃过多输人响应而 损失网络性能，汇合操作极少使用超过3 x 3大小的汇合操作。

11.2训练技巧
信息论(information theory)中曾提到：“从不相似的事件中学习总是比从相似 事件中学习更具信息量(Learning that an unlikely event has occurred is more informative than learning that a likely event has occurred.) ”。在训练卷积神 经网络时，尽管训练数据固定，但由于采用了随机批处理(mini-batch)的训 练机制，因此我们可在模型每轮(epoch)训练进行前将训练数据集随机打乱 (shuffle),确保模型不同轮数相同批次“看到”的数据是不同的。这样的处理不 仅会提高模型收敛速率，同时，相对固定次序训练的模型，此操作会略微提升

模型在测试集上的预测结果。

11.2.2学习率的设定

模型训练时另一关键设定便是模型学习率(learning rate), 一个理想的学习率

会促进模型收敛，而不理想的学习率甚至会直接导致模型直接目标函数损失值

爆炸”无法完成训练。学习率设定时可遵循下列两项原则：

1.模型训练开始时的初始学习率不宜过大，以0.01和0.001为宜；如发现刚 开始训练没几个批次(mini-batch)模型目标函数损失值就急剧上升，这 便说明模型训练的学习率过大，此时应减小学习率从头训练；

2.模型训练过程中，学习率应随轮数增加而减缓。减缓机制可有不同，一般 为如下三种方式：a)轮数减缓(step decay)。如五轮训练后学习率减半， 下一个五轮后再次减半；b)指数减缓(exponential decay)，即学习率按 训练轮数增长指数插值递减等，在MATLAB中可指定20轮每轮学习率 为 “lr = logspace(le-2，le-5,20)”； c)分数减缓(1/t decay)。若原始 学习率为lr。，学习率按照下式递减——lrt = lro/(1 + kt),其中k为超参 数用来控制学习率减缓幅度，t为训练轮数(epoch)。

除此之外，寻找

练曲线(learning cu

标函数上的损失值保

训练曲线与图中曲线

直接“爆炸”(黄色

络；若模型一开始损

较小学习率从头训练














想学习率或诊断模型训练学习率是否合适时可借助模型训 rve)的帮助。训练深度网络时不妨将每轮训练后模型在目 :存，以图11.2所示形式画出其训练曲线。读者可将自己的

“对号人座”：若模型损失值在模型训练刚开始的几个批次

线)，则学习率过大，此时应大幅减小学习率从头训练网

失值下降明显，但“后劲不足”(绿色曲线)，此时应使用

，或在后几轮改小学习率仅重新训练后几轮即可；若模型

(蓝色曲线)，此时应稍微加大学习率，然后继续观察训练

红色曲线所示的理想学习率下的训练曲线为止。此外，微

神经网络过程中，学习率有时也需特别关注，具体请参见第


训练损失



图11.2:不同学习率下训练损失值（loss）随训练轮数增加呈现的状态。

11.2.3批规范化操作

训练更深层的神经网络一直是深度学习中提高模型性能的重要手段之一。2015 年初，Google提出了批规范化操作（batch normalization,简称BN） [46]，不 仅加快了模型收敛速度，而且更重要的是在一定程度缓解了深层网络的一个难 题“梯度弥散”，从而使得训练深层网络模型更加容易和稳定。另外，批规范化 操作不光适用于深层网络，对传统的较浅层网络而言，批规范化也能对网络泛 化性能起到一定提升作用。目前批规范化已经成为了几乎所有卷积神经网络的 标配。

首先，我们来看一下批规范化操作（简称BN）的流程，见算法2。顾名思 义，“批规范化”，即在模型每次随机梯度下降训练时，通过mini-batch来对相 应的网络响应（activation）做规范化操作，使得结果（输出信号各个维度）的

























步。前两步分别计算批处理的数据均值和方差，第三步则 方差对该批数据做规范化。而最后的“尺度变换和偏移”操 I练所需而“刻意”加人的BN能够有可能还原最初的输人 :i）=卯和^ = E（xi）=卵时），从而保证整个网络的容量


ty)

•有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大

算法2批规范化算法BN

输人：批处理(mini-batch)输人x: B = {xi,…，m} 输出：规范化后的网络响应{yi = BNY，/3(Xi)}

1： MB    mm £i=i Xi H计算批处理数据均值

2: ♦ m zm=i(xi - mb)2 //计算批处理数据方差 3： Xi    ^+JI规范化

4： yi    Yxi + P = BNY„8(Xi) // 尺度变换和偏移

5： return学习的参数Y和P+


至于BN奏效的原因，需要首先来说 covariate shift)。读者应该知道在统计机器 (source domain)和目标空间(target doma 致的”。如果不一致，那么就出现了新的机




















ial

间


learning/domain adaptation)等。而协变量偏移(covariate shift)就是分布不 一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的， 但是其边缘概率不同，即：对所有x G X, Ps(Y|X = x) = Pt(Y|X = x),但 Ps(x) = Pt(x)。各位细想便会发现：的确，对于神经网络的各层输出，由于它 们经过了层内操作作用,其分布显然与各层对应的输人信号分布不同,而且差 异会随着网络深度增大越来越大，不过它们所“指示”的样本标记(label)仍 然保持不变,这便符合了协变量偏移的定义。由于是对层间信号的分析,也即 是“内部” (internal) 一称的由来。在实验中，Google的研究人员发现可通过 BN来规范化某些层或所有层的输人，从而可以固定每层输人信号的均值与方 差。这样一来，即使网络模型较深层的响应或梯度很小，也可通过BN的规范 化作用将其的尺度变大,以此便可解决深层网络训练很可能带来的“梯度弥散” 问题。一个直观的例子：对一组很小的随机数做12规范化操作2。这组随机数


可能会改变某层原来的输入。当然也可能不改变原始输入，此时便需要BN能做到“还原原来输

入”。如此一来，既可以改变同时也可以保持原输人，那么模型的容纳能力(capacity)便提升了。

2假设有向量V则向量的12规范化操作为：ve v/||v||2。


如下：

v = [0.0066, 0.0004,0.0085,0.0093,0.0068, 0.0076,0.0074,0.0039,0.0066, 0.0017]T .

在12规范化后，这组随机数变为：

v; = [0.3190,0.0174,0.4131, 0.4544, 0.3302,0.3687,0.3616, 0.1908, 0.3189,0.0833]T .

显然，经过规范化作用后，原本微小的数值其尺度被“拉大”了，试想如果未做 规范化的那组随机数v就是反向传播的梯度信息，规范化自然可起到缓解“梯 度弥散”效应的作用。

关于BN的使用位置，在卷积神经网络中BN —般应作用在非线性映射函 数前。另外，若神经网络训练时遇到收敛速度较慢，或“梯度爆炸”等无法训 练的状况发生时也可以尝试用BN来解决。同时，常规使用情况下同样可加人 BN来加快模型的训练速度，甚至提高模型精度。实际应用方面，目前绝大多 数开源深度学习工具包（如Caffe19 , torch20, theano21和MatConvNet22等）均已 提供了 BN的具体实现供使用者直接调用。

值得一提的是，BN的变种也作为一种有效的特征处理手段应用于人脸识别 等任务中，即特征规范化（feature normalization,简称FN） [32]。FN作用于 网络最后一层的特征表示上（FN的下一层便是目标函数层），FN的使用可提 高习得特征的分辨能力，适用于类似人脸识别（face recognition）、行人重检测 （person re-identification）、车辆重检测（car re-identification）等任务。

前一篇基础理论篇我们曾介绍过，深度卷积神经网络通常采用随机梯度下降类

型的优化算法进行模型训练和参数求解。经过近些年相关领域的发展，出现了

一系列有效的网络训练优化新算法，而且在实际使用中，许多深度学习工具箱

（如Theano等）均提供了这些优化策略的实现，工程实践时只需根据自身任务 的需求选择合适的优化方法即可。这里，本节以其中几种一阶优化算法的代表 为例，通过对比这些优化算法的形式化定义，介绍这些优化算法的区别以及选 择建议，至于算法详细繁杂的推导过程读者若有兴趣可参见原文献。以下介绍 中为简化起见，我们假设待学习参数为^,学习率(或步长)为n, —阶梯度值 为 g， t 表示第 t 轮训练。

随机梯度下降法 经典的随机梯度下降(Stochastic Gradient Descent,简称SGD)是神经网络训 练的基本算法，即每次批处理训练时计算网络误差并作误差的反向传播，后根 据一阶梯度信息对参数进行更新，其更新策略可表示为：

其中，一阶梯度信息g完全依赖于当前批数据在网络目标函数上的误差，故可 将学习率n理解为当前批的梯度对网络整体参数更新的影响程度。经典的随机 梯度下降是最常见的神经网络优化方法，收敛效果较稳定，不过收敛速度过慢。

基于动量的随机梯度下降法

受启发于物理学研究领域研究，基于动量(momentum)的随机梯度下降用于 改善SGD更新时可能产生的振荡现象，通过积累前几轮的“动量”信息辅助参 数更新，其更新策略可表示为：

(11+2)

叫卜叫-1+vt.    (11.3)

其中，M为动量因子，控制动量信息对整体梯度更新的影响程度，一般设为0.9。 基于动量的随机梯度下降法除了可以抑制振荡，还可在网络训练中后期趋于收 敛、网络参数在局部最小值附近来回震荡时帮助其跳出局部限制，找到更优的 网络参数。另外，关于动量因子，除了设定为0.9的静态设定方式，还可将其 设置为动态因子。一种常用的动态设定方式是将动量因子初始值设为0.5,之后 随着训练轮数的增长逐渐变为0.9或0.99。

Nesterov型动量随机下降法

Nesterov类型动量随机梯度下降方法是在上述动量梯度下降法更新梯度时加人 对当前梯度的校正(式11+4和式11.5)。相比一般动量法，Nesterov型动量法 对于凸函数在收敛性证明上有更强的理论保证，同时在实际使用中，Nesterov 型动量法也有更好的表现。具体为：

^ahead    叫一1 + M • vt-1 ,    (H-4)

vt


(11+5) (11+6)

其中，VWahead表示Wahead的导数信息。

可以发现，无论是随机梯度下降、基于动量的随机下降法，还是Nesterov

型的动量随机下降法,这些优化算法都是为了使梯度更新更加灵活,这对于优 化神经网络这种拥有非凸且异常复杂函数空间的学习模型尤为重要。不过,这 些方法依然有自身的局限。我们都知道稍小的学习率更加合适网络后期的优化, 但这些方法的学习率n却一直固定不变，并未将学习率的自适应性考虑进去。

Adagrc












根据训练轮数的不同,对学习率进行


nt


"global


• gt.


(11+7)








数(通常设定为 10-6 数量级)以防止分母为零。在网络训 p梯度的累加(m)较小，这一动态调整可放大原步长 K后期分母中梯度累加较大时，式11.7可起到约束原步长的

作用。不过，Adagrad法仍需人为指定一个全局学习率同时，网络训练 到一定轮数后，分母上的梯度累加过大会使得学习率为0而导致训练过早结束。

Adadelta 法

Adadelta法［94］是对Adagrad法的扩展，通过引人衰减因子p消除Adagrad 法对全局学习率的依赖，具体可表示为：


rt p • rt-1 + 1


nt


st


rt

St    P • st-i + (1 -


其中，P为区间［0,1］间的实值：较大的P值会促进网络更新；较小的P值会抑 制更新。两个超参数的推荐设定为P = 0.95, e = 10-6。


RMSProp 法


RMSProp 法［82］可视 掉Adadelta法中的s


(11+11)


(11+12)


式中P的作用与Adadelta法中的作用相同。不过，RMSProp法依然依赖全局 学习率是它的一个缺陷。实际使用中关于RMSProp法中参数设定一组推荐值


为"global 二


Adam 法

Adam法［49］本质上是带有动量项的RMSprop法，它利用梯度的一阶矩估计 和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校 正后，每一次迭代学习率都有一个确定范围，这样可以使得参数更新比较平稳。

mt    ^1 • mt-i 十（1 — A） • gt，    （11 + 13）


Vt    冷2 • Vt-1 十(1 -冷2) • gt2


(11+14)


mt


1-所’


Vt


vt

1-^1,




(11 + 17)

可以看出，Adam法仍然需指定基本学习率n,对于其中的超参数设定可遵循: 汍=0.9,灸=0.999, e = 10-8,n = 0.001。

11.2.5微调神经网络

本书在参数初始化一节层提到,除了从头训练自己的网络,一种更有效、高效

的方式是微调已预训练好的网络模型。微调预训练模型简单来说,就是用目标

任务数据在原先预训练模型上继续进行训练过程。这一过程中需注意以下细节：

1.    由于网络已在原始数据上收敛，因此应设置较小的学习率在目标数据上微 调,如 10-4 数量级或以下；

2.    在第3.1.3节曾提到，卷积神经网络浅层拥有更泛化的特征（如边缘、纹 理等）,深层特征则更抽象对应高层语义。因此,在新数据上微调时泛化 特征更新可能或程度较小,高层语义特征更新可能和程度较大,故可根据 层深对不同层设置不同学习率：网络深层的学习率可稍大于浅层学习率；

3.    根据目标任务数据与原始数据相似程度23采用不同微调策略：当目标数据 较少且目标数据与原始数据非常相似时,可仅微调网络靠近目标函数的后 几层；当目标数据充足且相似时,可微调更多网络层,也可全部微调；当 目标数据充足但与原始数据差异较大,此时须多调节一些网络层,直至微 调全部；当目标数据极少,同时还与原始数据有较大差异时,这种情形比 较麻烦,微调成功与否要具体问题具体对待,不过仍可尝试首先微调网络 后几层后再微调整个网络模型；

11.3.小结

4.此外，针对第三点中提到的“目标数据极少，同时还与原始数据

异”的情况,目前一种有效方式是借助部分原始数据与目标数

练。Ge和Yu [22]提出，因预训练模型的浅层网络特征更具泛 在浅层特征空间(shallow feature space)选择目标数据的近邻 neighbor)作为原始数据子集。之后，将微调阶段改造为多目标 (multi-task learning): 一者将目标任务基于原始数据子集，二者 务基于全部目标数据。整体微调框架如图11.3所示。实验证实， 调策略可大幅改善“目标数据极少,同时还与原始数据有较大差 下的模型微调结果(有时可取得约2至10个百分点的提升)。







务

任

微

况




图11.3: Ge和Yu [22]针对预训练网络模型微调的“多目标学习框架”示意图。

11.3小结
§ 本节介绍了深度卷积神经网络训练过程中需注意的细节设置,如网络输人 像素大小、网络层的超参数、学习率设置、不同的网络参数优化算法选择

及如


等；

关于图像样本输入大小设置，为方便GPU设备的并行计算，图像输入像 素一般设置为2的次幂；

卷积层和汇合层核大小最宜使用3 x 3或5 x 5等，同时可配合使用合适

像素大小的填充操作；

§关于学习率的设定，建议模型训练开始时设置0.01或0.001数量级学习 率,并随网络训练轮数增加逐渐减缓学习率,另外可通过观察模型训练曲 线判断学习率是否合适以及如何调整模型学习率；

§ 批规范化操作可一定程度缓解深层网络训练时的“梯度弥散”效应,一般

将批规范化操作设置于网络的非线性映射函数之前,批规范化操作可有效

提高模型收敛率；

§ 关于模型参数的优化算法选择。随机梯度下降法是目前使用最多的网络训 练方法,通常训练时间较长,但在理想的网络参数初始化和学习率设置方 案下,随机梯度下降法得到的网络更稳定,结果更可靠；若希望网络更

快收敛且需要训练较复杂结构的网络时，推荐使用Adagrad、Adadelta、 RMSProp 和 Adam 等优化算法；一■般来讲，Adagrad、Adadelta、RMSprop 和Adam是性能相近算法，在相同问题上表现并无较大差异；类似地，随 机梯度下降、动量法随机梯度下降和Nesterov型动量法也是性能相近算 法；上述优化算法均为一阶梯度方法,事实上,基于牛顿法的二阶优化方 法也是存在的（如limited-memory BFGS法［14,76］） „但是直接使用二 阶方法用于深度卷积网络优化目前来看并不现实,因为此类方法需在整体 （海量）训练集上计算海森矩阵24,会带来巨大的计算代价。因此，目前对 于深度网络模型的实际应用,训练网络的优化算法仍以上述一阶梯度算法 为主。基于批处理的二阶网络训练方法则是当下学术界深度学习领域的研 究热点之一；

§ 微调预训练模型时,需注意学习率调整和原始数据与目标数据的关系。另 外,还可使用“多目标学习框架”对预训练模型进行微调。























* * *




# COMMENT
