


# ORIGINAL






  1. 《解析卷积神经网络》魏秀参




# TODO






  * aaa





* * *





# INTRODUCTION






  * aaa




卷积神经网络基本

在了解了深度卷积神

一些重要部件(或模

从原始数据(raw d

2.1

深度学习的一个重 表示学习(represe: 学习算 selectio

假设样

学习时

难为无

端到端

1勺

使得卷积神经网络可以直接

终任务。

(end-to-end manner),属 深度学习区别于其他机器

其他机器学习算法，如特征选择算法(feature 〔法、集成学习(ensemble learning)算法等，均 在此基础上设计具体的机器学习算法。在深度

使用人工特征(hand-crafted feature),但“巧妇

征的优劣往往很大程度决定了最终的任务精度。

习分支-特征工程(feature engineering)。特

tation learning)是一个广泛的概念，并不特指深度学习。实际上在深度学 I于表示学习的研究，如“词包”模型(bag-of-word model)和浅层自动编 □ders)等。

征工程在数据挖掘的工业界应用以及计算机视觉应用中都是深度学习时代之前

非常重要和关键的环节。

特别是计算机视觉领域，在深度学习之前，针对图像、视频等对象的表

示可谓“百花齐放、百家争鸣”。仅拿图像表示(image representation)举 例，从表示范围可将其分为全局特征描述子(global descriptor)和局部特征 (local descriptor)，而单说局部特征描述子就有数十种之多，如SIFT [62]、

PCA-SIFT [48]、 SURF [2]、HOG [13]、 steerable filters [18]......同时，不同局

部描述子擅长的任务又不尽相同，一些适用于边缘检测、一些适用于纹理识 别，这便使得实际应用中挑选合适的特征描述子成为一件令人头疼的麻烦 事。对此，甚至有研究者于2004年在相关领域国际顶级期刊TPAMI (IEEE Transactions on Pattern Recognition and Machine Intelligence)上发表实验性 综述 “A Performance Evaluation of Local Descriptors” [66]来系统性的理解不 同局部特征描述子的作用，至今已获得近8000次引用。而在深度学习普及之 后，人工特征已逐渐被表示学习根据任务自动需求“学到”的特征表示所取代5。

更重要的是，过去解决一个人工智能问题(以图像识别为例)往往通过分 治法将其分解为预处理、特征提取与选择、分类器设计等若干步骤。分治法的 动机是将图像识别的母问题分解为简单、可控且清晰的若干小的子问题。不过 分步解决子问题时，尽管可在子问题上得到最优解，但子问题上的最优并不意 味着就能得到全局问题的最后解。对此，深度学习则为我们提供了另一种范式 (paradigm)即“端到端”学习方式，整个学习流程并不进行人为的子问题划分， 而是完全交给深度学习模型直接学习从原始输入到期望输出的映射。相比分治 策略，“端到端”的学习方式具有协同增效的优势，有更大可能获得全局最优解。

如图2.1所示，对深度模型而言，其输人数据是未经任何人为加工的原始样 本形式，后续则是堆叠在输入层上的众多操作层。这些操作层整体可看作一个 复杂的函数fcNN,最终损失函数由数据损失(data loss)和模型参数的正则化

职参始为下

re行原射 。 {进从映用 失型为据作 损模象数的

组传接随/

同向直}成

的 。^I'B本本

模各而为个

度rn”，射各

失可 5 务 损程尨做 终过陆耐 最隊餅目

在S觸即

则的1,—

对抽

o

n

o

a

iz

a

gu

re

失

损

卯

正

原始数据

(xi,yi)

n标凼数

数据损5

图。

2.2网络符号定义
同上一章类似[89],在此用三维张量x1 G RHlxWlxDl表示卷积神经网络第l 层的输人，用三元组(i1 ,j1,d1)来指示该张量对应第i1行，第j1列，第d1通 道(channel)位置的元素，其中 0 S i1 < H1, 0 < j1 < W1, 0 < d1 < D1,如图 2.2所示。不过，一般在工程实践中，由于采用了 mini-batch训练策略，网络第 l层输人通常是一个四维张量，即x1 G RHlxWlxDxN，其中N为mini-batch 每一批的样本数。

以N = 1为例，x1经过第l层操作处理后可得x1+1,为了后面章节书写方便, 特将此简写为y以作为第l层对应的输出，即y = x1+1 g RHl+1xW:+〜以十1。

图2+2:卷积

2.3卷积层
卷积层(convolution layer)是卷积神经网络中的基础操作，甚至在网络最后起 分类作用的全连接层在工程实现时也是由卷积操作替代的。

2.3.1什么是卷积？

卷积运算实际是分析数学中的一种运算方式，在卷积神经网络中通常是仅涉及 离散卷积的情形。下面以 dl =1的情形为例介绍二维场景的卷积操作。

假设输人图像(输人数据)为如图2.3中右侧的5 x 5矩阵，其对应的卷积核 (亦称卷积参数，convolution kernel 或 convolution filter)为一■个 3 x 3 的矩阵。 同时，假定卷积操作时每做一次卷积，卷积核移动一个像素位置，即卷积步长 (stride)为 1。

第一次卷积操作从图像(0,0)像素开始，由卷积核中参数与对应位置图像像 素逐位相乘后累加作为一次卷积操作结果，即 1x 1+2x 0+3x 1+6x 0+

7 x 1 + 8 x 0 + 9 x 1 + 8 x 0 + 7 x 1 = 1+ 3 + 7 +9 + 7 = 27,如图 2.4a所示。 类似地，在步长为1时，如图2.4b至图2.4d所示，卷积核按照步长大小在输人 图像上从左至右自上而下依次将卷积操作进行下去，最终输出 3 x 3 大小的卷 积特征，同时该结果将作为下一层操作的输人。

与之类似，若三维情形下的卷积层l的输人张量为x1 G RHlxWl-Dl，该层 卷积核为f1 e RHxWxDl。三维输人时卷积操作实际只是将二维卷积扩展到了 对应位置的所有通道上(即D'1)，最终将一次卷积处理的所有HWD1个元素

2.3.卷积层

1

0

1

0

1

0

1

0

1

卷积核

1

2

3

4

5

6

7

8

9

0

9

8

7

6

5

4

3

2

1

0

1

2

3

4

5

图2.3:二维场景下的卷积核与输人数据。图左为一个3 x 3的卷积核，图右为 5x5的输人数据。

求和作为该位置卷积结果。如图2.5所示。

进一步地，若类似f1这样的卷积核有D个，则在同一个位置上可得到

1 X 1 x 1 x D维度的卷积输出，而D即为第l +1层特征x1+1的通道数D1+1。 形式化的卷积操作可表示为：

yil+1,j

1+i,jl+1+j,dl

(2+1)

其中， (il+1,jl+1) 为卷

(2+2) (2+3)

l+1

需指出的是，式2+1中的fi,j,di,d可视作学习到的权重(weight),可以发现该 项权重对不同位置的所有输人都是相同的，这便是卷积层“权值共享”(weight sharing)特性。除此之外，通常还会在yii+i,ji+i,d上加人偏置项(bias term) bd。在误差反向传播时可针对该层权重和偏置项分别设置随机梯度下降的学习 率。当然根据实际问题需要，也可以将某层偏置项设置为全0,或将学习率设置 为0,以起到固定该层偏置或权重的作用。此外，卷积操作中有两个重要的超参 数(hyper parameters):卷积核大小(filter size)和卷积步长(stride)。合适的 超参数设置会对最终模型带来理想的性能提升，详细内容请参见第11.1节内容。

(a)第一次卷积操作及得到的卷积特征

:b)第二次卷积操作及得到的卷积特征。

(c)第三次卷积操作及得到的卷积特征

:d)第九次卷积操作及得到的卷积特征。

图2+4:卷积操作示例。

2.3.2

可以看出卷积是一种局部操作,通过一定大小的卷积核作用于局部图像区域获 得图像的局部信息。本节以三种边缘卷积核(亦可称为滤波器)来说明卷积神 经网络中卷积操作的作用。如图2.6,我们在原图上分别作用整体边缘滤波器、 横向边缘滤波器和纵向边缘滤波器，这三种滤波器(卷积核)分别为式2.4中的

0 -40

1

2

1

1

0

-1

-4 16 -4

，Kh =

0

0

0

，Kv =

2

0

-2

0 -40

-1

-2

-1

1

0

-1

(2+4)

试想，若原图像素(x,y)处可能存在物体边缘，则其四周(x- 1,y), (x +1,y), (x,y - 1), (x,y +1)处像素值应与(x,y)处有显著差异。此时，如作用以整体 边缘滤波器Ke,可消除四周像素值差异小的图像区域而保留显著差异区域，以

2.4.汇合层

图2+5:三维场景下的卷积核与输人数据。图左卷积核大小为3 x 4 x 3,图右为 在该位置卷积操作后得到的 1 x 1 x 1 的输出结果。

此可检测出物体边缘信息。同理，类似Kh和Kv6的横向、纵向边缘滤波器可 分别保留横向、纵向的边缘信息。

事实上,卷积网络中的卷积核参数是通过网络训练学出的,除了可以学到类 似的横向、纵向边缘滤波器,还可以学到任意角度的边缘滤波器。当然,不仅 如此，检测颜色、形状、纹理等等众多基本模式(pattern)的滤波器(卷积核) 都可以包含在一个足够复杂的深层卷积神经网络中。通过“组合” 7这些滤波器 卷积核)以及随着网络后续操作的进行,基本而一般的模式会逐渐被抽象为具 有高层语义的“概念”表示,并以此对应到具体的样本类别。颇有“盲人摸象” 后,将各自结果集大成之意。

2.4汇合层
本节讨论第l层操作为汇合(pooling) 8时的情况。通常使用的汇合操作为平 均值汇合(average-pooling)和最大值汇合(max-pooling),需要指出的是, 同卷积层操作不同，汇合层不包含需要学习的参数。使用时仅需指定汇合类 型(average或max等)、汇合操作的核大小(kernel size)和汇合操作的步长

(a)原图。

(c)横向边缘滤波器K

(stride

2.4.1什么是汇合？

遵循上一节的记号，第l层汇合核可表示为P1 G RHxWxDl。平均值(最大值)

汇合在每次操作时,将汇合核覆盖区域中所有值的平均值(最大值)作为汇合

结果，即：

y? xil+1 xH+i,jl+1 xW+j,dl .

0<i<H,0<j<W

(2.5)

(2.6)

yiw，ji+'d = 0<i<maxj<wxii+1 xh+i,jl+1 xW+j,dl ,

其中，0 S V+1 < H*1, 0 g 夕十1 < W*1, 0 g d < Dl+i = Dl。

图2.7为2 x 2大小、步长为1的最大值汇合操作示例。

除了最常用的上述两种汇合操作外，随机汇合(stochastic-pooling) [95]则

介于二者之间。随机汇合操作非常简单，只需对输人数据中的元素按照一定概

率值大小随机选择，并不像最大值汇合那样永远只取那个最大值元素。对随机

2.4.汇合层

(a)第一次汇合操作及得到的汇合特征。

图2+7:最大值汇合操作示例

汇合而言，元素值大的响应(activation)被选中的概率也大，反之易然。可以 说,在全局意义上,随机汇合与平均值汇合近似；在局部意义上,则服从最大 值汇合的准则。

2.4.2汇合操作的作用

在上图的例子中可以发现,汇合操作后的结果相比其输人降小了,其实汇合操 作实际上就是一种“降采样”(down-sampling)操作。另一方面，汇合也看成 是一个用P■范数(p-norm) 9作为非线性映射的“卷积”操作，特别的，当p趋 近正无穷时就是最常见的最大值汇合。

汇合层的引人是仿照人的视觉系统对视觉输人对象进行降维(降采样)和抽

象。在卷积神经网络过去的工作中,研究者普遍认为汇合层有如下三种功效：

1. 特征不变性(feature invariant)。汇合操作使模型更关注是否存在某些特 征而不是特征具体的位置。可看作是一种很强的先验,使特征学习包含某

种程度自由度,能容忍一些特征微小的位移。

2. 特征降维。由于汇合操作的降采样作用，汇合结果中的一个元素对应于原 输人数据的一个子区域(sub-region),因此汇合相当于在空间范围内做了 维度约减(spatially dimension reduction),从而使模型可以抽取更广范 围的特征。同时减小了下一层输人大小,进而减小计算量和参数个数。

3.在一定程度防止过拟合(overfitting),更方便优化

不过,汇合操作并不是卷积神经网络必须的元件或操作。近期,德国著名高校 弗赖堡大学(University of Freiburg)的研究者提出用一种特殊的卷积操作(即, “stride convolutional layer”)来代替汇合层实现降采样，进而钩建一个只含卷 积操作的网络(all convolution nets)，其实验结果显示这种改造的网络可以达 到、甚至超过传统卷积神经网络(卷积层汇合层交替)的分类精度［77］。

2.5激活函数
激活函数(activation function)层又称非线性映射(non-linearity mapping)层, 顾名思义,激活函数的引人为的是增加整个网络的表达能力(即非线性)。否则, 若干线性操作层的堆叠仍然只能起到线性映射的作用,无法形成复杂的函数。 在实际使用中,有多达十几种激活函数可供选择,有关激活函数选择和对比的 详细内容请参见第8章。本节以Sigmoid型激活函数和ReLU函数为例，介绍 涉及激活函数的若干基本概念和问题。

直观上,激活函数模拟了生物神经元特性：接受一组输人信号并产生输出。 在神经科学中,生物神经元通常有一个阈值,当神经元所获得的输人信号累积 效果超过了该阈值,神经元就被激活而处于兴奋状态；否则处于抑制状态。在 人工神经网络中，因Sigmoid型函数可以模拟这一生物过程，从而在神经网络

发展历史进程中曾处于相当重要的地位。

(j(x)=

1

1 + exp(-x)

(2+7)

其函数形状如图2.8a所示。很明显能看出，经过Sigmoid型函数作用后，输出 响应的值域被压缩到［0,1］之间，而0对应了生物神经元的“抑制状态”，1则 恰好对应了 “兴奋状态”。不过再深人的观察还能发现在Sigmoid函数两端，对 于大于5 (或小于-5)的值无论多大(或多小)都会压缩到1 (或0)。如此便 带来一个严重问题，即梯度的“饱和效应”(saturation effect)。对照Sigmoid 型函数的梯度图(图2.8b),大于5 (或小于-5)部分的梯度接近0,这会导致

2.5.激活函数

在误差反向传播过程中导数处于该区域的误差将很难甚至根本无法传递至前层，

进而导致整个网络无法训练（导数为0将无法更新网络参数）。此外，在参数

初始化的时候还需特别注意，要避免初始化参数直接将输出值域带人这一区域：

一种可能的情形是当初始化参数过大时，将直接引发梯度饱和效应而无法训练。

（a） Sigmoid 型函数 （b） Sigmoid 型

图2+8: Sigmoid型函数及其函数梯度

为了避免梯度饱和效应的发生，Nair和Hinton于2010年将修正线性单元 （Rectified Linear Unit,简称ReLU）引人神经网络［69］。ReLU函数是目前深 度卷积神经网络中最为常用的激活函数之一。另外，根据ReLU函数改进的其 他激活函数也展示出更好的性能（请参见第8章内容）。

ReLU函数实际上是一个分段函数，其定义为：

0 if x < 0

(2+8)

(2+9)

由图2+9可见，ReLU函数的梯度在x》0时为1,反之为0。对x》0部 分完全消除了 Sigmoid型函数的梯度饱和效应。同时，在实验中还发现相比 Sigmoid型函数，ReLU函数有助于随机梯度下降方法收敛，收敛速度约快6 倍左右［52］。正是由于ReLU函数的这些优质特性，ReLU函数已成为目前卷 积神经网络及其他深度学习模型（如递归神经网络RNN等）激活函数的首选

图2+9: ReLU函数及其函数梯度

2.6全连接层
全连接层（fully connected layers）在整个卷积神经网络中起到“分类器”的作 用。如果说卷积层、汇合层和激活函数层等操作是将原始数据映射到隐层特征 空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。 在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以 转化为卷积核为1x 1的卷积；而前层是卷积层的全连接层可以转化为卷积核 为h x w的全局卷积，h和w分别为前层卷积输出结果的高和宽。以经典的 VGG-16 [74]网络模型10为例，对于224 x 224 x 3的图像输人，最后一层卷积 层（指VGG-16中的Pools）可得输出为7 x 7 x 512的特征张量，若后层是一 层含4096个神经元的全连接层时，则可用卷积核为7 x 7 x 512 x 4096的全局 卷积来实现这一全连接运算过程,其中该卷积核具体参数如下：

1 % The first fully connected layer

2 filter_size = 7; padding = 0; stride = 1;

3 D_in = 512; D_out = 4096;

经过此卷积操作后可得1x1x4096的输出。如需再次叠加一个含2048个神 经元的全连接层,可设定以下参数的卷积层操作。

2.7.目标函数

1 % The second fully connected layer

2 filter_size = 1; padding = 0; stride = 1;

3 D_in = 4096; D_out = 2048;

2.7目标函数
刚才提到全连接层是将网络特征映射到样本的标记空间做出预测，目标函数的

作用则用来衡量该预测值与真实样本标记之间的误差。在当下的卷积神经网络 中，交叉熵损失函数和12损失函数分别是分类问题和回归问题中最为常用的目 标函数。同时，越来越多针对不同问题特性的目标函数被提出供选择。详细内 容请参见本书第9章内容。

2.8小结
§ 本章介绍了深度学习中的关键思想——“端到端”学习方式；

§ 介绍了卷积神经网络的基本部件：卷积操作、汇合操作、激活函数（非线

性映射）、全连接层和目标函数。整个卷积神经网络通过这些基本部件的

“有机组合”即可实现将原始数据映射到高层语义、进而得到样本预测标

记的功能。下一章将介绍卷积神经网络结构中的几个重要概念以及如何对

这些部件进行“有机组合”。

上一章内

连接层和

“纸上学来

能呢？本

神经网络





















* * *





# COMMENT



