


# ORIGINAL






  1. 《解析卷积神经网络》魏秀参




# TODO






  * aaa





* * *





# INTRODUCTION






  * aaa




卷积神经网络基础

卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相

图像语义分割

物体检测（ot 入，如自然语 数据挖掘（so 网络解决，并

汾类（image classification）、 3像检索（image retrieval）、 ?计算机视觉问题。此外，随着CNN研究的深 language processing）中的文本分类，软件工程 的软件缺陷预测等问题都在尝试利用卷积神经 法甚至其他深度网络模型更优的预测效果。

本章首先回顾卷积神经网络发展历程，接着从抽象层面介绍卷积神经网络的

基本结构，以及卷积神经网络中的两类基本过程：前馈运算（预测和推理）和

反馈运算（训练和学习）。

1.1发展历程
卷积神经网络发展历史中的第一件里程碑事件发生在上世纪60年代左右的 神经科学(neuroscience)中，加拿大神经科学家David H. Hubei和Torsten Wiesel于1959年提出猫的初级视皮层中单个神经元的“感受野” (receptive field)概念，紧接着于1962年发现了猫的视觉中枢里存在感受野、双目视觉和 其他功能结构，标志着神经网络结构首次在大脑视觉系统中被发现。1

图1.1: Torsten Wiesel (左)和David H. Hubei (右)。因其在视觉系统中信息 处理方面的杰出贡献，两人于1981年获得诺贝尔生理学或医学奖。

1980年前后，日本科学家福岛邦彦(Kunihiko Fukushima)在Hubei和 Wiesel工作的基础上，模拟生物视觉系统并提出了一种层级化的多层人工神经 网络，即“神经认知” (neurocognitron) [19],以处理手写字符识别和其他模式 识别任务。神经认知模型在后来也被认为是现今卷积神经网络的前身。在福岛 邦彦的神经认知模型中，两种最重要的组成单元是“S型细胞”(S-cells)和“C 型细胞”(C-cells),两类细胞交替堆叠在一起构成了神经认知网络。其中，S 型细胞用于抽取局部特征(local features)，C型细胞则用于抽象和容错，如图 1+2所示，不难发现这与现今卷积神经网络中的卷积层(convolution layer)和汇 合层(pooling layer)可一一对应。

随后，Yann LeCun等人在1998年提出基于梯度学习的卷积神经网络算

法[54],并将其成功用于手写数字字符识别，在那时的技术条件下就能取得低

1.1.发展历程

Us2^-

Uc3

k，1

图1.2: 1980年福岛邦彦提出

于1%的错误率。因此，LeNet这一卷积神 有的邮政系统，用来识别手写邮政编码进「 是第一个产生实际商业价值的卷积神经网络 展奠定了坚实的基础。鉴于此，Google在: 意将“L”大写，从而向“前辈” LeNet致

w

STet

J发 遊

图 1.3: LeNet-5 结构 “矩形”代表一张特 layer）。

时间来到2012 年 类竞赛四周年之际，

积神经网络。其中，每一个 !全连接层（fully connected

戸，在有计算机视觉界“世界杯”之称的ImageNet图像分

Geoffrey E. Hinton等人凭借卷积神经网络Alex-Net力挫 园牛津大学VGG组等劲旅，且以超过第二名近12%的准 B冠军［52］，霎时间学界业界纷纷惊愕哗然。自此便揭开了 ［机视觉领域逐渐称霸的序幕2，此后每年ImageNet竞赛的

隹生的2012年为计算机视觉领域中的深度学习元年。同时也有人将Hinton

冠军非深度卷积神经网络莫属。直到2015年，在改进了卷积神经网络中的激 活函数(activation function)后，卷积神经网络在ImageNet数据集上的性能 (4.94%)第一次超过了人类预测错误率(5.1%) [34]。近年来，随着神经网络特 别是卷积神经网络相关领域研究人员的增多、技术的日新月异，卷积神经网络 也变得愈宽愈深愈加复杂，从最初的5层、16层，到诸如MSRA提出的152 层Residual Net [36]甚至上千层网络已被广大研究者和工程实践人员司空见惯。

不过有趣的是，图1.4a为Alex-Net网络结构，可以发现在基本结构方面它 与十几年前的LeNet几乎毫无差异。但数十载间，数据和硬件设备(尤其是 GPU)的发展确实翻天覆地，它们实际上才是进一步助力神经网络领域革新的 主引擎。正是如此，才使得深度神经网络不再是“晚会的戏法”和象牙塔里的 研究，真正变成了切实落地可行的工具和应用手段。深度卷积神经网络自2012 年的一炮而红，到现在俨然已成为目前人工智能领域一个举足轻重的研究课题， 甚至可以说深度学习是诸如计算机视觉、自然语言处理等领域主宰性的研究技 术，同时更是工业界各大公司和创业机构着力发展力求占先的技术奇点。

1.2基本结构
总体来说，卷积神经网络是一种层次模型(hierarchical model)，其输人是原 始数据(raw data),如RGB图像、原始音频数据等。卷积神经网络通过卷 积(convolution)操作、汇合(pooling)操作和非线性激活函数(non-linear activation function)映射等一系列操作的层层堆叠，将高层语义信息逐层由原 始数据输人层中抽取出来，逐层抽象，这一过程便是“前馈运算” (feed-forward)。 其中，不同类型操作在卷积神经网络中一般称作“层”：卷积操作对应“卷积 层”，汇合操作对应“汇合层”等等。最终，卷积神经网络的最后一层将其目标 任务(分类、回归等)形式化为目标函数(objective function) 2 3。通过计算预 测值与真实值之间的误差或损失(loss),凭借反向传播算法(back-propagation algorithm [72])将误差或损失由最后一层逐层向前反馈(back-forward)，更新

1.2.基本结构

3 48

(a) AlexNet 结构［52"

图1.4: Alex-Net网络结构和Geoffrey E. Hinton。值得一提的，Hinton因其杰 出的研究成就，获得2016年度电气和电子工程师协会（IEEE）和爱丁堡皇家 科学会 DRoyal Society of Edinburgh）联合颁发的 James Clerk Maxwell 奖，以 表彰其在深度学习方面的突出贡献。

每层参数，并在更新 到模型训练的目的。

更通俗地讲，卷积 层作为“基本单元” 砌”，以损失函数的计 一个三维张量（tens：

A后再次前馈，如此往复，直到网络模型收敛，从而达

X只神经网络犹如搭积木的过程（如公式1.1）,将卷积等操作 ”依次“搭”在原始数据（公式1.1中的X1）上，逐层“堆 f计算（公式1.1中的Z）作为过程结束，其中每层数据形式是 Lsor）。具体地D在计算机视觉应用中，卷积神经网络的数据 包空间的图像：H行，W）列，3个通道（分别为R, G, B）, 经过第一层操作可得x2,对应第一层操作中的参数记为^1; 层W的输人，可得x3……直到第L- 1层，此时网络输出

程中,理论上每层操5作层可为单独卷积操作、汇合操作、非

线性映射或其他操作/变換，当然也可以是不同形式操作/变換的组-

最后，整个网络以损失函 (ground truth),则损失函数表

应的真实标记

其中，函数L( + )中的参数即 其参数W是可以为空的，如" 数的计算等。实际应用中，对 回归问题为例，常用的12损:

z = Lregression(xL，y) = 2 ||尤【-

交叉墒(cross entropy)损失函$ 其中 Pi = EjESfe (i = 1-归问题还是分类问题，在计算 xL，方可正确计算样本预测的 本书第2.7节。

(xL,y), (1+2)

实上，可以发现对于层中的特定操作，

作、无参的非线性映射以及无参损失函

任务，损失函数的形式也随之改变。以

即可作为卷积网络的目标函数，此时有

若对于分类问题，网络的目标函数常采用

=(classification (x^, y) = — ^2 i yi log(pi)，

)， C 为分类任务类别数。显然，无论回 需要通过合适的操作得到与 y 同维度的 差值。有关不同损失函数的对比请参见

练模型时计算误差还是模型训练完毕后获得样本预测，卷积神经网络的 :feed-forward)运算都较直观。同样以图像分类任务为例，假设网络已训 5,即其中参数^1,...,^L-1已收敛到某最优解，此时可用此网络进行图 预测。预测过程实际就是一次网络的前馈运算：将测试集图像作为网络 :1送进网络，之后经过第一层操作w1可得x2,依此下去……直至输出 RC。上一节提到，xL是与真实标记同维度的向量。在利用交叉墒损失函 ;后得到的网络中，xL的每一维可表示x1分别隶属C个类别的后验概

1.4.反馈运算

率。如此，可通过下式得到输人图像x1对应的预测标记:

arg max xL . i

1.4反馈运算
同其他许多机器学习模型(支持向量机等)一样,卷积神经网络包括其他所有

深度学习模型都依赖最小化损失函数来学习模型参数，即最小化式1.2中的z。 不过需指出的是，从凸优化理论来看，神经网络模型不仅是非凸(non-convex) 函数而且异常复杂,这便带来优化求解的困难。该情形下,深度学习模型采用 随机梯度下降法(Stochastic Gradient Descent,简记为SGD)和误差反向传播 (error back propogation)进行模型参数更新。有关随机梯度下降法详细内容可 参见附录B。

具体来讲,在卷积神经网络求解时,特别是针对大规模应用问题(如, ILSVRC分类或检测任务)，常采用批处理的随机梯度下降法(mini-batch SGD)。批处理的随机梯度下降法在训练模型阶段随机选取n个样本作为一批 (batch)样本，先通过前馈运算得到预测并计算其误差，后通过梯度下降法更 新参数,梯度从后往前逐层反馈,直至更新到网络的第一层参数,这样的一个 参数更新过程称为一个“批处理过程”(mini-batch)。不同批处理之间按照无放 回抽样遍历所有训练集样本，遍历一次训练样本称为“一轮”(epoch4)。其中, 批处理样本的大小(batch size)不宜设置过小。过小时(如batch size为1，2 等),由于样本采样随机,按照该样本上的误差更新模型参数不一定在全局上最 优(此时仅为局部最优更新),会使得训练过程产生振荡。而批处理大小的上限 则主要取决于硬件资源的限制，如GPU显存大小。一般而言，批处理大小设 为32, 64, 128或256即可。当然在随机梯度下降更新参数时，还有不同的参 数更新策略，具体可参见第11章有关内容。

下面我们来看误差反向传播的详细过程。按照第1.2节的记号，假设某批处

理前馈后得到n个样本上的误差为z，且最后一层L dz

d^L =0 ,

不难发现，实际上每层操作都

数的导数dZi，另一部分是误差

d

关于第 i 层参

关于参数w的导数

dz

dw

n是每次随机梯度下降的 细内容请参见第11.2.2节

关于输入 x4 的导数 误差从最后一层传递

dz

dx4

下面以 第 i +1 值。根据

^ - nd^i, (1-6)

-般随训练轮数(epoch)的增多减小，详

差向前层的反向传播。可将其视作最终

差信号。

更新信号(导数)反向传播至第 i 层时，

昏参数更新时需计算d^和dX^的对应

d vec(x4+4) i+4)T) d(ve«), z d vec(x4+4)

(1+7)

(1+8)

d (vec(x4)T) d (vec(x4+4)T) d (vec(x4)T)

:用向量标记“vec”是由于实际工程实现时张量运算均转化为向量运算。 量运算和求导可参见附录A。前面提到，由于第i + 1层时已计算得到 在第 i 层用于更新该层参数时仅需对其做向量化和转置操作即可得到 I+1)T)，即公式1.7和1.8中等号右端的左项。另一方面，在第i层，由于 直接作用得x4+4,故反向求导时亦可直接得到其偏导数和

1.5.小结

算法1反向传播算法

输人：训练集(N个训练样本及对应标记)(xn, yn), n =1,...,N;训练轮数 (epoch): T

输出：^ , i = 1,…，L l： for t =1 ...T do 2: while训练集数据未遍历完全do

3： 前馈运算得到每层V，并计算最终误差Z;

4： for i = L... 1 do

5: (a)用公式1 + 7反向计算第i层误差对该层参数的导数：d(vecd：i)');

6： (b)用公式1 + 8反向计算第i层误差对该层输人数据的导数:

_dz_.

d(vec(xi)T) 7

7:

8:

9:

(C)用公式1.6更新参数：

end for

end while

10： end for

ll： return ^i.

当然,上述方法是通过手动书写导数并用链式法则计算最终误差对每层不 同参数的梯度,之后仍需通过代码将其实现。可见这一过程不仅繁琐,且容易 出错,特别是对一些复杂操作,其导数很难求得甚至无法显式写出。针对这种 情况，一些深度学习库，如Theano和Tensorflow都采用了符号微分的方法进 行自动求导来训练模型。符号微分可以在编译时就计算导数的数学表示,并进 一步利用符号计算方式进行优化。在实际应用时,用户只需把精力放在模型构 建和前向代码书写上,不用担心复杂的梯度求导过程。不过,在此需指出的是, 读者有必要对上述反向梯度传播过程了解,也要有能力求得正确的导数形式。

本章回顾了卷积神经网络自1959年至今的发展历程;

介绍了卷积神经网络的基本结构,可将其理解 作层的“堆叠”将原始数据表示(raw data rep 人为干预直接映射为高层语义表示(high-level s 并实现向任务目标映射的过程——这也是为何 端” (end-to-end)学习或作为“表示学习” (repr 最重要代表的原因；

learning）中

§ 介绍了卷积神经网络中的两类基本过程：前馈运算和反馈运算。神经网络 模型通过前馈运算对样本进行推理(inference)和预测(prediction),通 过反馈运算将预测误差反向传播逐层更新参数,如此两种运算依次交替迭 代完成模型的训练过程。





















* * *





# COMMENT



