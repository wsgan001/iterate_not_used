
基本上是对整个课程的总结。


大纲：

- 课程总结
    - 深度学习的本质
    - 深度学习目前的进展
- 一个实例：鲸鱼识别



深度学习何处适用

深度学习最重要的作用是表示学习。
初始表示 与 合适表示 相距甚远时适用。<span style="color:red;">嗯，好像是这样。</span>

它做出来的对于图像的合适的表示， domain specific


深度学习为何有效

- 模型复杂度
- 大量数据
- 硬件崛起


往哪儿走：

- 鲁棒性： 高风险性的开放性的环境，比如自动驾驶
    - 可重用：比如别人的 VGG ，可以很方便迁移到自己的图像分类器里。还比较原始，因为仅限于某几个模型的图像到图像的迁移
    - 可演进：随着我的分布的迁移，我的识别还是比较有鲁棒性的
    - 可了解：模型最好是可解释的
- Learnware = model + specification 形成一种市场，某种模型和模型的接口，


自动驾驶，输入的可能是乱七八糟的，这个时候的鲁棒性怎么保证？

老师这个地方，关于 VGG 换了一个 head 就叫做迁移学习的问题，这个只能说是归迁移学习管，但是迁移学习要研究的范畴比这个大多了。



计算机视觉和机器学习的关系

机器学习方法解决CV问题具有通用性
Eg：CNN/LSTM 不仅仅可以用来做CV

深度学习： （目前）最少领域知识的绝决方案

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/cehAlHbE5d.png?imageslim)


深度学习 还是一种需要最少的机器学习理论的东西。






What makes AI great again

- 计算力 Compute (the obvious one: Moore’s Law, GPUs, ASICs),
- 高质量的数据 Data (in a nice form, not just out there somewhere on the internet - e.g. ImageNet),
- 算法与模型 Algorithms (research and ideas, e.g. backprop, CNN, LSTM), and
- 非常好用的平台，比如 tensorflow Infrastructure (software under you - Linux, TCP/IP, Git, TensorFlow,Theano, etc.).



关于深度学习，再说两件事情

## The Dark Knowledge

这个老师主要强调的是这种迁移学习的形式。

一个 普通的神经网络，和一个好用的复杂的网络。我们想把这个复杂的网络掌握的信息传给这个普通的神经网络，怎么办？

Hinton说，比如对于这个 VGG ，我输入了一张图片，然后，最后的 FC 是 0,0.2,0.1...等等，这个 label distributation，然后再过一个 sigmoid 然后变成 00010 。那么我直接把图片作为普通网络的输入，label distribution 作为普通网络的输出。来训练这个普通网络。训练的时候，相当于是回归。训练完接一个 softmax 然后输出分类。<span style="color:red;">惊了，这也行！！厉害，有点震惊。但是，我想问的是，这样的网络在 测试集上使用与真正的 VGG 会有多大差距？而且，为什么信息可以这样压缩？感觉这样应该是损失了一些样本空间中的可能到达的地方吧？比如 VGG 原先能到达很多地方，但是现在这个普通的网络只能到达一部分地方？要确认下。</span>

VGG 学出的 label distribution 比他预测的 0010 这样的label 包含了更多的信息。比如说分类是：猫、狗、猪、鸟。那么我对狗图片进行预测时：0.2、0.65、0.1、0.05 这个结果能说明 猫和狗比较近，比鸟和狗相比要近，而 0010 却没有这种信息。<span style="color:red;">真的是厉害。</span>


所以，如果你以后模型很大的时候，可以这么做，这就是迁移学习的一种方法，这样你就可以使用一个很小的模型。

Hinton 做了一个实验，在 MNIST 上，把 7 的图片拿走了，然后用比较复杂的网络训练，训练的时候，除了7 以外的图片都作为了输入，然后 0~9 这几个数字作为输出。但是这个网络从来没见过 7 。然后，用这样训练好的网络按照上面的方式训练简单的网络，训练完后，把7 的图片输入，它认识出了7。<span style="color:red;">这个怎么感觉不可能呢？是不是我听岔了？要确认下。我感觉，网络学习到了人的写字的特征这个没有问题，但是，网络只能知道这个不是我之前学习过的那些数字，至于是几，它不管，也不用知道，因为它直接把它归到other 上面去，这个other 就是 7 这个数字，也就是说，如果你输入的是 $\alpha$ 这种四不像，也很大可能被归到 7 上面去。</span>

<span style="color:red;">感觉Hinton 上面这个实验还是要确认下，到底是什么情况？可能是我听错了。</span>


<span style="color:red;">想到了昨天看的科技园人的视频，一道 蓝眼睛绿眼睛的题目，外来人带来了新信息，蓝颜A知道(蓝颜B知道(蓝眼C知道..)) 。信息，还是要好好学习一下的，想要系统学习一下，关于信息，不是信息论，而是比如什么是新信息，信息之间的比较等等。</span>


嗯，老师最后说，这个简单的网络可以是一个随机森林。这样就把一个跑起来比较慢的东西，变成了一个飞快的模型。<span style="color:red;">震惊了！确定这样不会损失什么吗？泛化性能不会降低吗？</span>

嗯，老师还说了：把 VGG 转成单隐层是可以的，但是，效果可能比 VGG 差的，但是比直接训练单隐层要好。那么什么时候会使用这种情况呢？看起来得不偿失？实际上，如果你的芯片只能跑一个单隐层，那么这种迁移就非常适用了。






## Dropout

Dropout 背后的理论依据是集成学习。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/4b1J6HdIj3.png?imageslim)

具体是这样的：

- 在 BP 的时候，随机的把神经元关掉，只更新剩下的网络。这样就造成我每次更新后实际上训练的是一个不同的网络。然后，我们之前的网络权重我们还会保留。这样，我们就相当于以这么一种方式在训练出了成千上万个神经网络。
- 大量的网络在训练完之后，我们对这些权重取平均 average，就得到了一个 OK 的网络，虽然最开始我们保留的那个网络的权重可能是初识随机选的，但是，大量的这样的网络进行了一个集成学习，效果就会比较好。keras 的 dropout 就是对权重做的平均。

<span style="color:red;">震惊了，之前学习过 dropout ，但是这次听了他寥寥几句的讲解，就非常的清晰。嗯。真厉害。而且我之前一直不知道Dropout 的背后的理论依据是集成学习。</span>

老师说：要想在工业界吃得开，任何东西到最后都要做一个集成，比如 SVM+LR+... 等。

老师说：Dropout 不仅仅可以在神经网络上用，如果掌握了这个精髓就是训练的时候随机的关掉一些单元进行更新，那么我们做 xgboost GBDT 的时候也能 dropout，随机的关掉一些节点。<span style="color:red;">厉害。</span>





## GAN

生成式对抗神经网络

事情的起因来自一个玩世不恭的论文：把特意弄出来的一个奇怪的图片给 VGG，让 VGG 以大于 99.6% 的信心返回一个label。<span style="color:red;">厉害</span>

Fool your Conv-net

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/7fCiiE7KFa.png?imageslim)


这个人是怎么找到这个图片的呢？

他是用遗传算法，先生成一个随机的图像，然后给了 VGG，VGG 给了一个label，然后我让这个 遗传算法根据这个label 进行演化，这样不断的重复，我们利用 遗传算法，把 VGG 当做一个 fitness 的函数，我就不断的生长出来一些非常随机的图片，什么时候 VGG 很有信心了，什么时候停下来。

idea 还是很简单的，但是一下就击中了 VGG 软肋。毕竟这么多钱投到神经网络的开发中去了？如果这么容易被糊弄，那么还用神经网络干嘛？

当时的解释是：这个事情不怪我们的神经网络，当我们的 X 在一个超高维的空间中，如果我们做一个小的扰动，会以很大的概率从一类调到另一类。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/C8mgmi2Jf5.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/cKADEjEE3j.png?imageslim)


这个是从数学上的解释，但是没有解决这个网络被愚弄的问题。

直到一年半之后，Hinton 的学生提出了这么一个模型 GAN：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180817/BgJlCJCcI0.png?imageslim)

他是这样的：我们想让网络分辨出这个是真的还是假的。我会有这么一个模型：他的输出是二分类，0/1 。训练集是：一部分是从真实的训练集中拿出来的照片，一部分是非真实的，他不用上面的遗传算法造假图片，他又用了一个神经网络来造假图片。那么他是怎么用这个神经网络造假照片的呢？他是这样的：他生成一个 28*28 的随机数矩阵，我对这个矩阵进行卷积什么的，我可以让我最后的输出是一个矩阵。这个矩阵就作为 Fake image 来输入这个区分模型中。

嗯，这时候就可以判断出真图片和假图片了，然后呢，我这个区分模型输出的这个真假的信息是可以用来更新 生成模型的权重的。我会慢慢生成一个比较像真的的一个照片。这样的迭代这来回的走。直到 区分模型根本没办法区分为止。他分不出来了。<span style="color:red;">这里想知道他是怎么更新权重的？而且，区分模型不会进化吗？</span>

这个时候，我就得到了一个非常好的一个生成模型，这个生成模型是一个神经网络，输入一个随机数，输出一个长的特别真的一个照片。<span style="color:red;">牛逼！从随机数到以假乱真的照片！感觉这个像一个工厂，原料进去，成品就出来了，而且，感觉应该是熵减少了？嗯，要好好理解下。</span>

<span style="color:red;">GAN 老师说有各种各样的实现，要都整理下。</span>


## 在 CV 里面，提出一个问题比解决一个问题更重要，

提出的问题：

- Image caption
- Attention 输入一个label，返回图像上的某个区域 <span style="color:red;">这个用在什么场景中的？现在什么进展了？</span>
- Generating images 用一个模型来生成很多的图像。<span style="color:red;">这个是什么意思？类似上面的生成模型吗？还是什么别的？</span>如果一个模型能够知道学会生成一些样本，这个在人工智能的角度是非常重要的，他的意义在于，这个接着往下做的话，有可能真正理解某个图像。而不是某种像素，这个事情不是一个终极目标，是达到一个终极目标必须达到的一个里程碑。<span style="color:red;">还是没明白，为什么这么重要。为什么说生成一些图像有可能真正理解某个图像？感觉上是这样，比如我告诉网络说：一只小马在过河，他如果能画出了小马过河的图，那么感觉他应该是有一些理解的。是不是？要确认下，而且，感觉可以用 COCO的数据反向训练一下试试。</span>老师说：这个生成图像，相当于学出了一个流行到另一个流行的一个映射。<span style="color:red;">到底什么是流行？为什么老师总是提到流行？</span>老师说：流行是：比如一个图像是 28*28=784 的，上面是 8 这个数字，但是这个 784 这个维度可能太高了，他应该是在一个更低维度的，比如30维度空间的流行上的。只不过被嵌入在高维的空间中，也就是说他不是真正的784 维的物体，只不过是一个低维空间在高维空间中的嵌入，embedding。


解决方案：

。。。总会有的







## 我们有什么 vs 我们想做什么

工具和目标是不一样的。工具比如分类和回归，目标比如是物体检测，图像生成等。

这两个之间是有一个gap的，而 CV 、NLP 等偏应用的领域都是从机器学习这个工具包中选一个工具，然后达到我们的目标，在 CV、NLP 等偏应用的领域，他们设计的其实不是算法，而是一种 pipeline，来让这个工具达到我们的目标。

所以，我们一定要知道手中有什么。

我们看一篇论文的时候，看到他的intruction ，一定要把论文放在一边，想一下如果你是作者，你怎么办？因为，别人在这个 intruction 里已经把问题提出来了。至于怎么解决，只要大家有一些专业训练，总是能想到一些解决方案的。最锻炼人的就是，看别人的问题，然后自己想一个解决方案与他提供的解决方案对比一下。这个也是 kaggle 刷题的好处。而且，论文这样看的话，比 kaggle 刷题还锻炼人，毕竟论文是很长时间才做出的。
