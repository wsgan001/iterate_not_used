主要看课程的资料


两个案例

- 高维数据怎么处理好
- 看起来不是一个高维数据问题的时候怎么转化成一个高维数据问题


- 解决⾼维数据分类/回归问题
    - 线性/⾮线性回归⽅程
    - 决策树
    - 弱分类器的进化: Ensemble
    - 神经⽹络
- ⾮标准数据的处理
    - ⽂本处理
    - 图⽚处理
    - 视频处理


⼀张典型的股指数据表

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/5mLFdDbHjJ.png?imageslim)


对于这个股票来说，我们可以：

1. 把今天的各个特征作为X
    - X: [Open, Prev Close, Big, Ask, Beta, …]  #M 个特征
    - y: today’s Close #要预测的是今天的闭市价格

2. 我们把10天的特征作为X
    - X: [Day n-10 figures, Day n-9 figures, Day n-8 figures, …] # 10*M个特征
    - y: Day n’s Close

3.把 y 也扩张一下
    - X: [各种数据...]  # 还是 10天或100天的数据
    - y: [各种触发状态…]  # 并不是单一的Close，而是很多种状态：比如今天是不是卖空


OK，那么交易是什么呢？

触发状态 + 决策规则= 交易


那么怎么解决呢？

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/dJeKC2eF5E.png?imageslim)

第一个：回归方程

$y_i=\alpha+\beta *x_i+\epsilon_i$
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/61IIikj748.png?imageslim)


可以拟合出一条线来表示这么多点在空间中是怎么愤怒的。


各种高级的回归方程：

根据 Order, Norm, 因⼦选取等等的不同产⽣各种变种：
- LR
- Ridge
- Lasso
- …

<span style="color:red;">什么是因子选取？</span>


第二种：决策树

白富美找对象：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/H10KG51AG9.png?imageslim)

分治思想：
- 把数据集分成两组：通过 Entropy 熵和 InformationGain 信息增益来决定从哪个var开始搞分裂
- 不同数据点被完美区分（Pure）开了么？
- 不是：重复楼上两步
- 是的：打完收⼯


决策树的优劣：

优势：
- 非黑盒。**为什么还在用？关键就是他的可解释性。**
- 轻松去除无关 attribute （Gain = 0）
- Test起来很快 （O(depth)）树有多深就判断多少次。因此很快
劣势：
- 只能线性分割数据。是的
- 贪婪算法（可能找不到最好的树）。是的，哪个指标放在前面是非常重要的，A在前面和B在前面都会有非常大的不同。

如果你是一个量化交易员，那么LSTM并不是那么容易要用的，因为要解释为什么这样决策。

由于决策树存在这些劣势，因此有对决策树进行一些优化。

Ensemble：把弱分类器融合成一个厉害的。

注意：Ensemble 不是一个算法，而是一套算法框架。

**Bagging** (Breiman, 1996)

Fit many large trees to bootstrap-resampled versions of the training
data, and classify by majority vote.

**Random Forest** (Freund & Shapire, 1996)

Fancier version of bagging, add one more randomness in variable choosing.

**Boosting** (Breiman, 1999)

Fit many large or small trees to re-weighted versions of the training data.
Classify by weighted majority vote.<span style="color:red;">掌握的还是不熟练，re-weighted 的时候权重怎么定？</span>


Bagging

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/A81EcdejHc.png?imageslim)

从上图可以看出：Bagging 实际上每个模型只会使用部分的数据。这样减少了噪音数据对每个模型的误导。

随机森林

1. 从原始训练数据集中，应⽤ bootstrap ⽅法有放回地随机抽取 k 个新的⾃助样本集，并由此构建k棵分类回归树，每次未被抽到的样本组成了 K 个袋外数据（out-of-bag,BBB）。
2. 设有 n 个特征，则在每⼀棵树的每个节点处随机抽取 m 个特征，通过计算每个特征蕴含的信息量，特征中选择⼀个最具有分类能⼒的特征进⾏节点分裂。
3. 每棵树最⼤限度地⽣长， 不做任何剪裁
4. 将⽣成的多棵树组成随机森林， ⽤随机森林对新的数据进⾏分类，分类结果按树分类器投票多少⽽定。

可见，不仅数据集是取的一部分，而且特征也是取的一部分，这样是为了防止误导性的特征，如果这个特征每次都被考虑进去，那么他们每个树会都陷入一个陷阱中。


Boosting

1. 先在原数据集中长出⼀个 tree
2. 把前⼀个 tree 没能完美分类的数据重新 weight
3. ⽤新的 re-weighted sample 再训练出⼀个 tree
4. 最终的分类结果由加权投票决定 $C(x)=sign[\sum a_mC_m(x)]$ <span style="color:red;">这个 $a_m$ 怎么确定？</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/jjBC1910kB.png?imageslim)

<span style="color:red;">这个地方老师讲的不对吧？确认下到底是什么？合并到 Boosting 那一章里面去。</span>


Ensemble 实际上里面的每个小分类器可以是不同种类的，比如里面把树和回归模型混合。

神经网络

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180720/BgEjDdjLCC.png?imageslim)
