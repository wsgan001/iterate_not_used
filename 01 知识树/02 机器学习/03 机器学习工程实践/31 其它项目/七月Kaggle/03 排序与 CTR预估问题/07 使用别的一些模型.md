刚才都是使用 LR 来完成这个案例，
实际上工业界很少只会用一个模型来做所有的事情。比赛中也是，前几名的方法都会进行模型的融合。

- Random Forest
- GBDT
- FM（factorization machine）<span style="color:red;">这个是什么？</span>

请参见比赛
https://www.kaggle.com/c/avazu-ctr-prediction
Rank 2nd Owen Zhang的解法：
https://github.com/owenzhang/kaggle-avazu

第二名的比赛使用的是几个常见的模型，第一名使用的方法是一个在CTR预估里面很好用的方法FFM，但是在别的地方使用的不是很多。

Owen Zhang 是一个特征工程和模型融合的高手。经常进前3

liblinear 和 libsvm 都是台大开源的库，都比较好用

下面我们看看第二名的方法：

我们先看看它的整个结构：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/be5a8EJc0J.png?imageslim)



它用到了这么几个方法：

- VW 是以前 yahoo 出的一个库，现在用的不是很多了，可以看做一个 LR 的模型，相当于一个更高级的 LR ，因为它会做一些hash 的处理，一些映射的处理，<span style="color:red;">为什么要做hash 处理？</span>
- RF 就是 随机森林
- GBDT
- FM factorization machine

可见，他用了4个模型来做最后的融合，

他的这个在单机上跑很难跑的动，要两天才能出来结果。

_4_post_processing.py 是最后一步，后处理，这里把4个模型拿到的结果做一个混合。他用的是 blending ：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/6Le6dEehhb.png?imageslim)


这个权重是怎么得到的呢？是学习出来的，把4个分类器的结果作为一个新的线性回归模型的特征传进去，然后进行训练。这样就得到了权重。

自己手敲的权重肯定是不如学习出来的好的。


这个blending 要求我们有这4个分类器，


_3a_rt.py 就是随机森林的脚本，他实际上就是使用的sklearn，没有什么别的高级的东西。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/4G4GgLKiB3.png?imageslim)

他实际上给的是最终的结果，实际上这个参数是通过交叉验证和 Gridsearch 得到的。他最后提交的代码都是最简洁的代码，实际上这个过程中调参是需要很长时间的。Owen Zhang 的专访：他自己说，他很多时候就是写了一个 grid search 、cross validation 放在那里，然后放在那里跑。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/c5i881B2lI.png?imageslim)

list_param 是他认为的有用的特征，可见，他并不是所有的特征都用了

feature_list 里面的特征就要看之前的代码了，可以从结构图上可以看出是之前的两个模型 FM、GBDT 产出的特征。


_3b_gbdt.py 使用的是 XGBoost，也是用 grid search 找到最好的参数，这里没写怎么用 grid search ，第一课的时候已经教了。

_3c_vw.py

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/m0Fa91KBF5.png?imageslim)

他用的是一个叫 vw 的一个工具，以前  yahoo 出的，然后使用cmd 来启动这个工具，这个可执行的工具有自己的输入文件的格式，因此他前面做了很多事情来把数据处理成他要的格式。

可以看到他的 loss_function 是  logistic，说明这是一个logistic 的损失函数，就大概知道了他是一个 LR 的模型，--hash all 是进行hash 处理，<span style="color:red;">不知道这个在 sklearn 中是对应的什么，为什么要hash 处理？</span>


_3d_fm.py 这个中的 libMF matrix 是台大开源的库这里面的 fm_cmd 意味着这又是命令行执行的，Owen Zhang 是一位很精通各类工具，精通特征处理，精通模型融合的一个人，使用的工具都是其他人的工具，并没有手写自己的工具。libfm 也是一个开源库，是最早开发的，不是台大的。 SVDFeature 是交大的库，速度比较快，也可以做快速的矩阵分解。

他里面使用了 XGBoost 来生成一部分特征来放到 fm 里面训练。

那么传到 fm 这种可执行文件的参数是怎么来的呢？<span style="color:red;">老师好像没有怎么说。只说了这个参数也是要找到的。</span>


_1_encode_cat_features.py 他里面不是简单粗暴的使用 one-hot ，他加了很多的判断，因为不一定是多有的特征都要用，有些特征也可以组合起来成一列，这里面也就是这些技巧，他里面不同的类别的编码不一样，做了一些统计的处理，做了一些阈值的界定。可以拿一些小的样本来放到 jupyter notebook 里面跑一下，来理解一下他是怎么处理的。

_2b_generate_dataset_for_vw_fm.py 这个就是在刚在的一些特征处理的基础上，为 fm 产生更多的特征，

这个人的代码还是很清洗的，非常好，向他学习。是一个非常典型的特征工程加上模型融合的一种方式。
