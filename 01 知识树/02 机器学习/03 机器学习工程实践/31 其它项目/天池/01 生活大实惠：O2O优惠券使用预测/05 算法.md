
## 单模型

最初使用RF、[GBDT](http://blog.csdn.net/shine19930820/article/details/65633436)两种模型，GBDT效果优于RF，


基于以上提取到的特征，进行模型设计与融合。

- 单模型

  第一赛季只训练了XGBoost单模型提交，连续几周位居排行榜第一位。

  第二赛季训练了XGBoost，GBDT，RandomForest三种单模型，其中GBDT表现最好，XGBoost次之，RandomForest相比之下最差。GBDT和XGBoost单模型在第二赛季仍然名列Top3,融合后效果更佳，尝试了以下两种方法：

### RandomForest


### GBDT

### XGBoost



## 模型融合

### 加权融合

得到了单模型的预测结果后，直接将概率预测值进行加权融合，我们简单地用`0.65 * GBDT + 0.35 * XGBoost`就得到了第一的成绩。

### Blending模型

我们尝试了两层的blending模型，首先将训练集分为两部分（D1和D2），一部分用于第一层（level 1）的训练，另一部分用于第二层（level 2）的训练。level1 在D1上训练了4个XGBoost，4个GBDT，4个RandomForest，将这些模型的预测结果作为level2的feature，在D2上训练第二层模型。Blending模型的结果相比单模型有细微的提升，但这点提升相对于模型复杂度带来的计算代价显得微不足道。


### rank 融合

后期使用了多个 GBDT 和 XGBoost，分别使用不同的参数、不同的正负样本比例以rank的方式进行多模型的融合，效果有微小提升，但是由于计算量的限制没有进一步展开。


由于评估指标是计算每个coupon_id 核销预测的AUC值，然后所有优惠券的 AUC 值平均作为最终的评估指标，而 rank 融合方式对 AUC 之类的评估指标特别有效，所以采用此方法，公式为：<span style="color:red;">为什么 rank 融合方式对 AUC 之类的评估指标特别有效？</span>

$$\sum\limits_{i=1}^{n}\frac{Weight_i}{Rank_i}$$

其中 $n$ 表示模型的个数， $Weight_i$ 表示该模型权重，所有权重相同表示平均融合。$Rank_i$ 表示样本在第 $i$ 个模型中的升序排名。它可以较快的利用排名融合多个模型之间的差异，而不需要加权融合概率。<span style="color:red;">什么叫较快的利用排名融合多个模型之间的差异？</span>

应用

基于参数，样本(采样率)，特征获得多个模型，得到每个模型的概率值输出，然后以 $coupon_id$ 分组，把概率转换为降序排名，这样就获得了每个模型的 $Rank_i$ ，然后这里我们使用的是平均融合，$Weighti=1/n$，这样就获得了最终的一个值作为输出。<span style="color:red;">平均融合？</span>








------

# **线下评估**

虽然这次比赛每天有四次评测机会，但是构建线下评估在早期成绩比较差的时候用处很大，早期添加特征之后线下评估基本和线上的趋势保持一致（例如在添加了Label区间的领券特征之后，线下提升十多个百分点，线上也是一致），对于新特征衡量还是有参照性的。后期差距在0.1%级别的时候，就没有参照性了。<span style="color:red;">嗯。</span>

线下评估在训练集中采样 1/3 or 1/4 or 1/5 做线下评估集合，剩下的做为训练集训练模型，并将评估集合中全0或者全1的优惠券ID去掉，然后使用训练的模型对评估集合预测，将预测结果和实际标签作异或取反（相同为1，不同为0），然后算出每个优惠券 ID 的 AUC ，最后将每个 ID 的优惠券 AUC 取均值就得到最终的 AUC 。
