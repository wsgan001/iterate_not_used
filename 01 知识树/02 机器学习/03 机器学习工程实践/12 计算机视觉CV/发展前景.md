


**1. Robotics (or Simulation Graphics)+Vision**. Robotics那边的人普遍比较保守, 更执着于传统template matching之类的传统方法. 这里有个段子, 我们MIT机械工程系robotics方向的大牛教授John Leonard很久以前评论Computer vision, 直接说你们'CVPR'里面的各种论文, 就是Computer Vision and Precision Recall. 什么意思大家应该能理解:). 不过在deep learning开始真正work的时代, 他这句话应该不太适用了(笑). 回到正题, Robitics本身是块非常大的饼, 很多问题和方法都可以用deep learning (CNN + Deep Reinforcement learning) 重新解决. 偏Robotics的话, 大家可以留意一下Berkeley的大红人Sergey Levine最近的工作([Sergey Levine](https://link.zhihu.com/?target=https%3A//people.eecs.berkeley.edu/%7Esvlevine/)). 偏Vision的话，可以看看CMU的大红人Abinav Gupta的ECCV paper Curious Robot ([https://arxiv.org/pdf/1604.01360v2.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1604.01360v2.pdf)). Jianxiong Xiao之前主打的3D deep learning ([http://robots.princeton.edu/talks/2016_MIT/RobotPerception.pdf](https://link.zhihu.com/?target=http%3A//robots.princeton.edu/talks/2016_MIT/RobotPerception.pdf))也可以算在这个里面，他们团队和MIT团队最近搞了个Amazon Pick challenge, 模型和方法还有点意思（[MIT-Princeton Vision Dataset for the APC 2016](https://link.zhihu.com/?target=http%3A//www.cs.princeton.edu/%7Eandyz/apc2016)）. 不过Xiao已经下海经商, 不知道还会不会actively publish. 现在各大公司和startup猛搞的autonomous drive, 也可以放在这个方向之下.

最近我还留意到一个非常有潜力的方向Simulation+Vision. 我觉得有两个具体方向，一个是利用graphics里面的rendering仿真技术，生成大量数据．这些数据因为是生成出来的，任何ground-truth都有，而且要多少有多少, 是获取训练数据的一个捷径．CVPR'16有篇做synthetic image dataset for semantic segmentation of urban scene（[http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf](https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf)）．另外一个方向是结合graphics中的simulation，利用deep reinforcement learning等active learning的算法可以无监督／弱监督训练出agent model，这里就不仅限于纯CV了．DeepMind和OpenAI在猛搞这个方向．偏vision的话大家可以参考下Allen Institute这篇（[https://arxiv.org/pdf/1609.05143v1.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.05143v1.pdf)）．

**2. Generative visual models.** 目前大部分的模型都是discrminative model, 给定input, 然后识别label. 但这个故事的另外一半其实是generative model, 给定label, 然后生成图片. generative models是一个很有潜力的大方向. 这里的最新进展一方面是基于GAN ([https://arxiv.org/pdf/1511.06434v2.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1511.06434v2.pdf)) 所带来的一种训练图片生成的新思路, 也包括一些基于传统image model, 如MRF和CRF在deep learning的新思路下面进行重新理解. DeepMind的这篇PixelCNN([https://arxiv.org/pdf/1606.05328v2.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.05328v2.pdf)), 最近Zhirong和Dahua的挺不错的ECCV论文([http://dahua.me/papers/dhlin_deepmrf.pdf](https://link.zhihu.com/?target=http%3A//dahua.me/papers/dhlin_deepmrf.pdf)). 个人觉得Varionational Autoencoder也是个蛮漂亮的模型, 这里有篇关于VAE的最新的tutorial还不错([https://arxiv.org/pdf/1606.05908v2.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.05908v2.pdf)). 以后deep learning跟bayesian model的结合也会是个颇具潜力的方向.

3. **Multimedia Computer Vision.** 其实人的感知系统本身就是多模态的, 视频和声音共同结合．Video analysis不再局限于action recognition, 对内容本身有更深的理解. 比如说最近的MoiveQA ([MovieQA](https://link.zhihu.com/?target=http%3A//movieqa.cs.toronto.edu/home/)), Visual Anticipation prediction ([http://web.mit.edu/vondrick/prediction.pdf](https://link.zhihu.com/?target=http%3A//web.mit.edu/vondrick/prediction.pdf)
). 另外, sound也是一个大家普遍忽略掉的一个东西. 大家可以看看我们组Andrew Owen的两个蛮有意思的工作ECCV'16 Ambient Sound Provides Supervision for Visual Learning ([https://arxiv.org/pdf/1608.07017.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.07017.pdf)), CVPR'16 Visually Indicated Sounds ([Visually Indicated Sounds](https://link.zhihu.com/?target=http%3A//vis.csail.mit.edu/)). 多模态来研究vision是个大趋势.











视觉只是识别吗？肯定不是，还有理解和分析。也许你看ImageNet错误率这么低了，机器是不是很厉害了，我来这个领域还干嘛，还来得及吗。其实，再in the wild 的数据集和真实环境一比还是个实验环境，太太太单纯了。比如现在你用LFW用cnn判断个性别轻而易举，在老旧的监控下人都糊成一团了，cnn还能识别么？但为啥人一看就可以识别，这里面是不是还有我们没利用起来的信息？我们怎么通过各种方法的交叠左右解决这个实际问题。所以，从算法到产品，从理论到实际，没有想象的那么近，还有很多的问题有待开发和解决。

<span style="color:red;">什么是 LFW？ 图像的理解现在有什么进展？有什么paper吗？</span>








博磊特别提到的Robotics (or Simulation)+Vision，我见到北美几个组热火朝天地搞起来了（比如我好朋友Yuke做的[https://arxiv.org/pdf/1609.05143v1.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.05143v1.pdf)）。



我感觉这是往强AI方向的升级。以前，vision一直都在做“let machine see like human”。现在，开始思考“let machine act like human”（大家想想，这显然更AI，能干更多的事情啦）。



以前是 “see“这个基础都没有，机器就是一个瞎子，让它“act like human”就很不靠谱了。现在因为深度学习，基本具备“see”这个条件后，自然有一批visioner去琢磨这件事了。这对vision也就提出新的挑战，同时学习perception和how to act。deep reinforcement learning 刚好就是干这个事的，所以关注的人也很多，也很work，我自己也在无人车仿真玩过，效果真不错（当然做到真正强AI, deep RL目前也有自己的缺陷）。



想想baby 的学习过程中 ， 学习perception和how to act是相互伴随，互相促进的。这样也会使perception的档次提高，目前的object detection and segmentation，对应到人类语言语义大概是名词级别的语义，是非常浅的，接下来还有动词，短语级别，句子级别 ，故事级别。如果机器完全理解了how to act，会很大程度地帮助我们获得更深层的perception。当然，深语义的perception也帮助了how to act。Deep reinforcement learning 比传统reinforcement learning好，就是因为perception 更好了。



为什么是simulation+vision呢？因为要在真实世界里学习how  to act成本太高了（找个机器人来不停试错不现实）。所以一条路就是create strong AI in virtual world, thus apply in real world。从研究角度来讲，simulation+vision to create stronger AI就是成为第一步，后面apply to real world还有一大堆问题可以探索的。要是做好了按在各种机器人上让它自主干活，那就是威力巨大了（从AI角度看，无人车算是简单的机器人吧），估计也是不小的产业。




以上只是一个例子而已（也是一个尝试），我想说的我们在解detection和segmentation这些传统问题同时，作为研究者，不妨也站在整个大AI的背景下看看，想想vision和DL如何面向强AI，迈一小步，再迈一下步，再迈一小步…….







个人觉得是刚刚开始，其实图像识别的大思路目前就是采用海量训练库（看了一些图像识别的文章的体会，应该算不上最前沿的方法），也就是模拟人学习的过程，但是这并没有完全模拟人的识别过程，人之所以能分辨东西除了大脑的特性以外，输入也都是立体图像且有各种感知，所以从这点来看单纯一个摄像头的图片就想实现人的识别效果输入明显是不够的，所以这点上把输入变成立体，多传感的应该可以起到更加好的效果，现在各种传感技术3D技术发展成熟以后应该会有大步跃进，至少我觉得是这样，模拟人就要尽量模拟全一些，毕竟有些信息可以压缩裁剪但是也有很多关键信息不能去裁剪压缩的，否则无法还原，也就无法起到输入和输出的一致性了

作者：张忠虎
链接：https://www.zhihu.com/question/51863955/answer/129100280
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。






## REF

- [计算机视觉是否已经进入瓶颈期？](https://www.zhihu.com/question/51863955)
