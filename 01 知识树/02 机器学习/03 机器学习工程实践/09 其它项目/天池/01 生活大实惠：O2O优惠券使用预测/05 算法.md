
# **算法及模型融合**

最初使用RF、[GBDT](http://blog.csdn.net/shine19930820/article/details/65633436)两种模型，GBDT效果优于RF，后期使用了多个 GBDT 和 XGBoost，分别使用不同的参数、不同的正负样本比例以rank的方式进行多模型的融合，效果有微小提升，但是由于计算量的限制没有进一步展开。

## **模型融合**

由于评估指标是计算每个coupon_id 核销预测的AUC值，然后所有优惠券的 AUC 值平均作为最终的评估指标，而 rank 融合方式对 AUC 之类的评估指标特别有效，所以采用此方法，公式为：<span style="color:red;">为什么 rank 融合方式对 AUC 之类的评估指标特别有效？</span>

$$\sum\limits_{i=1}^{n}\frac{Weight_i}{Rank_i}$$

其中 $n$ 表示模型的个数， $Weight_i$ 表示该模型权重，所有权重相同表示平均融合。$Rank_i$ 表示样本在第 $i$ 个模型中的升序排名。它可以较快的利用排名融合多个模型之间的差异，而不需要加权融合概率。<span style="color:red;">什么叫较快的利用排名融合多个模型之间的差异？</span>

## **应用**

基于参数，样本(采样率)，特征获得多个模型，得到每个模型的概率值输出，然后以 $coupon_id$ 分组，把概率转换为降序排名，这样就获得了每个模型的 $Rank_i$ ，然后这里我们使用的是平均融合，$Weighti=1/n$，这样就获得了最终的一个值作为输出。<span style="color:red;">平均融合？</span>

------

# **线下评估**

虽然这次比赛每天有四次评测机会，但是构建线下评估在早期成绩比较差的时候用处很大，早期添加特征之后线下评估基本和线上的趋势保持一致（例如在添加了Label区间的领券特征之后，线下提升十多个百分点，线上也是一致），对于新特征衡量还是有参照性的。后期差距在0.1%级别的时候，就没有参照性了。<span style="color:red;">嗯。</span>

线下评估在训练集中采样1/3 or 1/4 or 1/5做线下评估集合，剩下的做为训练集训练模型，并将评估集合中全0或者全1的优惠券ID去掉，然后使用训练的模型对评估集合预测，将预测结果和实际标签作异或取反（相同为1，不同为0），然后算出每个优惠券 ID 的 AUC ，最后将每个 ID 的优惠券 AUC 取均值就得到最终的 AUC 。
