TODO

- 讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。
- DL 来做 NLP 的还没讲，老实说第六课会讲。
- n-gram 没讲
- KDD2013 比赛：判定文章作者 好像没讲
- 影评数据做情感分析 好像没讲


# 自然语言处理类问题

## 主要内容

- 讲解NLP的基本思路与技法
- Kaggle题⽬详解


两个问题：一个是上一次的一个问题，



## NLTK

今天课上的code会基于NLTK
做⼀点更详细的讲解
NLTK是Python上著名的⾃然语⾔处理库
⾃带语料库，词性分类库
⾃带分类，分词，等等功能
强⼤的社区⽀持
还有N多的简单版wrapper


唯一的缺憾是并不原生支持中文，但是中文与英文在分完词之后是相同的意思。

NLTK 至少在国外是必会的第三方库，在国内回它也是应该的事情，还要回一些在对应处理中文分词问题的一些库

安装 NLTK 的时候，遇到的一些bug 和error 很大部分是因为没有下载语料库。
在你安装好 NLTK 之后，进入自己的python console 然后输入：

```python
import nltk
nltk.download()
```
然后运行，他就会显示这个界面：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/AFj8j7Jg74.png?imageslim)


NLTK 功能一览：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/Hgg3K7m6aB.png?imageslim)

NLTK 自带的语料库是什么？

有很多的预料信息，算法

```python
>>> from nltk.corpus import brown
>>> brown.categories()
['adventure', 'belles_lettres', 'editorial',
'fiction', 'government', 'hobbies', 'humor',
'learned', 'lore', 'mystery', 'news', 'religion',
'reviews', 'romance', 'science_fiction']
>>> len(brown.sents())
57340
>>> len(brown.words())
1161192
```

语料在自然语言中还是比较重要的。<span style="color:red;">不知道这样的语料库与数据集有什么区别吗？</span>

brown 是布朗大学的语料库，是一个分类语料库，他分成了冒险类、兴趣类等等。
也可以看到这个语料库里面有多少个句子，有多少个单词。


# 完整的文本预处理过程

## 那么我们自然语言处理大概的流程是什么？

比如说，我们拿到一篇文章，我们应该怎么处理它？

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/45536kf1Am.png?imageslim)

比如上面这句歌词，Hello from the other side 这个是阿黛尔的一句歌。

我们来判断这句话是不是来自阿黛尔的歌：

那，我们的输入就是这个字符串，输出就是 0 或 1。

- 我们用 Tokenize 这个方法将这个字符串分成了5个占有不同位置的小字符串。
- 然后通过一些预处理的方法 Preprocess 把不太需要的一些东西处理掉。比如 a girl 中的 a ，上句中的 the。
- 之后，我们把上面的机器位置变成数字化的表达形式，<span style="color:red;">什么是机器位置？怎么变成数字化的表达的？</span>
- 我们得到这个数字化的向量之后就可以使用各种模型来进行label的判定。


OK，介绍一下上面的几个步骤：

## Tokenize 把长句⼦拆成有“意义”的⼩部件

这里中英文是不同的。

对于英文来说：可以直接用 work_tokenize 把单词分割开来。

```python
>>> import nltk
>>> sentence = “hello, world"
>>> tokens = nltk.word_tokenize(sentence)
>>> tokens
['hello', ',', 'world']
```

但是对于中文就比较麻烦了：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/Jf4DIJ0B8m.png?imageslim)

因为中文的词中间没有空格，每个中文字相当于英文中的一个字母。
那么中文的分词怎么做呢？有两种方法：

- 启发式 Heuristic 有一个字典，字典里面包含了不同的词汇，直接以最长 match 扫过去。就进行了分割
- 机器学习/统计方法：HMM、CRF

在中文分词界比较简单好用的是：

- jieba 他速度比较快，而且没有那么复杂。
- 哈工大的NLP ，讯飞的NLP
- 在国际界的中文分词比较好的是 ：斯坦福的 core NLP，它是基于 CRF 的一种方式


```python
import jieba
seg_list = jieba.cut("我来到北京清华⼤学", cut_all=True)
print "Full Mode:", "/ ".join(seg_list) # 全模式
seg_list = jieba.cut("我来到北京清华⼤学", cut_all=False)
print "Default Mode:", "/ ".join(seg_list) # 精确模式
seg_list = jieba.cut("他来到了⽹易杭研⼤厦") # 默认是精确模式
print ", ".join(seg_list)
seg_list = jieba.cut_for_search("⼩明硕⼠毕业于中国科学院计算所，后在⽇本京都⼤学深造")# 搜索引擎模式
print ", ".join(seg_list)

【全模式】 : 我/ 来到/ 北京/ 清华/ 清华⼤学/ 华⼤/ ⼤学
【精确模式】 : 我/ 来到/ 北京/ 清华⼤学
【新词识别】：他, 来到, 了, ⽹易, 杭研, ⼤厦
(此处， “杭研”并没有在词典中，但是也被 Viterbi 算法识别出来了)
【搜索引擎模式】： ⼩明, 硕⼠, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算,
计算所, 后, 在, ⽇本, 京都, ⼤学, ⽇本京都⼤学, 深造
```

全模式的好处：用在搜索引擎模式的时候比较好，这样就不会漏掉一些可能被用户需要的信息
精确模式：只会把它包含的分割开来
新词识别：新词会被 Viterbi 算法识别出来
搜索引擎模式：比全模式提供更多信息，分出更多的分词。

一般我们使用的是精确模式。

分词后的效果：

['what', 'a', 'nice', 'weather', 'today']
['今天', '天⽓', '真', '不错']

经过分词之后，每个输入的句子都变成了一个数组，数组



有时候tokenize没那么简单

⽐如社交⽹络上，这些乱七⼋糟的不合语法不合正常逻辑的语⾔很多：
拯救 @某⼈, 表情符号, URL, #话题符号

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/ADdcHl6mmK.png?imageslim)

<span style="color:red;">这种没有怎么细讲</span>

要分割这些就要用到一些特殊的技艺，比如正则表达式。

```python
from nltk.tokenize import word_tokenize
tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(word_tokenize(tweet))
# ['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':',
# ’D', 'http', ':', '//ah.love', '#', '168cm']
```

上面这句在正常的语料库中都不会有
这种情况下使用正则表达式：

```python
import re
emoticons_str = r"""
    (?:
        [:=;] # 眼睛
        [oO\-]? # ⿐⼦
        [D\)\]\(\]/\\OpP] # 嘴
    )"""
regex_str = [
    emoticons_str,
    r'<[^>]+>', # HTML tags
    r'(?:@[\w_]+)', # @某⼈
    r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # 话题标签
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # 数字
    r"(?:[a-z][a-z'\-_]+[a-z])", # 含有 - 和 ‘ 的单词
    r'(?:[\w_]+)', # 其他
    r'(?:\S)' # 其他
]
```

使用这种正则表达式把预料中出现的一些神奇的字符都归纳出来。



这个是正则表达式的使用方法：对照表 http://www.regexlab.com/zh/regref.htm


```python
tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
def tokenize(s):
    return tokens_re.findall(s)

def preprocess(s, lowercase=False):
    tokens = tokenize(s)
    if lowercase:
        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
return tokens

tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(preprocess(tweet))
# ['RT', '@angelababy', ':', 'love', 'you', 'baby',
# ’!', ':D', 'http://ah.love', '#168cm']
```

上面这个就讲了怎么把 reg 结合到 tokenize 方法中。然后直接 preprocess 这个 tweet 就行。可见，想要的分词结果已经得到了。



## 纷繁复杂的词形

在英文中，这个是比较复杂的：

Inflection 变化: walk => walking => walked 不影响词性

derivation 引申: nation (noun) => national (adjective) => nationalize (verb) 影响词性


在中文中也有：

比如茴香豆的茴字，有三种的写法，实际上表达的是相同的东西，那么这些对于我的计算的就是增加了负担。

因此，遇到的时候，想统一成一种茴香豆的茴字。这样只在算法中作为一个特征出现。


对于英文中的复杂词形的处理方法：

- Stemming 词⼲提取：⼀般来说，就是把不影响词性的 inflection 的⼩尾巴砍掉
    - walking 砍ing = walk
    - walked 砍ed = walk

- Lemmatization 词形归⼀：把各种类型的词的变形，都归为⼀个形式
    - went 归⼀ = go
    - are 归⼀ = be

词干提取就是直接砍掉，只提取公有的东西。
词形归一：这就是类似于对照字典的形式，他知道 went ，就归一成 go 的形式。

这两种都不过。


## NLTK 实现 Stemming

nltk 里面的 stemmer 就是词干提取器。

```python
>>> from nltk.stem.porter import PorterStemmer
>>> porter_stemmer = PorterStemmer()
>>> porter_stemmer.stem(‘maximum’)
u’maximum’
>>> porter_stemmer.stem(‘presumably’)
u’presum’
>>> porter_stemmer.stem(‘multiply’)
u’multipli’
>>> porter_stemmer.stem(‘provision’)
u’provis’
```


```python
>>> from nltk.stem import SnowballStemmer
>>> snowball_stemmer = SnowballStemmer(“english”)
>>> snowball_stemmer.stem(‘maximum’)
u’maximum’
>>> snowball_stemmer.stem(‘presumably’)
u’presum’
```

```python
>>> from nltk.stem.lancaster import LancasterStemmer
>>> lancaster_stemmer = LancasterStemmer()
>>> lancaster_stemmer.stem(‘maximum’)
‘maxim’
>>> lancaster_stemmer.stem(‘presumably’)
‘presum’
>>> lancaster_stemmer.stem(‘presumably’)
‘presum’
```


```python
>>> from nltk.stem.porter import PorterStemmer
>>> p = PorterStemmer()
>>> p.stem('went')
'went'
>>> p.stem('wenting')
'went'
```

可见，在不同的 stemmer 中，认为的词根也不相同。各有各自的认为的处理方式。没有绝对的好坏之分

## NLTK实现Lemma

```python
>>> from nltk.stem import WordNetLemmatizer
>>> wordnet_lemmatizer = WordNetLemmatizer()
>>> wordnet_lemmatizer.lemmatize(‘dogs’)
u’dog’
>>> wordnet_lemmatizer.lemmatize(‘churches’)
u’church’
>>> wordnet_lemmatizer.lemmatize(‘aardwolves’)
u’aardwolf’
>>> wordnet_lemmatizer.lemmatize(‘abaci’)
u’abacus’
>>> wordnet_lemmatizer.lemmatize(‘hardrock’)
‘hardrock’
```

WordNet 是语言学家整理出来的全世界英文的一个前后对照关系表


Lemma的⼩问题

Went
v. go的过去式
n. 英⽂名：温特

可见，不能每次遇到 Went 就归一成 go，因为 温特与 go 还是差别很大的。
那么这种情况怎么办呢？为了更好的使用 Lemma，可以给这个词标注词性，这样归一化的结果就会跟着词性变化。

```python
# ⽊有POS Tag，默认是NN 名词
>>> wordnet_lemmatizer.lemmatize(‘are’)
‘are’
>>> wordnet_lemmatizer.lemmatize(‘is’)
‘is’
# 加上POS Tag
>>> wordnet_lemmatizer.lemmatize(‘is’, pos=’v’)
u’be’
>>> wordnet_lemmatizer.lemmatize(‘are’, pos=’v’)
u’be’
```

pos 就是 part of speech 简单的讲就是词性：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/IbEkdBbm3i.png?imageslim)



那么怎么知道句子中的一个词是什么词性呢？NLTK 中有自耦定标注的方法：

NLTK标注POS Tag

```python
>>> import nltk
>>> text = nltk.word_tokenize('what does the fox say')
>>> text
['what', 'does', 'the', 'fox', 'say']
>>> nltk.pos_tag(text)
[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]
```

奇怪了，这个pos_tag 难道不是使用的原始的字符串吗？使用数组的话会标注错吧？还是说其实是不考虑语境的？

## Stopwords

之前的 went 和 go 实际上指的都是 go ，但是这个 he 却可能指的不同的人。

这类词成为停止词。

如果我们的问题场景是基于意思这个场景，那么我们统一删除掉这些停止词，不让他出现在我们的场景中。如果是处理别的任务，比如文法判断，判断这个人写的论文语法用的好不好，与另一篇论文的相似度高不高，这里面就可能并不需要减去这个停止词。因为停止词的使用可能也包含了一些意义在里面。注重的是文本被创造的过程。

⼀千个 HE 有⼀千种指代
⼀千个 THE 有⼀千种指事

对于注重理解⽂本『意思』的应⽤场景来说
歧义太多
全体stopwords列表 http://www.ranks.nl/stopwords 英文停止词


NLTK去除stopwords


注意，在使用之前要在 console ⾥⾯下载⼀下词库
或者 nltk.download(‘stopwords’)

```python
from nltk.corpus import stopwords
# 先token⼀把，得到⼀个word_list
# ...
# 然后filter⼀把
filtered_words =
[word for word in word_list if word not in stopwords.words('english')]
```

上面就把停止词从 word_list 里面忽略掉了。


## 总结一下

⼀条典型的⽂本预处理流⽔线

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/2f55E550lE.png?imageslim)

Pos Tag 不一定用。标注之后更好。最后得到我们的 word_list

这就是一整套完整的文本预处理流水线。




# 自然语言处理的几类问题的对应

<span style="color:red;">这三类问题没有仔细讲，可以看看之前的七月的一些课程视频。</span>

## 那么什么是自然语言处理呢？

⾃然语⾔ -> 计算机数据

⽂本预处理让我们得到了什么？

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/hKil2k41Fm.png?imageslim)

可见，在上面的文本预处理的过程之后就是 Feature 化。


## NLTK在NLP上的经典应⽤

- 情感分析
- ⽂本相似度
- ⽂本分类

这三类问题，课程都有对应的初级的代码应用。这里就暂时不讲了。到时候自己对应看下：


### 情感分析

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/1dJBCFeIdm.png?imageslim)

哪些是在夸？哪些是在黑？

最简单的 sentiment dictionary
like 1
good 2
bad -2
terrible -3
类似于关键词打分机制
⽐如： AFINN-111
http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010

NLTK 完成简单的情感分析

```python
sentiment_dictionary = {}
for line in open('data/AFINN-111.txt')
    word, score = line.split('\t')
    sentiment_dictionary[word] = int(score)
# 把这个打分表记录在⼀个Dict上以后
# 跑⼀遍整个句⼦，把对应的值相加
total_score = sum(sentiment_dictionary.get(word, 0) for word in words)
# 有值就是Dict中的值，没有就是0

# 于是你就得到了⼀个 sentiment score
```

显然这个⽅法太Naive
新词怎么办？
特殊词汇怎么办？
更深层次的玩意⼉怎么办？

配上ML的情感分析

```python
from nltk.classify import NaiveBayesClassifier
# 随⼿造点训练集
s1 = 'this is a good book'
s2 = 'this is a awesome book'
s3 = 'this is a bad book'
s4 = 'this is a terrible book'
def preprocess(s):
    # Func: 句⼦处理
    # 这⾥简单的⽤了split(), 把句⼦中每个单词分开
    # 显然 还有更多的processing method可以⽤
    return {word: True for word in s.lower().split()}
    # return⻓这样:
    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}
    # 其中, 前⼀个叫fname, 对应每个出现的⽂本单词;
    # 后⼀个叫fval, 指的是每个⽂本单词对应的值。
    # 这⾥我们⽤最简单的True,来表示,这个词『出现在当前的句⼦中』的意义。
    # 当然啦, 我们以后可以升级这个⽅程, 让它带有更加⽜逼的fval, ⽐如 word2vec

# 把训练集给做成标准形式
training_data = [[preprocess(s1), 'pos'],
                  [preprocess(s2), 'pos'],
                  [preprocess(s3), 'neg'],
                  [preprocess(s4), 'neg']]

# 喂给model吃
model = NaiveBayesClassifier.train(training_data)
# 打出结果
print(model.classify(preprocess('this is a good book')))

```

### 应⽤：⽂本相似度

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/Gj3HBek527.png?imageslim)

⽤元素频率表⽰⽂本特征

| we  | you | he  | work | happy | are |
| --- | --- | --- | ---- | ----- | --- |
| 1   | 0   | 3   | 0    | 1     | 1   |
| 1   | 0   | 2   | 0    | 1     | 1   |
| 0   | 1   | 0   | 1    | 0     | 0   |

余弦定理

$$similarity=cos(\theta)=\frac{A\cdot B}{||A||||B||}$$


Frequency 频率统计

```python
import nltk
from nltk import FreqDist
# 做个词库先
corpus = 'this is my sentence ' \
          'this is my life ' \
          'this is the day'
# 随便tokenize⼀下
# 显然, 正如上⽂提到,
# 这⾥可以根据需要做任何的preprocessing:
# stopwords, lemma, stemming, etc.
tokens = nltk.word_tokenize(corpus)
print(tokens)
# 得到token好的word list
# ['this', 'is', 'my', 'sentence',
# 'this', 'is', 'my', 'life', 'this',
# 'is', 'the', 'day']

# 借⽤NLTK的FreqDist统计⼀下⽂字出现的频率
fdist = FreqDist(tokens)

# 它就类似于⼀个Dict
# 带上某个单词, 可以看到它在整个⽂章中出现的次数
print(fdist['is'])
# 3


# 好, 此刻, 我们可以把最常⽤的50个单词拿出来
standard_freq_vector = fdist.most_common(50)
size = len(standard_freq_vector)
print(standard_freq_vector)
# [('is', 3), ('this', 3), ('my', 2),
# ('the', 1), ('day', 1), ('sentence', 1),
# ('life', 1)


# Func: 按照出现频率⼤⼩, 记录下每⼀个单词的位置
def position_lookup(v):
    res = {}
    counter = 0
    for word in v:
        res[word[0]] = counter
        counter += 1
    return res
# 把标准的单词位置记录下来
standard_position_dict = position_lookup(standard_freq_vector)
print(standard_position_dict)
# 得到⼀个位置对照表
# {'is': 0, 'the': 3, 'day': 4, 'this': 1,
# 'sentence': 5, 'my': 2, 'life': 6}




# 这时, 如果我们有个新句⼦:
sentence = 'this is cool'
# 先新建⼀个跟我们的标准vector同样⼤⼩的向量
freq_vector = [0] * size
# 简单的Preprocessing
tokens = nltk.word_tokenize(sentence)
# 对于这个新句⼦⾥的每⼀个单词
for word in tokens:
    try:
        # 如果在我们的词库⾥出现过
        # 那么就在"标准位置"上+1
        freq_vector[standard_position_dict[word]] += 1
    except KeyError:
        # 如果是个新词
        # 就pass掉
        continue

print(freq_vector)
# [1, 1, 0, 0, 0, 0, 0]
# 第⼀个位置代表 is, 出现了⼀次
# 第⼆个位置代表 this, 出现了⼀次
# 后⾯都⽊有

```

### 文本分类

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/HEJdA8jKDL.png?imageslim)

TF-IDF


- TF: Term Frequency, 衡量⼀个term在⽂档中出现得有多频繁。
    TF(t) = (t出现在⽂档中的次数) / (⽂档中的term总数).
- IDF: Inverse Document Frequency, 衡量⼀个term有多重要。
    有些词出现的很多，但是明显不是很有卵⽤。⽐如’is'， ’the‘， ’and‘之类
    的。
    为了平衡，我们把罕见的词的重要性（weight）搞⾼，
    把常见词的重要性搞低。
    IDF(t) = log_e(⽂档总数 / 含有t的⽂档总数).
- TF-IDF = TF * IDF

举个栗⼦:
⼀个⽂档有100个单词，其中单词baby出现了3次。
那么， TF(baby) = (3/100) = 0.03.
好，现在我们如果有10M的⽂档， baby出现在其中的1000个⽂档中。
那么， IDF(baby) = log(10,000,000 / 1,000) = 4
所以， TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12



NLTK实现TF-IDF

```python
from nltk.text import TextCollection
# ⾸先, 把所有的⽂档放到TextCollection类中。
# 这个类会⾃动帮你断句, 做统计, 做计算
corpus = TextCollection(['this is sentence one',
                          'this is sentence two',
                          'this is sentence three'])
# 直接就能算出tfidf
# (term: ⼀句话中的某个term, text: 这句话)
print(corpus.tf_idf('this', 'this is sentence four'))
# 0.444342

# 同理, 怎么得到⼀个标准⼤⼩的vector来表示所有的句⼦?

# 对于每个新句⼦
new_sentence = 'this is sentence five'
# 遍历⼀遍所有的vocabulary中的词:
for word in standard_vocab:
    print(corpus.tf_idf(word, new_sentence))
    # 我们会得到⼀个巨⻓(=所有vocab⻓度)的向量
```

接下来就是 ML 的过程：

可能的ML模型:
- SVM
- LR
- RF
- MLP
- LSTM
- RNN
- …


OK，我们看看Kaggle 上有哪些问题，可以怎么处理。
