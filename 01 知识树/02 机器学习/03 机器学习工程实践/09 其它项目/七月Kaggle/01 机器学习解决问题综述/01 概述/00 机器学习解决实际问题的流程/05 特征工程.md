## 然后，进行特征工程


特征工程大概内容如下所示：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/k3ALldHFHI.png?imageslim)

<span style="color:red;">离散化的时候的段怎么界定？</span>


### 对于单个特征的处理

- 归一化
- 离散化
- Dummy Coding
- 缺失值
- 数据变换(log，指数，Box-Cox)

其中，对于缺失值：

如果缺省值很少，那么可以填充；如果缺的很多，那么看一下这个特征要不要使用；如果缺的数量适中，那么可以把这个缺省值当做一个值：缺失来对待，比如说有三个类别123 那么如果30% 的值缺失了，那么可以把缺失值当做 4。<span style="color:red;">没想到还可以这样做。</span>

对于数据变换：

数据变换，如果在当前数据域可能没有什么变动，但是变动到log域可能就很有规律，<span style="color:red;">但是这样的变换是不是都要尝试一下？然后作为特征？</span>

如果特征的分布非常长尾而且值的差距很大，这种情况说明你这个特征可能对最后的影响不是那么的大。<span style="color:red;">什么意思？</span>


### 对于每种类型的特征的处理方式


- 数值型
- 类别型
- 时间类。这个比较特殊，
- 文本型
- 统计型
- 组合特征

对于时间类的处理：

1. 不仅看是几月几号，还可能要看前后有什么节日？比如说双11之前只加购物车不买。
2. 也可以看这一周内发生了多少次。这周内搜索的次数。
3. 离散化，比如饿了么可以把一天的时间切分成几块。也可以


对于文本类型的：

n-gram ，bag of words，TF-IDF


对于统计型：

这个他举的例子是：开机3分钟，击败了全国30% 的人，那么这个30% 就是统计型，比开机3分钟这个数据可能能更好的与业务想结合。<span style="color:red;">有没有更确切的例子？理解的不是很到位</span>



## 需要仔细研究的

<span style="color:red;">这个sklearn 的两个网页还是要仔细看的。</span>

- 课程提供特征工程PDFl
- http://scikit-learn.org/stable/modules/preprocessing.html
- http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction

preprocessing 和 feature_extraction 模块 里面有很多特征抽取的方法。

sklearn 适合多大规模数据使用？数据的量、数据的维数都要考虑的，基本上比赛中的数据都是可以使用这个sklearn 的。在工业界的话，在抽象的数据上可以试试sklearn，但是全量的数据一般还是要转到spark 的mlib上。

有时候你的内存真的存不下，那么sklearn 是不是就一定不可以用呢？不一定，也可以一部分一部分的特征进行scaling。

scaling 是把每个维度的特征约束到一个范围内。

离散化，最简单的离散化就是Binarization 。比如小于阈值是0，大于阈值是1。


generating polynomial features 这个构造高维特征还是要慎用，因为维度很容易炸掉，然后会带来overfitting。
<span style="color:red;">那么什么时候使用呢？</span>




## 特征选择

<span style="color:red;">这个还是要单独拿出来说的。</span>

http://scikit-learn.org/stable/modules/feature_selection.html

过滤型
- sklearn.feature_selection.SelectKBest
包裹型
- sklearn.feature_selection.RFE
嵌入型
- feature_selection.SelectFromModel
- Linear model， L1正则化

过滤型用的非常少，什么情况下使用呢？比如说你的模型是一个LR线性模型，可以直接评估这一维 的特征与结果特征之间的相关度。对每个维度求方差是会做的，用来看这个维度的波动，波动大的时候只管的理解是对分类问题可能会有作用。

RFE 会对特征的重要度做排序，然后不断的去掉末尾的特征。

SelectFromModel

后面两种工业都在频繁的用，过滤型用的比较少。
