## 然后，进行数据预处理(清洗，调权)

数据预处理主要做的是下面几件事情：

- 数据清洗
    - 不可信的样本丢掉。
    - 缺省值极多的字段考虑不用

- 数据采样
    - 下/上采样 upsampling downsampling
    - 保证样本均衡。为什么要保证均衡呢？因为比如说1:100 的正负样本，那么我全估计为负样本的时候，这个时候的正确率已经很高了。那么怎么办呢？<span style="color:red;">样本不均衡时候的对应方法还是要总结下的。</span>




实际上，是没有自动化的东西来帮助你清洗干净的，这个都要各自结合各自的场景来进行清洗。基本上就是人来 review 数据。



### 所要用到的工具

同样的，根据数据量进行区分：

- 如果可以载到内存里，就用pandas 来清洗

- 如果数据量还是很大，那么要用 mapreduce 或者 hive sql/spark sql来处理。<span style="color:red;">这个也是要掌握的</span>
