---
author: evo
comments: true
date: 2018-03-31 00:24:32+00:00
layout: post
link: http://106.15.37.116/2018/03/31/ai-calculus-differential-and-taylor-series/
slug: ai-calculus-differential-and-taylor-series
title: 
wordpress_id: 2108
categories:
- 随想与反思
tags:
- '@want_to_know'
---

<!-- more -->

[mathjax]


# REF：






    1. 七月在线 深度学习

  2. 七月在线 机器学习

********************************************************************************


# 缘由：


对ai相关的微分和泰勒级数的知识进行总结。


# 微分学的核心思想：逼近




## 函数的导数定义：


如果一个函数f(x)在 \(x_0\) 附近有定义，而且存在极限：

\[L=\lim_{x\rightarrow x_0}\frac{f(x)-f(x_0)}{x-x_0}\]

那么f(x)在 \(x_0\) 处可导，并且导数 \(f'(x_0)=L\)


## 使用无穷小量进行表述：线性逼近：


如果存在一个实数L是的f(x)满足：

\[f(x)=f(x_0)+L(x-x_0)+o(x-x_0),x\rightarrow x_0\]

那么 f(x) 在 \(x_0\) 处可导并且导数 \(f'(x_0)=L\)

**对极限和导数之间的关系仍然心存忐忑。想知道清楚**






# 导数




导数就是曲线的斜率，是曲线变化快慢的反应。而二阶导数是斜率变化快慢的反应，表征曲线的凸凹性。




聊到三阶导数的情况微乎其微，一半都是用到一阶导数和二阶导数。嗯。





## 一个例子：




\[f(x)=x^x\, x>0\)的最小值是什么？这个函数看起来很怪异，但是与信息熵有关系。**有什么关系？**


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ac5d521dfb04.png)


对于幂指函数这种取对数进行处理的方法，在极大似然估计中经常会用到。




\(N^{\frac{1}{logN})\)，这个在算法中的Skip List中，要想建立一个完美的跳表，会用到这个。**怎么用到的？要补充一下。**





## 




## 




# 求导法则：






  * 链式法则：\(\frac{d}{dx}(g\circ f)=\frac{dg}{dx}(f)\bullet \frac{df}{dx}\)

  * 加法法则：\(\frac{d}{dx}(g+f)=\frac{dg}{dx}+\frac{df}{dx}\)




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abed328b3a7b.png)


所有求导法则原则上都可以由链式法则结合二元函数的偏导数来推出来。**怎么推导出来的？**


## 




## 




# 函数的高阶导数


如果函数的导数函数仍然可导，那么导数函数的导数是二阶导数，二阶导数函数的导数是三阶导数. 一般地记为

\[f^{(n)}(x)=\frac{d}{dx}f^{(n-1)}(x)\]

或者进一步

\[f^{(n)}(x)=\frac{d^n}{dx^n}f(x)\]

导数是对函数进行线性逼近，高阶导数是对导数函数的进一步逼近，因为没有更好的办法，所以数学家选择继续使用线性逼近.




## 




# 凸函数




## 定义：


若函数f的定义域domf为凸集，且满足

\[\forall x,y\in dom\, f,0\leq \theta\leq 1,有 f(\theta x+(1-\theta)y)\leq \theta f(x)+(\1-\theta)f(y)\]


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ac5d9df1f925.png)


**注意，这个图形以前可能认为是凹函数，但是现在机器学习领域中统一称为凸函数。这个没有歧义。**


定理：\(f(x)\) 在区间[a,b]上连续，在(a,b)内二阶可导，那么：






 	
  * 若\( f''(x)>0\)，则\(f(x)\) 是凸的；

  * 若\( f''(x)<0\)，则\(f(x)\) 是凹的




即：一元二阶可微的函数在区间上是凸的，当且仅当它的二阶导数是非负的




## 凸函数的表述：


将两个变量的情况变成n个变量的情况


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5ab9e06949f94.png)




意义：可以在确定函数的凸凹性之后，对函数进行不等式替换。





## 凸性质的应用：


设p(x)、q(x)是在X中取值的两个概率分布，给定如下定义式：

\[D(p||q)=\sum_{x}^{ }p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}\]

试证明\(D(p||q)\geq 0\)。注：D是distance，标识两个概率的距离，**后面半段的写法为什么这么写？**

注意到\(y=-logx\)在定义域上是凸函数


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5ab9e10c3acb1.png)




**上式在最大熵模型等内容中会详细讨论。讲到最大熵模型的时候回来补充下。**




**而且上面的式子还没有很明白到底有什么意义。**





## 




# 一元微分学的顶峰：泰勒级数




## 泰勒级数


用多项式逼近的方式描述高阶导数，我们就得到了泰勒级数，因此泰勒/迈克劳林级数实质上就是 多项式逼近。

当然前提是 f(x) 是一个无限次可导的函数。

Taylor公式：在任何一点\(x_0\)附近对 f(x) 做多项式逼近：

\[f(x_0+Δ_x)=f(x_0)+f'(x_0)Δ_x+\frac{f''(x_0)}{2!}Δ_x^2+\cdots +\frac{f^{(n)}(x_0)}{n!}Δ_x^n+o(Δ_x^n)\]

Maclaurin公式：在0附近对 f(x) 做多项式逼近：即将上面的\(x_0\)变成0

\[f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x^2+\cdots +\frac{f^{(n)}(0)}{n!}x^n+o(x^n)\]

注：在ai中我们不关注对于尾巴上的余项 \(o(Δ_x^n)\)的大小估计 。

只要是初等函数，就可以展开成多项式累加的形式，因为f(x)的唯一要求就是无限次可导。这个是Taylor公式的一个很重要的应用。



**对泰勒级数感觉理解不深，到底应用在什么情况下？最好将Taylor公式的所有应用都总结一下。**

**牛顿法的推导用的就是泰勒展开式？是的，牛顿法用的是二阶的泰勒级数来逼近，因为是二阶，所以可以很容易找到\(x_0\)附近的使f(x)最小的x，然后再新建一个二阶泰勒级数来逼近，再求一个新的x，以此类推。关于牛顿法怎么用Taylor公式的要补充在这里。**牛顿法，除非你知道目标函数非常好才采用牛顿法。

这个地方视频里引申了一下，求\(e^x\)的时候，虽然可以用泰勒公式展开来求，但是实际的计算机中并不是用这种方式来求的。不作为重点，因此没有写。


## Taylor公式的使用：




察基尼指数的图像、熵、分类误差率三者之间的关系：








![](http://106.15.37.116/wp-content/uploads/2018/03/img_5ab9dd5c6818a.png)






**这个地方视频里没有怎么讲，说在讲决策树的时候会讨论，是一个非常重要的基尼指数，因此，看到决策树章节的时候回来补充下。**






# 方向导数与梯度




## 方向导数：




如果函数z=f(x,y)在点P(x,y)是可微分的，那么，函数在该点沿任一方向L的方向导数都存在，且有：




\[\frac{\partial f}{\partial l}=\frac{\partial f}{\partial x} cos\theta +\frac{\partial f}{\partial y}sin\theta \]


其中，ψ为x轴到方向L的转角。


## 梯度：




设函数z=f(x,y)在平面区域D内具有一阶连续偏导数，则对于每一个点P(x,y)∈D，向量




\[(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})\]


为函数z=f(x,y)在点P的梯度，记做gradf(x,y) 。梯度的方向是函数在该点变化最快的方向。


## 梯度下降法：


若下山方向和梯度呈θ夹角，下降速度是多少？就是上面的方向导数。

梯度下降就是最速下降。

**可以不沿着梯度下降吗？可以，有些时候还更好，什么时候更好？对这个梯度下降法任然心存疑虑？**




# COMMENT：


**还需要再看下，只要遇到不清楚，不明白的，就补充进来**
