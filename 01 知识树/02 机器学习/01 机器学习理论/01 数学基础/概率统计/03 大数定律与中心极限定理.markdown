---
author: evo
comments: true
date: 2018-03-31 07:29:11+00:00
layout: post
link: http://106.15.37.116/2018/03/31/ai-probability-statistics-the-law-of-large-numbers-and-the-central-limit-theorem/
slug: ai-probability-statistics-the-law-of-large-numbers-and-the-central-limit-theorem
title:
wordpress_id: 2161
categories:
- 随想与反思
tags:
- '@todo'
- '@want_to_know'
---

<!-- more -->

[mathjax]


# REF：






  1. 七月在线 深度学习

********************************************************************************


# 缘由：


对大数定理和中心极限定理进行总结


# 随机变量的矩




## 原点矩与中心矩


对于随机变量X，X的k阶原点矩为：

\[E(X^k)\]

X的k阶中心矩为：

\[E(\{[X-E(X)]^k\}\]


## 统计参数的总结：






  * 期望 一阶原点矩

  * 方差 (标准差)    二阶中心矩

  * 变异系数(Coefficient of Variation)     标准差与期望的比值称为变异系数，记为\(C\cdot V\)

  * 偏度 Skew  (三阶)

  * 峰度 Kurtosis  (四阶)




### 偏度：






  * 偏度衡量随机变量概率分布的不对称性，是概率密度曲线相对于平均值不对称程度的度量。

  * 偏度的值可以为正，可以为负或者无定义。

  * 偏度为负(负偏态)意味着在概率密度函数左侧的尾部比右侧的长，绝大多数的值(包括中位数在内)位于平均值的右侧。

  * 偏度为正(正偏态)意味着在概率密度函数右侧的尾部比左侧的长，绝大多数的值(包括中位数在内)位于平均值的左侧。

  * 偏度为零表示数值相对均匀地分布在平均值的两侧，但不一定意味着一定是对称分布。因此只是一定意义下度量是否偏了。




### 偏度公式：






  * 三阶累积量与二阶累积量的1.5次方的比率。

  * 偏度有时用Skew[X]来表示。




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/gECH510Fhe.png?imageslim)

**这个偏度再实际中会用到吗？使用的场景是什么？**

化成第二个式子的好处是方便编程来求偏度。


### 峰度：


这个不一定是正态分布才有的。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/aED5h64CGl.png?imageslim)

减3是因为正态分布的峰度是3，为了使正态分布从0开始，因此强制减去3。不减3一般叫做超值分布。


##  这个暂时没有讲


X 是一个随机变量，对于任何正整数 n，定义：（**为什么有这么奇怪的定义？怎么出现的？不理解？**）

\[E(X^n)=\int {p(x)}x^ndx\]




  * 当 n = 1 时，\(E(X)\) 为随机变量的期望

  * 当 n = 2 时，\(E(X^2 ) − E(X)^2\) 为随机变量的方差


特征函数，\(E(e^{itX})=\sum_{n=0}^{\infty }\frac{E(X^n)}{n!}(it)^n\)   这实际上使一个涵盖了所有阶矩的一个函数，i是虚数单位，结合之前学的Taylor级数，\(E(e^{itX})\)写出来基本是这样。这个可以全面描述这个概率分布。在证明中心极限定理和大数定律的时候非常有用 ，当你想研究两个随机变量的和的时候就需要用到这个。**为什么这个是特征函数？没明白？在证明中心极限定理和大数定律的时候有用到吗？**

矩可以描述随机变量的一些特征，期望是X“中心”卫视的一种描述，方差可以描述X的分散程度，特征函数可以全面描述概率的分布。**为什么特征函数可以全面描述概率的分布？**




# 切比雪夫公式：




## 如何度量两个随机变量的距离


给定两个随机变量X和Y，如何度量这两个随机变量的“距离”？

相关系数，欧氏距离，K-L距离即相对熵 （在最大熵模型中还会看到） **K-L距离是什么？相对熵是什么？**


## 求随机变量的变化值落在期望值附近的概率


设随机变量X的期望是\(\mu \)，方差是\(\sigma^2\)，对于任意整数\(\epsilon\)，试估计概率\(P\{\mid X-\mu\mid <\epsilon\}\)的下限：

即：随机变量的值落在期望值附近的概率至少是多少？

解：以连续型随机变量为例


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/5bCbl3LgED.png?imageslim)



![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/g9JI8fek5g.png?imageslim) 这个是方差的定义，因此等于σ的平方。


也就是说期望给定，方差给定，则x不会偏离期望太远，而且没有说x是什么分布。

而这个东西就是切比雪夫不等式。


## 切比雪夫公式


设随机变量X的期望是\(\mu \)，方差是\(\sigma^2\)，对于任意整数\(\epsilon\)，有：

\[P\{\mid X-\mu\mid \geq\epsilon\}\leq \frac{\sigma^2}{\epsilon^2}\]

切比雪夫不等式说明，X的方差越小，事件\(\{\mid X-\mu\mid \geq\epsilon\}\)发生的概率越大。即：X取得值基本上集中在期望\(\mu\)附近。

该不等式记忆不说明了方差的含义。而且，可用该不等式证明大数定理。


# 大数定理 概率论的第一个大定理


\(X\) 是随机变量，\(μ\) 是 \(X\) 的期望，\(\sigma^2\) 是 \(X\) 的方差。 \(\{X_k\}_{k=1}^{\infty }\) 是服从 \(X\) 的独立同分步随机变量，那么 \(\overline{X_n}=\frac{\sum_{k=1}^{n}X_k}{n}\) 依概率收敛于 \(μ\) 。也就是说对于任何 \(\epsilon >0\) 有：

\[\lim_{n\rightarrow \infty}P(|\overline{X_n}-μ|>ε)=0\]

**厉害，但是什么是独立同分布随机变量？即：随机变量\(X_1,X_2,\cdots X_n\cdots \)互相独立，并且有相同的期望\(\mu\)和方差\(\sigma^2\) 不确定是不是这样？还是一定要是分布函数是一样的？**


## 大数定理的意义


当n很大时，随机变量\(X_1,X_2,\cdots X_n\)的平均值\(Y_n\)再概率意义下无限接近期望\(\mu\)。出现偏离时可能的，但是这种可能性很小，当n无限大时，这种可能性的概率为0。

即统计平均与期望非常接近，也即统计平均依概率收敛于期望。


## 如何证明大数定理


提示：根据Y的定义，求出它的期望和方差，带入切比雪夫不等式即可。


## 伯努利定理 ：由大数定理得出的重要推论


一次实验中事件A发生的概率为p；重复n次独立实验中，事件A发生了\(n_A\)次，则p、n、n_A的关系满足：**嗯 **

\[lim_{n\rightarrow \infty}P\{\mid\frac{n_A}{n}-p\mid <\epsilon\}=1,\; \forall \epsilon>0\]

这个就是大数定理集合一个给定的分布二项分布得到的一个公式。**这个公式很重要啊，相当于化学里的原子的发现。**

上述推论是最早的大数定理的形式，称为伯努利定理。该定理表明事件A发生的**频率 **\(\frac{n_A}{n}\)以概率收敛于事件A的**概率** p ，**以严格的数学形式表达了频率的稳定性**。**厉害，注意这个地方的频率与概率。**

一旦我们用频率推断概率，实际上用的就是这个伯努利定理！**厉害。**




  * 朴素贝叶斯做垃圾邮件分类  **没明白？为什么这个是这两件事情的依据？**

  * 正态分布的参数估计


频率估计的样本容量怎么确定？给定一个显著性水平，然后进行区间估计。这个没有着重讲，现在讲的都是点估计。**会不会用到？**

朴素贝叶斯假定了大量的独立性，而贝叶斯则是实际中是什么样子就做成什么样子。


# 中心极限定理：概率论的第二个大定理


可以认为是大数定理的加强版。

设随机变量\(X_1,X_2,\cdots X_n\)互相独立，服从同一分布，并且具有相同的期望\(\mu\)和方差\(\sigma^2\)，则随机变量：

\[Y_n=\frac{\sum_{i=1}^{n} X_i-n\mu}{\sqrt{n}\sigma}\]

也就是：

\[Y_n=\frac{\sqrt{n}}{\sigma}(\bar{X}_n-\mu)\]

的分布收敛到标准正态分布。

也就是说，对任何\(\epsilon>0\)，有：

\[\underset{n\rightarrow \infty}{lim} P(Z_n<z)=\Phi(z),\;\forall z\]

其中\(\Phi\)是标准正态分布的分布函数。

而且容易得到：\(\sum_{i=1}^{n}X_i\)收敛到正态分布\(N(n\mu,n\sigma^2)\)。

这个定理的意义在历史上非常重要，它想告诉你的是，当你想通过\(\bar{X}_n\)来估计这个\(\mu\)的时候，到底估计的有多准。嗯，厉害。

**也就是说大数定理和中心极限定理，一个告诉你如何用样本估计\(\mu\)，一个告诉你估计的能有多准。这就是为什么这两个定理为什么这么重要。**

**之前说到n个rand7()的平均值实际上是一个正态分布，就是这个原因。正太分布是高斯分布吗？**






## 标准的中心极限定理的问题：


有一批样本(字符串)，其中a-z开头的比例是固定的，但是量很大，需要从中随机抽样。样本量n，总体中a开头的字符串占比1%，需要每次抽到的a开头的字符串占比(0.99%,+1.01%)，样本量n至少是多少？

问题可以重新表述一下：大量存在的两点分布Bi(1,p)，其中，Bi发生的概率为0.01，即p=0.01。取其中的n个，使得发生的个数除以总数的比例落在区间(0.0099,0.0101)，则n至少是多少？

解：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fD08El80Fb.png?imageslim)




## 中心极限定理的意义：


实际问题中，很多随机现象可以看做许多因素的独立影响的综合反应，往往近似服从**正态分布**。




  * 城市耗电量：大量用户的耗电量总和

  * 测量误差：许多观察不到的、微小误差的总和  **  注意：是多个随机变量的和才可以，有些问题是乘性误差，则需要鉴别或者取对数后再使用。怎么处理？ 这个地方的误差，在线性回归中就用到了，在线性回归中将\(\epsilon\)认为是测量误差，认为它服从正态分布。从而来论证最小二乘法的合理性。什么是最小二乘法？**





#




# COMMENT：


**上面的中心极限定理还是不清楚。**

**追加一下什么是独立：\(P(x_2\mid x_1=a)=P(x_2)\) 就是无论\(x_1\)取什么都对\(x_2\)不影响**
