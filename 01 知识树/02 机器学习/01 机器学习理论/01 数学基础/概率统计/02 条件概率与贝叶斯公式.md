
# TODO
- 还是要补充的，关于似然，没讲，而且关于贝叶斯，还想知道更多更系统。




# ORIGIN
对条件概率和贝叶斯公式进行总结。




# 事件的概率
  * 整个概率空间是一个事件，这个事件一定发生所以全空间的概率为 1
  * 事件是随机变量值域的子集 S
  * 事件的概率则表示 S 里面概率之和或概率密度之积分. <span style="color:red;">这个句子有点怪吧？</span>


# 条件概率公式
  * 条件也是事件，也可表示为随机变量值域的子集:A
  * 条件概率里面的事件，又是这个条件的子集： $S\cap A\subset A$
  * 事件的条件概率则表示 S ∩ A 在 A 里面所占的比例. 故而 $P(S|A)=\frac{P(S\cap A)}{P(A)}$


条件概率公式如下：

$$P(A\mid B)=\frac{P(AB)}{P(B)}$$


# 全概率公式


假设\(\{B_n:n=1,2,3,\cdots \}\)是一个概率空间的有限或者可数无限的[分割](https://zh.wikipedia.org/wiki/%E9%9B%86%E5%90%88%E5%88%92%E5%88%86)（即\(B_n\)为一完备事件组），且每个集合\(B_n\)是一个可测集和，则对任意事件_A_有**全概率公式**：

 \[P(A)=\sum_{n}\P(A\cap B_n)\]

又因为

\[P(A\cap B_{n})=\P(A\mid B_{n})\P(B_{n})\]

此处\(P(A\mid B)\)是B发生后A的[条件概率](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)，所以**全概率公式**又可写作：

\[P(A)=\sum_{n}\P(A\mid B_{n})\P(B_{n}\]

到这里，全概率公式将对一复杂事件A的概率求解问题转化为了在不同情况或不同原因\(B_n\)下发生的简单事件的概率的求和问题。**厉害**




# 贝叶斯公式 Bayes


如果 A,B 是两个事件，那么条件概率满足公式：

\[P(A|B)=\frac{P(B|A)P(A)}{P(B)}\]

利用前面的定义我们知道，事件A，B同时发生的概率为：\(P(A\cap B)\)，一方面：

\[P(A\cap B)=P(B|A)P(A)\]

另一方面对称的有：

\[P(A\cap B)=P(A|B)P(B)\]

所以\(P(B|A)P(A)=P(A|B)P(B)\)，两边同时除以\(P(B)\)就得到了贝叶斯公式。

\[P(B_n\mid A)=\frac{P(A\mid B_n)P(B_n)}{\sum_{n}\P(A\mid B_{n})\P(B_{n}}\]

本来是A发生的时候B_n发生的概率，即B_n是原因，A是结果，但是现在A是原因，B_n是结果，感觉相当于把因果关系搞乱。**是这样吗？**因此，后面谈到贝叶斯网络里面，会用自然语言说因果，但是实际上都说有关联，至于谁是因谁是果不考虑。

**这个地方对贝叶斯公式没怎么讲，实际上我是非常想知道贝叶斯公式的来龙去脉的。再补充下**




# 贝叶斯公式的应用：




支步枪中有5支已校准过，3支未校准。一名射手用校准过的枪射击，中靶概率为0.8；用未校准的枪射击，中靶概率为0.3；现从8支枪中随机取一支射击，结果中靶。求该枪是已校准过的概率。




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mjCfbIgj36.png?imageslim)




# 由贝叶斯公式引发的，频率学派与贝叶斯学派：


给定某系统的若干样本，求该系统的参数。怎么求？




  * 可以用：矩估计/MLE/MaxEnt/EM等：即假定参数是某个/某些未知的定值，求这些参数如何取值，能够使得某目标函数取极大/极小。 这种就属于频率学派。**什么是MLE？MaxEnt？EM？**

  * 也可以用：贝叶斯模型来求：假定参数本身是变化的，服从某个分布。求在这个分布约束下使得某目标函数极大/极小。这种就属于贝叶斯学派。


这两种方式没有高低好坏之分，都是认识自然的手段。只是，在当前人们掌握的数学工具和需解决的实践问题中，贝叶斯学派的理论体系往往能够比较好的解释目标函数、分析相互关系等。**为什么可以比较好的解释和分析？**


前面章节的内容，大多是频率学派的思想；下面的推理，使用贝叶斯学派的观点。




**还是没明白什么时频率学派，什么是贝叶斯学派？**





# 先验概率，后验概率，似然函数


**这个是我一直没有明白清楚的**

给定某系统的若干样本x，计算该系统的参数，即：

\[P(\theta \mid x)=\frac{P(x\mid \theta)P(\theta )}{P(x)}\]

那么，先验概率是什么呢？

就是没有给定任何信息，你想猜这个概率，没有任何东西可言，比如没有任何信息猜一个人姓什么。对应式子中就是：P(\theta)，即在没有数据支持的情况下，\(\theta\)发生的概率。

后验概率是什么呢？

有了一些知识之后猜的概率，比如某人来自牛家村，那么姓牛的概率就比较大，这个就是后验概率，但是他虽然来自牛家村也可能姓马，所以不管先验后验都可能出错的，不是一定对的。对应式子中就是：\(P(\theta\mid x)\)：在数据x的支持下，\(\theta\)发生的概率。

那么什么是似然函数呢？

式子中的P(x\mid\theta)\)：给定某参数\(\theta\)的概率分布就是似然函数。即：在某一个参数的情况下，样本的发生概率就是似然函数，**样本像那个样子得到了。****这个似然这个词到底是什么意思？确认下。**




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/LhhIFe0dEc.png?imageslim)

**这个地方是在讲贝叶斯网络的时候讲到的。在琢磨下。**


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fkKFkH8HlF.png?imageslim) 这个地方我们假定了 \(P(A_i)\) 是相等的。要注意。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/7G5D8Di30l.png?imageslim)指的是：在给定数据的时候，我们想去看一下那一条结论是最有可能发生的。但是我们往往算的是反方向，在A_i给定的时候这个数据出现的极大值。而这个


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/ad45g7Flcm.png?imageslim)就是极大似然估计。再理解下。


所以极大似然估计其实是先验性的假定了A_1,A_2,A_n  出现的概率是相同的。

但是在贝叶斯学派中，最多的是探讨先验概率 \(P(A_i)\) 如果是不相等的时候如何进行建模的问题。**那么先验概率怎么知道是不是相等的？**

怎么理解P(D)是常数呢，仅为归一化因子？P(D)其实是似然，它是表示样本已经出现了。**还是没有很理解？**




# REF
1. 七月在线 深度学习
2. 七月在线 机器学习
