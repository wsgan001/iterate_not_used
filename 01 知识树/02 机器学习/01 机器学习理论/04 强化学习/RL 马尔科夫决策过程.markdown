---
author: evo
comments: true
date: 2018-05-17 05:53:08+00:00
layout: post
link: http://106.15.37.116/2018/05/17/rl-%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/
slug: rl-%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b
title: RL 马尔科夫决策过程
wordpress_id: 5928
categories:
- 人工智能学习
tags:
- NOT_ADD
- Reinforcement Learning
---

<!-- more -->

[mathjax]

**注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。**


# ORIGINAL





 	
  1. 


[强化学习入门 第一讲 MDP](https://zhuanlan.zhihu.com/p/25498081)







# TODO





 	
  * aaa





* * *





# INTRODUCTION





 	
  * aaa




# 知识前提





 	
  * 概率相关知识。概率在强化学习中还是很重要的。

 	
    * 首先，强化学习的策略往往是随机策略。采用随机策略的好处是可以将探索耦合到采样的过程中，探索就是指机器人尝试其他的动作以便找到更好的策略。**什么是随机策略？什么是将探索耦合到采样的过程中？**

 	
    * 其次，在实际应用中，存在各种噪声，这些噪声大都服从正态分布，如何去掉这些噪声也需要用到概率的知识。**怎么去除噪声？**








# 马尔科夫决策过程介绍




## 马尔科夫决策过程的发展


无数学者们通过几十年不断地努力和探索，提出了一套可以解决大部分强化学习问题的框架，这个框架就是马尔科夫决策过程，简称 MDP。**是这样吗？马尔科夫决策过程是为了解决强化学习的问题的吗？**


## 到底什么是马尔科夫决策过程？





# 文章主题


我们会按下列顺序循序渐进地介绍马尔科夫决策过程：



 	
  1. 马尔科夫性

 	
  2. 马尔科夫过程

 	
  3. 马尔科夫决策过程




## 





# 首先，是马尔可夫性


什么是马尔科夫性？

所谓马尔科夫性，就是指系统的下一个状态 \(s_{t+1}\) 仅与当前状态 \(s_t\) 有关，而与以前的状态无关。**这个马尔可夫性的前提和应用范围还是要提一下的吧？不然大家以为是普遍适用的吧？前提是什么？**

OK，我们把定义写出来：

状态 \(s_t\) 是马尔科夫的，当且仅当 \( P\left[s_{t+1}|s_t\right]=P\left[s_{t+1}|s_1,\cdots ,s_t\right] \) 。

从定义中可以看到，当前状态 \(s_t\) 其实是蕴含了所有相关的历史信息 \(s_1,\cdots ,s_t\)，因此一旦当前状态已知，那么所有的历史信息可以被抛弃。**我们怎么知道一个系统是不是满足马尔科夫性呢？**

OK，马尔科夫性描述的是每个状态的性质。

那么马尔科夫随机过程又是什么呢？


# 马尔科夫过程是什么？


首先我们要知道：在数学中，所谓的随机过程就是指**随机变量序列**。因此，如果随机变量序列中的每个状态都是马尔科夫的，那么我们就称这个随机过程为马尔科夫随机过程。它本质上还是一个变量序列，即一个状态序列。**马尔科夫随机过程跟马尔科夫过程是一个意思吗？**

那么马尔科夫过程呢？定义如下：

马尔科夫过程是一个二元组 \(\left(S,P\right)\) ，且满足：



 	
  * \(S\) 是有限状态集合

 	
  * \(P\) 是状态转移概率 ，是一个矩阵：\( P=\left[\begin{matrix} P_{11}& \cdots& P_{1n}\\ \vdots& \vdots& \vdots\\ P_{n1}& \cdots& P_{nn}\\ \end{matrix}\right] \)


OK，下面我们借用一个例子来说明这里面的参数和整个的马尔科夫过程：

![](https://pic1.zhimg.com/80/v2-6722fb1c69f547d8f1630142ee63d231_hd.jpg)



 	
  * 学生的状态集为： { 娱乐，课程1，课程2， 课程3，考过，睡觉，论文 }，7 种状态。每种状态之间的转换概率已经标注在图上。




现在，一个学生以 课程1 为开始，那么接下来他的状态序列可能为：

 	
  * 课1-课2-课3-考过-睡觉

 	
  * 课1-课2-睡觉


上面这种状态序列就被称为马尔科夫链。

可以看出，当给定状态转移概率的时候，从某个状态出发存在着多条马尔科夫链。

实际上，对于人或者机器人，马尔科夫过程还不足以描述其行为特点，因为不管是人还是机器人，他们都是通过动作与环境进行交互，并从环境中获得奖励，而马尔科夫过程中不存在动作和奖励。

OK，我们现在把动作（策略）和回报添加进去看下：


# 把决策和回报考虑在内的马尔科夫过程就是马尔科夫决策过程


把决策和回报考虑在内的马尔科夫过程就是马尔科夫决策过程。

定义如下：



马尔科夫决策过程由元组 \(\left(S,A,P,R,\gamma\right)\) 描述，其中：



 	
  * \(S\) 为有限的状态集

 	
  * \(A\) 为有限的动作集

 	
  * \(P\) 为状态转移概率

 	
  * \(R\) 为回报函数 **回报函数是怎么定的？**

 	
  * \(\gamma \) 为折扣因子，用来计算累积回报。**什么是折扣因子？以前看的时候就没怎么明白。**


注意：跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的即： \(P_{ss'}^{a}=P\left[S_{t+1}=s'|S_t=s,A_t=a\right]\)

OK，我们同样举上面的例子，然后把动作的回报添加进来：

![](https://pic2.zhimg.com/80/v2-604dcc4d56bfdfe410886f99520a5bdb_hd.jpg)上面就是马尔科夫决策过程的示例图，与之前的例子类似。之前讲马尔科夫过程，只有状态，现在有动作集了：



 	
  * 学生的状态集为 \(S=\left\{s_1,s_2,s_3,s_4,s_5\right\}\) ，五个状态。

 	
  * 动作集为 A= { 玩，退出，学习，发论文，睡觉 } 。


而动作的立即回报已经用红色标记出来。**要强调是立即回报吗？**

强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。而所谓的策略其实就是状态到动作的映射。

OK，策略的定义如下：是用条件概率分布给出的

策略常用符号 \(\pi \) 表示，它是指给定状态 \(s\) 时，动作集上的一个分布，即

\[ \pi\left(a|s\right)=p\left[A_t=a|S_t=s\right] \] (1.1)

这个公式是什么意思呢？





言归正传，公式(1.1)的含义是：策略 \(\pi\) 在每个状态 \(s\) 指定一个动作概率。如果给出的策略 \(\pi\) 是确定性的，那么策略 \(\pi\) 在每个状态 \(s\) 指定一个确定的动作。

例如其中一个学生的策略为 \[ \pi_1\left(\textrm{玩}|s_1\right)=0.8 \] ，是指该学生在状态 \(s_1\) 时玩的概率为0.8，不玩的概率是0.2，显然这个学生更喜欢玩。

另外一个学生的策略为 \(\pi_2\left(\textrm{玩}|s_1\right)=0.3\) ，是指该学生在状态 \(s_1\) 时玩的概率是0.3，显然这个学生不爱玩。依此类推，每学生都有自己的策略。强化学习是找到最优的策略，这里的最优是指得到的总回报最大。

当给定一个策略 \(\pi\) 时，我们就可以计算累积回报了。首先定义累积回报：

\[ G_t=R_{t+1}+\gamma R_{t+2}+\cdots =\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}}\\\\\ \left(1.2\right) \]

当给定策略 \(\pi\) 时，假设从状态 \(s_1\) 出发，学生状态序列可能为：

\( s_1\rightarrow s_2\rightarrow s_3\rightarrow s_4\rightarrow s_5 ;\\ s_1\rightarrow s_2\rightarrow s_3\rightarrow s_5 \\ ......\)

此时，在策略 \(\pi\) 下，利用(1.2)式可以计算累积回报 \(G_1\) ，此时 \(G_1\) 有多个可能值 。由于策略 \(\pi\) 是随机的，因此累积回报也是随机的。为了评价状态 \(s_1\) 的价值，我们需要定义一个确定量来描述状态 \(s_1\) 的价值，很自然的想法是利用累积回报来衡量状态 \(s_1\) 的价值。然而，累积回报 \(G_1\) 是个随机变量，不是一个确定值，因此无法进行描述。但其期望是个确定值，可以作为状态值函数的定义。

状态值函数：

当智能体采用策略 \(\pi\) 时，累积回报服从一个分布，累积回报在状态 \(s\) 处的期望值定义为状态-值函数：

\(\upsilon_{\pi}\left(s\right)=E_{\pi}\left[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s}\right]\) (1.3)

注意：状态值函数是与策略 \(\pi\) 相对应的，这是因为策略 \(\pi\) 决定了累积回报G的状态分布。

图1.4 状态值函数示意图



如图1.4所示为与图1.3相对应的状态值函数图。图中白色圆圈中的数值为该状态下的值函数。即：

\( \upsilon_{\pi}\left(s_1\right)=-2.3,\\ \upsilon_{\pi}\left(s_2\right)=-1.3,\\ \upsilon_{\pi}\left(s_3\right)=2.7,\\\upsilon_{\pi}\left(s_4\right)=7.4,\\ \upsilon_{\pi}\left(s_5\right)=0 \)



相应地，状态-行为值函数为：

\[ q_{\pi}\left(s,a\right)=E_{\pi}\left[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s,A_t=a}\right] \] (1.4)

式(1.3)和式(1.4)分别给出了状态值函数和状态-行为值函数的定义计算式，但在实际真正计算和编程的时候并不会按照定义式去编程。接下来我们会从不同的方面对定义式进行解读。

**状态值函数与状态-行为值函数的贝尔曼方程**

由状态值函数的定义式(1.3)可以得到：

\( \upsilon\left(s\right)=E\left[G_t|S_t=s\right] \\ =E\left[R_{t+1}+\gamma R_{t+2}+\cdots |S_t=s\right] \\ =E\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\cdots\right)|S_t=s\right] \\ =E\left[R_{t+1}+\gamma G_{t+1}|S_t=s\right] \\ =E\left[R_{t+1}+\gamma\upsilon\left(S_{t+1}\right)|S_t=s\right] \) (1.5)

**最后一个等号的补充证明：**

\(\\ V\left(S_t\right)=E_{s_t,s_{t+1},\cdots}\left(R\left(t+1\right)+\gamma G\left(S_{t+1}\right)\right) \\ =E_{s_t}\left(R\left(t+1\right)+\gamma E_{s_{t+1},\cdots}\left(G\left(S_{t+1}\right)\right)\right) \\ =E_{s_t}\left(R\left(t+1\right)+\gamma V\left(S_{t+1}\right)\right) \\ =E\left(R\left(t+1\right)+\gamma V\left(S_{t+1}\right)\right) \)

**需要注意的是对哪些变量求期望。**



同样我们可以得到状态-动作值函数的贝尔曼方程：

\[q_{\pi}\left(s,a\right)=E_{\pi}\left[R_{t+1}+\gamma q\left(S_{t+1},A_{t+1}\right)|S_t=s,A_t=a\right] \eqno{(1.6)}\]

状态值函数与状态-行为值函数的具体推导过程：

图1.5和图1.6分别为状态值函数和行为值函数的具体计算过程。其中空心圆圈表示状态，实心圆圈表示状态-行为对。

图1.5 状态值函数的计算示意图

图1.5为值函数的计算分解示意图，图1.5B计算公式为：

\[ \upsilon_{\pi}\left(s\right)=\sum_{a\in A}{\pi\left(a|s\right)q_{\pi}\left(s,a\right)}\\\\\left(1.7\right) \]

图1.5B给出了状态值函数与状态-行为值函数的关系。图1.5C计算状态-行为值函数为：

\[ q_{\pi}\left(s,a\right)=R_{s}^{a}+\gamma\sum_{s'}{P_{ss'}^{a}}\upsilon_{\pi}\left(s'\right)\\\\\left(1.8\right) \]

将(1.8)式带入到(1.7)式可以得到：

\[ \upsilon_{\pi}\left(s\right)=\sum_{a\in A}{\pi\left(a|s\right)\left(R_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}\upsilon_{\pi}\left(s'\right)}\right)}\\\\\left(1.9\right) \]

图1.6 状态-行为值函数计算

在1.6C中，

\[ \upsilon_{\pi}\left(s'\right)=\sum_{a'\in A}{\pi\left(a'|s'\right)q_{\pi}\left(s',a'\right)}\\\\\left(1.10\right) \]

将(1.10)带入到(1.8)中，得到行为状态-行为值函数：

\[ q_{\pi}\left(s,a\right)=R_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}\sum_{a'\in A}{\pi\left(a'|s'\right)q_{\pi}\left(s',a'\right)}}\\\\\left(1.11\right) \]

公式(1.9)可以在图1.4中进行验证。选择状态 \(s_4\) 处。由图1.4知道 \(\upsilon\left(s_4\right)=7.4\) ，由公式(1.9)得：

\[ \upsilon\left(s_4\right)=0.5*\left(1+0.2*\left(-1.3\right)+0.4*2.7+0.4*7.4\right)+0.5*10=7.39 \]

保留一位小数为7.4

计算状态值函数的目的是为了构建学习算法从数据中得到最优策略。每个策略对应着一个状态值函数，最优策略自然对应着最优状态值函数。

定义：最优状态值函数 \(\upsilon^*\left(s\right)\) ,为在所有策略中值最大的值函数即： \(\upsilon^*\left(s\right)=\max_{\pi}\upsilon_{\pi}\left(s\right)\) ，最优状态-行为值函数 \(q^*\left(s,a\right)\) 为在所有策略中最大的状态-行为值函数，即：

\(q^*\left(s,a\right)=\max_{\pi}q_{\pi}\left(s,a\right)\)

我们由(1.9)式和(1.11)式分别得到最优状态值函数和最优状态-行动值函数的贝尔曼最优方程：

\[ \upsilon^*\left(s\right)=\max_aR_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}\upsilon^*\left(s'\right)\\\\\left(1.12\right)} \]

\[ q^*\left(s,a\right)=R_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}\max_{a'}q^*\left(s',a'\right)}\\\\\left(1.13\right) \]

若已知最优状态-动作值函数，最优策略可通过直接最大化 \(q^*\left(s,a\right)\) 来决定。

\[ \pi_*\left(a|s\right)=\left\{\begin{array}{c} 1\ if\ a=\underset{a\in A}{arg\max}q_*\left(s,a\right)\\ 0\ otherwis\\ \end{array}\right. \] (1.14)

图1.7 最优值函数和最优策略



图1.7为最优状态值函数示意图，图中红色箭头所示的动作为最优策略。

至此，我们将强化学习的基本理论介绍完毕。现在是时候该对强化学习算法进行形式化描述了。

我们定义一个离散时间有限范围的折扣马尔科夫决策过程 \[ M=\left(S,A,P,r,\rho_0,\gamma ,T\right) \] ，其中 \(S\) 为状态集， \(A\) 为动作集， \(P:S\times A\times S\rightarrow R\) 是转移概率， \[ r:S\times A\rightarrow\left[-R_{\max},R_{\max}\right] \] 为立即回报函数， \[ \rho_0:S\rightarrow R \] 是初始状态分布， \[ \gamma\in\left[0,1\right] \] 为折扣因子， \(T\) 为水平范围（其实就是步数）. \(\tau\) 为一个轨迹序列，即 \(\tau =\left(s_0,a_0,s_1,a_1,\cdots\right)\) ，累积回报为 \[ R=\sum_{t=0}^T{\gamma^t}r_t \] ，强化学习的目标是：找到最优策略 \(\pi\) ，使得该策略下的累积回报期望最大，即： \[ \max_{\pi}\int{R\left(\tau\right)}p_{\pi}\left(\tau\right)d\tau \] 。

根据策略最优定理知道，当值函数最优时采取的策略也是最优的。反过来，策略最优时值函数也最优。



图1.8 强化学习算法分类

如图1.8所示，强化学习算法根据以策略为中心还是以值函数最优可以分为两大类，策略优化方法和动态规划方法。其中策略优化方法又分为进化算法和策略梯度方法；动态规划方法分为策略迭代算法和值迭代算法。策略迭代算法和值迭代算法可以利用广义策略迭代方法进行统一描述。

另外，强化学习算法根据策略是否是随机的，分为确定性策略强化学习和随机性策略强化学习。根据转移概率是否已知可以分为基于模型的强化学习算法和无模型的强化学习算法。另外，强化学习算法中的回报函数 \(r\) 十分关键，根据回报函数是否已知，可以分为强化学习和逆向强化学习。逆向强化学习是根据专家实例将回报函数学出来。

第一讲结束，如果对你有帮助，请赞赏，支持原创教程。





















* * *





# COMMENT





