[TOC]

# 机器学习背景介绍

1. [机器学习介绍](http://106.15.37.116/2018/03/27/ml-introduce/)

2. [机器学习发展历程](http://106.15.37.116/2018/05/23/ml-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b/)

3. [机器学习应用现状](http://106.15.37.116/2018/05/23/ml-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%ba%94%e7%94%a8%e7%8e%b0%e7%8a%b6/)


# 机器学习的发展探讨





# 机器学习术语及概念

1. [基本术语](http://106.15.37.116/2018/05/23/ml-%e5%9f%ba%e6%9c%ac%e6%9c%af%e8%af%ad/)

2. [假设空间](http://106.15.37.116/2018/05/23/ml-%e5%81%87%e8%ae%be%e7%a9%ba%e9%97%b4/)

3. 归纳偏好

    4. [距离](http://106.15.37.116/2018/05/20/ml-distance/)  这个要好好总结下 都吸收补充过来

# 模型评估与选择

1. [经验误差与过拟合](http://106.15.37.116/2018/05/23/ml-%e7%bb%8f%e9%aa%8c%e8%af%af%e5%b7%ae%e4%b8%8e%e8%bf%87%e6%8b%9f%e5%90%88/)

2. [划分好测试集、验证集，为评估做准备](http://106.15.37.116/2018/05/23/ml-%e5%88%92%e5%88%86%e5%a5%bd%e6%b5%8b%e8%af%95%e9%9b%86%e3%80%81%e9%aa%8c%e8%af%81%e9%9b%86%ef%bc%8c%e4%b8%ba%e8%af%84%e4%bc%b0%e5%81%9a%e5%87%86%e5%a4%87/)

3. [调参数](http://106.15.37.116/2018/05/23/ml-%e8%b0%83%e5%8f%82%e6%95%b0/)  这个放在模型的评估里面不是很恰当。

4. [性能度量](http://106.15.37.116/2018/05/23/ml-%e6%80%a7%e8%83%bd%e5%ba%a6%e9%87%8f/)

5. [比较检验](http://106.15.37.116/2018/05/24/ml-%e6%af%94%e8%be%83%e6%a3%80%e9%aa%8c/) 没有整理完

6. [偏差与方差](http://106.15.37.116/2018/05/24/ml-%e5%81%8f%e5%b7%ae%e4%b8%8e%e6%96%b9%e5%b7%ae/) 讲的还是很好的，再看下。



# 计算学习理论

1. 计算学习理论

 2. [基础知识](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86/)

             2. [PAC学习](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-pac%e5%ad%a6%e4%b9%a0/)

             3. [有限假设空间](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-%e6%9c%89%e9%99%90%e5%81%87%e8%ae%be%e7%a9%ba%e9%97%b4/)

             4. [VC维](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-vc%e7%bb%b4/)

             5. [Rademacher 复杂度](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-rademacher%e5%a4%8d%e6%9d%82%e5%ba%a6/)

             6. [稳定性](http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-%e7%a8%b3%e5%ae%9a%e6%80%a7/)

# 数据与特征的处理

1. 降维与度量学习

     1. [k近邻学习](http://106.15.37.116/2018/05/24/ml-k%e8%bf%91%e9%82%bb%e5%ad%a6%e4%b9%a0/)        [k近邻学习](http://106.15.37.116/2018/05/06/knn/)

         1. [例子1：根据信息选择约会对象](http://106.15.37.116/2018/05/06/knn-sample1/)
         2. [例子2：手写数字识别系统](http://106.15.37.116/2018/05/07/knn-sample2/)

     2. [低维嵌入](http://106.15.37.116/2018/05/24/ml-%e4%bd%8e%e7%bb%b4%e5%b5%8c%e5%85%a5/)
     3. [主成分分析](http://106.15.37.116/2018/05/24/ml-%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90/)
     4. [核化线性降维](http://106.15.37.116/2018/05/24/ml-%e6%a0%b8%e5%8c%96%e7%ba%bf%e6%80%a7%e9%99%8d%e7%bb%b4/)
     5. [流形学习](http://106.15.37.116/2018/05/24/ml-%e6%b5%81%e5%bd%a2%e5%ad%a6%e4%b9%a0/)
     6. [度量学习](http://106.15.37.116/2018/05/24/ml-%e5%ba%a6%e9%87%8f%e5%ad%a6%e4%b9%a0/)

1. 特征选择与稀疏学习

 2. [子集搜索与评价](http://106.15.37.116/2018/05/24/ml-%e5%ad%90%e9%9b%86%e6%90%9c%e7%b4%a2%e4%b8%8e%e8%af%84%e4%bb%b7/)

             2. [过滤式选择](http://106.15.37.116/2018/05/24/ml-%e8%bf%87%e6%bb%a4%e5%bc%8f%e9%80%89%e6%8b%a9/)

             3. [包裹式选择](http://106.15.37.116/2018/05/24/ml-%e5%8c%85%e8%a3%b9%e5%bc%8f%e9%80%89%e6%8b%a9/)

             4. [嵌入式选择与L1正则化](http://106.15.37.116/2018/05/24/ml-%e5%b5%8c%e5%85%a5%e5%bc%8f%e9%80%89%e6%8b%a9%e4%b8%8el1%e6%ad%a3%e5%88%99%e5%8c%96/)

             5. [稀疏表示与字典学习](http://106.15.37.116/2018/05/24/ml-%e7%a8%80%e7%96%8f%e8%a1%a8%e7%a4%ba%e4%b8%8e%e5%ad%97%e5%85%b8%e5%ad%a6%e4%b9%a0/)

             6. [压缩感知](http://106.15.37.116/2018/05/24/ml-%e5%8e%8b%e7%bc%a9%e6%84%9f%e7%9f%a5/)

[采样 更新](http://106.15.37.116/2018/03/30/sampling/)

[变分](http://106.15.37.116/2018/04/28/variational-inference/)



# 机器学习

1. 线性模型    回归算法 需要拆分

       1. [基本形式](http://106.15.37.116/2018/05/24/ml-%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b-%e5%9f%ba%e6%9c%ac%e5%bd%a2%e5%bc%8f/)
         2. [线性回归](http://106.15.37.116/2018/04/29/linear-regression/)
         3. [逻辑回归](http://106.15.37.116/2018/04/29/logistic-regression/)
         4. [线性判别分析 LDA](http://106.15.37.116/2018/04/29/lda-2/)
         5. [多分类问题](http://106.15.37.116/2018/05/24/ml-%e5%a4%9a%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98/) 未整理
         6. [类别不平衡问题](http://106.15.37.116/2018/05/24/ml-%e7%b1%bb%e5%88%ab%e4%b8%8d%e5%b9%b3%e8%a1%a1%e9%97%ae%e9%a2%98/) 未整理
   2. 决策树   决策树算法
          1. [基本流程](http://106.15.37.116/2018/04/30/decision-tree-basic-flow/)
        2. [划分选择](http://106.15.37.116/2018/04/30/decision-tree-division-selection/)
        3. [剪枝处理](http://106.15.37.116/2018/05/24/ml-%e5%86%b3%e7%ad%96%e6%a0%91-%e5%89%aa%e6%9e%9d%e5%a4%84%e7%90%86/)
        4. [连续与缺失值](http://106.15.37.116/2018/05/24/ml-%e5%86%b3%e7%ad%96%e6%a0%91-%e8%bf%9e%e7%bb%ad%e4%b8%8e%e7%bc%ba%e5%a4%b1%e5%80%bc/) 未整理
        5. [多变量决策树](http://106.15.37.116/2018/05/24/ml-%e5%86%b3%e7%ad%96%e6%a0%91-%e5%a4%9a%e5%8f%98%e9%87%8f%e5%86%b3%e7%ad%96%e6%a0%91/) 未整理
        6. [例子1：判定鱼类和非鱼类](http://106.15.37.116/2018/05/08/decision-tree-sample1/)
 3. 支持向量机     支持向量机（SVM）
        1. [间隔与支持向量](http://106.15.37.116/2018/05/24/ml-svm-%e9%97%b4%e9%9a%94%e4%b8%8e%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f/)
        2. [对偶问题](http://106.15.37.116/2018/05/24/ml-svm-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98/)
        3. [核函数](http://106.15.37.116/2018/05/24/ml-svm-%e6%a0%b8%e5%87%bd%e6%95%b0/)
        4. [软间隔与正则化](http://106.15.37.116/2018/05/24/ml-svm-%e8%bd%af%e9%97%b4%e9%9a%94%e4%b8%8e%e6%ad%a3%e5%88%99%e5%8c%96/)
        5. [支持向量回归](http://106.15.37.116/2018/05/24/ml-svm-%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e5%9b%9e%e5%bd%92/)
        6. [核方法](http://106.15.37.116/2018/05/24/ml-svm-%e6%a0%b8%e6%96%b9%e6%b3%95/)	
4. 贝叶斯分类
       1. [贝叶斯决策论](http://106.15.37.116/2018/05/24/ml-bayes-%e8%b4%9d%e5%8f%b6%e6%96%af%e5%86%b3%e7%ad%96%e8%ae%ba/)
       2. [极大似然估计](http://106.15.37.116/2018/05/24/ml-bayes-%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1/)
       3. [朴素贝叶斯分类器](http://106.15.37.116/2018/05/24/ml-bayes-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8/)     [朴素贝叶斯](http://106.15.37.116/2018/04/26/naive-bayes/)
       4. [半朴素贝叶斯分类器](http://106.15.37.116/2018/05/24/ml-bayes-%e5%8d%8a%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e5%99%a8/)
       5. [贝叶斯网络](http://106.15.37.116/2018/05/24/ml-bayes-%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c/)   [贝叶斯网络（Bayesian Belief Network）](http://106.15.37.116/2018/03/30/bayesian-network/)
       6. [EM算法](http://106.15.37.116/2018/05/24/ml-bayes-em%e7%ae%97%e6%b3%95/)    [EM 算法](http://106.15.37.116/2018/03/30/em/) 到底是属于聚类算法还是贝叶斯里面的？	

5. 集成学习   集成算法
       1. [个体与集成](http://106.15.37.116/2018/05/24/ml-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0-%e4%b8%aa%e4%bd%93%e4%b8%8e%e9%9b%86%e6%88%90/)
       2. [Boosting](http://106.15.37.116/2018/05/24/ml-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0-boosting/)
       3. [AdaBoost](http://106.15.37.116/2018/03/30/boosting/)
       4. [GBDT](http://106.15.37.116/2018/05/12/gbdt/)
       5. xgboost
       6. [lightGBM](http://106.15.37.116/2018/05/05/lightgbm/)
       7. [Bagging与随机森林](http://106.15.37.116/2018/05/24/ml-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0-bagging%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97/)   [随机森林](http://106.15.37.116/2018/04/26/random-forests/)
       8. [结合策略](http://106.15.37.116/2018/05/24/ml-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0-%e7%bb%93%e5%90%88%e7%ad%96%e7%95%a5/)
       9. [多样性](http://106.15.37.116/2018/05/24/ml-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0-%e5%a4%9a%e6%a0%b7%e6%80%a7/)	

6. 聚类   聚类算法
       1. [聚类任务](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e8%81%9a%e7%b1%bb%e4%bb%bb%e5%8a%a1/)
       2. [性能度量](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e6%80%a7%e8%83%bd%e5%ba%a6%e9%87%8f/)
       3. [距离计算](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e8%b7%9d%e7%a6%bb%e8%ae%a1%e7%ae%97/)
       4. [原型聚类](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e5%8e%9f%e5%9e%8b%e8%81%9a%e7%b1%bb/)
              1. [K-means ](http://106.15.37.116/2018/05/06/k-means/)  之前不知道k-means 还是原型聚类
       5. [密度聚类](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e5%af%86%e5%ba%a6%e8%81%9a%e7%b1%bb/)       [密度聚类](http://106.15.37.116/2018/05/06/density-clustering/)
              1. DBSCAN
       6. [层次聚类](http://106.15.37.116/2018/05/24/ml-%e8%81%9a%e7%b1%bb-%e5%b1%82%e6%ac%a1%e8%81%9a%e7%b1%bb/)             [层次聚类](http://106.15.37.116/2018/05/06/hierarchical-clustering/)
       7. [谱聚类](http://106.15.37.116/2018/05/06/%e8%b0%b1%e8%81%9a%e7%b1%bb/)
       8. [网格聚类](http://106.15.37.116/2018/05/06/grid-clustering/)
       9. [模型聚类](http://106.15.37.116/2018/05/06/model-clustering/)
 7. 半监督学习

        1. [未标记样本](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e6%9c%aa%e6%a0%87%e8%ae%b0%e6%a0%b7%e6%9c%ac/)
        2. [生成式方法](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e7%94%9f%e6%88%90%e5%bc%8f%e6%96%b9%e6%b3%95/)
        3. [半监督SVM](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e5%8d%8a%e7%9b%91%e7%9d%a3svm/)
        4. [图半监督学习](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e5%9b%be%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0/)
        5. [基于分歧的方法](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e5%9f%ba%e4%ba%8e%e5%88%86%e6%ad%a7%e7%9a%84%e6%96%b9%e6%b3%95/)
        6. [半监督聚类](http://106.15.37.116/2018/05/24/ml-%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0-%e5%8d%8a%e7%9b%91%e7%9d%a3%e8%81%9a%e7%b1%bb/)

8. 概率图模型
       1. [隐马尔科夫模型](http://106.15.37.116/2018/05/24/ml-%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e6%a8%a1%e5%9e%8b/)    [HMM](http://106.15.37.116/2018/03/30/hmm/)
       2. [马尔科夫随机场](http://106.15.37.116/2018/05/24/ml-%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e9%9a%8f%e6%9c%ba%e5%9c%ba/)
       3. [条件随机场](http://106.15.37.116/2018/05/24/ml-%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba/)   [条件随机场](http://106.15.37.116/2018/03/30/conditional-random-field-crf/)
       4. [学习与推断](http://106.15.37.116/2018/05/24/ml-%e5%ad%a6%e4%b9%a0%e4%b8%8e%e6%8e%a8%e6%96%ad/)
       5. [近似推断](http://106.15.37.116/2018/05/24/ml-%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad/)
       6. [话题模型](http://106.15.37.116/2018/05/24/ml-%e8%af%9d%e9%a2%98%e6%a8%a1%e5%9e%8b/)   [主题模型](http://106.15.37.116/2018/04/27/topic-model/)
       7. [pLSA](http://106.15.37.116/2018/04/27/plsa/)
       8. [LDA （Latent Dirichlet allocation）](http://106.15.37.116/2018/03/30/lda/)

9. 规则学习
       1. [基本概念](http://106.15.37.116/2018/05/24/ml-%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0-%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5/)
       2. [序贯覆盖](http://106.15.37.116/2018/05/24/ml-%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0-%e5%ba%8f%e8%b4%af%e8%a6%86%e7%9b%96/)
       3. [剪枝优化](http://106.15.37.116/2018/05/24/ml-%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0-%e5%89%aa%e6%9e%9d%e4%bc%98%e5%8c%96/)
       4. [一阶规则学习](http://106.15.37.116/2018/05/24/ml-%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0-%e4%b8%80%e9%98%b6%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0/)
       5. [归纳逻辑程序设计](http://106.15.37.116/2018/05/24/ml-%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0-%e5%bd%92%e7%ba%b3%e9%80%bb%e8%be%91%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1/)
 10. [异常检测算法](http://106.15.37.116/2018/04/28/anomaly-detection-algorithm/) NULL
        1. [梯度下降和拟牛顿](http://106.15.37.116/2018/03/28/gradient-descent-and-quasi-newton-method/)
        2. [最大熵模型](http://106.15.37.116/2018/03/28/maximum-entropy-model/)
       3. [从线性分类器开始](http://106.15.37.116/2018/04/01/linear-classifier/)
11. 推荐算法
       1. [协同过滤](http://106.15.37.116/2018/04/28/collaborative-filtering/)
12. 关系图算法
       1. [标签传播](http://106.15.37.116/2018/05/06/%e6%a0%87%e7%ad%be%e4%bc%a0%e9%80%92%e7%ae%97%e6%b3%95/)  这个当时七月在线里面是与聚类一起讲的。
       2. Dijkstra最短路径

# 需要补充的

- 多示例学习 Multi Instance Learning
- 多标签学习 multi-label
- 

# 需要消化的

- 《机器学习》 周志华 书中的题目也要总结一下。

  
