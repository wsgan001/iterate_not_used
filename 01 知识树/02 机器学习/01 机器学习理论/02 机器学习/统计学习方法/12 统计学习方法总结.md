##### 第12章统计学习方法总结

本书共介绍了 10种主要的统计学习方法：感知机、女近邻法、朴素贝叶斯 法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、EM算法、隐 马尔可夫模型和条件随机场.这10种统计学习方法的特点概括总结在表12.1中.

表12.1 10种统计学习方法特点的概括总结

| 方法                       | 适用问题          | 模型特点                                               | 模型类型 | 学习策略                             | 学习的损 失函数      | 学习算法                                  |
| -------------------------- | ----------------- | ------------------------------------------------------ | -------- | ------------------------------------ | -------------------- | ----------------------------------------- |
| 感知机                     | 二类分类          | 分离超平面                                             | 判别模型 | 极小化误分点 到超平面距离            | 误分点到 超平面距 离 | 随机梯度下降                              |
| *近邻法                    | 多类分类， 回归   | 特征空间，样 本点                                      | 判别模型 |                                      |                      |                                           |
| 朴素贝叶斯 法              | 多类分类          | 特征与类别 的联合概率 分布，条件独 立假设              | 生成模型 | 极大似然估 计，极大后验 概率估计     | 对数似然 损失        | 概率计算公式， EM算法                     |
| 决策树                     | 多类分类， 回归   | 分类树，回归 树                                        | 判别模型 | 正则化的极大 似然估计                | 对数似然 损失        | 特征选择，生 成，剪枝                     |
| 逻辑斯谛回 归与最大熵 模型 | 多类分类          | 特征条件下 类别的条件 概率分布，对 数线形模型          | 判别模型 | 极大似然估 计，正则化的 极大似然估计 | 逻辑斯谛 损失        | 改进的迭代尺 度算法，梯度 下降，拟牛顿 法 |
| 支持向量机                 | 二类分类          | 分离超平面， 核技巧                                    | 判别模型 | 极小化正则化 合页损失，软 间隔最大化 | 合页损失             | 序列最小最优 化算法（SMO〉                |
| 提升方法                   | 二类分类          | 弱分类器的 线性组合                                    | 判别模型 | 极小化加法模 型的指数损失            | 指数损失             | 前向分步加法算法                          |
| EM算法①                    | 概率模型参 数估计 | 含隐变量概 率模型                                      |          | 极大似然估 计，极大后验 概率估计     | 对数似然 损失        | 迭代算法                                  |
| 隐马尔可夫 模型            | 标注              | 观测序列与 状态序列的 联合概率分 布模型                | 生成模型 | 狱懺估计， 极大后验概率 估计         | 对数似然 损失        | 概率计算公式， EM算法                     |
| 条件随机场                 | 标注              | 状态序列条 件下观测序 列的条件概 率分布，对数 线性模型 | 判别模型 | 极大似然估 计，正则化极 大似然估计   | 对数似然 损失        | 改进的迭代尺 度算法，梯度 下降，拟牛顿 法 |

下面对各种方法的特点及其关系进行简单的讨论.

1.适用问题

本书主要介绍监督学习方法.监督学习可以认为是学习一个模型，使它能对 给定的输入预测相应的输出.监督学习包括分类、标注、回归.本书主要考虑前 两者的学习方法.分类问题是从实例的特征向鼇到类标记的预测问题，标注问题 是从观测序列到标记序列（或状态序列）的预测问题.可以认为分类问题是标注 问题的特殊情况.分类问题中可能的预测结果是二类或多类.而标注问题中可能 的预测结果是所有的标记序列，其数目是指数级的.

感知机、ft近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、 支持向量机、提升方法是分类方法.原始的感知机、支持向量机以及提升方法是 针对二类分类的，可以将它们扩展到多类分类.隐马尔可夫模型、条件随机场是 标注方法.EM算法是含有隐变量的概率模型的一般学习算法，可以用于生成模 型的非监督学习.

感知机、/t近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直 观、方法简单、实现容易等特点.逻辑斯谛回归与最大熵模型、支持向量机、提 升方法是更复杂但更有效的分类方法，往往分类准确率更髙.隐马尔可夫模型、 条件随机场是主要的标注方法.通常条件随机场的标注准确率更髙.

2.模型

分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的 映射.它们可以写成条件概率分布p（r|x）或决策函数y=/（x）的形式.前者表 示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型.有时，模 型更直接地表示为概率模型，或者非概率模型：但有时模型兼有两种解释.

朴素贝叶斯法、隐马尔可夫模型是概率模型.感知机、*近邻法、支持向量 机、提升方法是非概率模型.而决策树、逻辑斯谛回归与最大熵模型、条件随机 场既可以看作是概率模型，又可以看作是非概率模型.

直接学习条件概率分布或决策函数r=/（A3的方法为判别方法，对 应的模型是判别模型.感知机、近邻法、决策树、逻辑斯谛回归与最大熵模型、 支持向量机、提升方法、条件随机场是判别方法.首先学习联合概率分布 从而求得条件概率分布PW）的方法是生成方法，对应的模型是生成模型.朴素 贝叶斯法、隐马尔可夫模型是生成方法.图12.1给出部分模型之间的关系。

可以用非监督学习的方法学习生成模型.具体地，应用EM算法可以学习朴 素贝叶斯模型以及隐马尔可夫模型.

决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量.感知 机、支持向量机、A近邻法的特征空间是欧氏空间（更一般地，是希尔伯特空 间).提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方 法模型的特征空间.

感知机模型是线性模型，而逻辑斯谛回归与最大熵模型、条件随机场是对数 线性模型.走近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是 非线性模型.

图12.1从生成与判别、分类与标注两个方面描述了几个统计学习方法之间的 关系.

图12.1部分棋型之间的关系

3.学习策略

在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升 方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数.3种损失函数 分别写为

[1-測 L    (12.1)

log[l + exp(-j/(jc))]    (12.2)

exp(-^f(x))    (12.3)

这3种损失函数都是0-1损失函数的上界，具有相似的形状，如图12.2所示.所以， 可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损 失函数(surrogate loss function)表示分类的损失，定义经验风险或结构风险函数， 实现二类分类学习任务.学习的策略是优化以下结构风险函数：

1 N

德(12.4)

这里，第1项为经验风险(经验损失)，第2项为正则化项，£(j，/(*))为损失函 数，J(/)为模型的复杂度，又＞0为系数.

支持向量机用么范数表示模型的复杂度.原始的逻辑斯谛回归与最大摘模型 没有正则化项，可以给它们加上矣范数正则化项.提升方法没有显式的正则化项， 通常通过早停止(early stopping)的方法达到正则化的效果.

0 5 0 5 2.1.1.0.

|      | —损失 一合页损失 一逻規薪谛损失---•••指数损失 |
| ---- | --------------------------------------------- |
|      |                                               |
|      | ::、—-------"......................           |
|      | s、， ,                                       |



-%



函数间隔:yf(x)

图12.2 0-1损失函数、合页损失函数、逻辑斯谛损失函数、指数损失函数的关系

以上二类分类的学习方法可以扩展到多类分类学习以及标注问题，比如标注 问题的条件随机场可以看作是分类问题的最大熵模型的推广.

概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率 估计.这时，学习的策略是极小化对数似然损失或极小化正则化的对数似然损 失.对数似然损失可以写成

-logP(j|x)

极大后验概率估计时，正则化项是先验概率的负对数.

决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正

则化项是决策树的复杂度.

逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然 估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则 化的逻辑斯谛损失〉.

朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后 验概率估计，但这时模型含有隐变量.

4.学习算法

统计学习的问题有了具体的形式以后，就变成了最优化问题.有时，最优化 问题比较简单，解析解存在，最优解可以由公式简单计算.但在多数情况下，最 优化问题没有解析解，需要用数值计算的方法或启发式的方法求解.

朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可 以由概率计算公式直接计算.

感知机、逻缉斯谛回归与最大嫡模型、条件随机场的学习利用梯度下降法、 拟牛顿法等.这些都是一般的无约束最优化问题的解法.

支持向量机学习，可以解凸二次规划的对偶问题.有序列最小最优化算法等 方法.

决策树学习是基于启发式算法的典型例子.可以认为特征选择、生成、剪枝 是启发式地进行正则化的极大似然估计.

提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启 发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的.

EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以 保证，但是不能保证收敛到全局最优.

支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优 化问题，全局最优解保证存在.而其他学习问题则不是凸优化问题.
