##### 附录A梯度下降法

梯度下降法(gradientdescent)或最速下降法(steepestdescent)是求解无約 束最优化问题的一种最常用的方法，有实现简单的优点.梯度下降法是迭代算 法，每一步需要求解目标函数的梯度向量.

假设/CO是R”上具有一阶连续偏导数的函数.要求解的无约束最优化问

题是

min /(x)

(A.1)



Z表示目标函数/(x)的极小点.

梯度下降法是一种迭代算法.选取适当的初值不断迭代，更新x的值， 进行目标函数的极小化，直到收敛.由于负梯度方向是使函数值下降最快的方向， 在迭代的每一步，以负梯度方向更新*的值，从而达到减少函数值的目的.

由于/(幻具有一阶连续偏导数，若第*次迭代值为，则可将/(x)在，附 近进行一阶泰勒展开：

，(x) =，(x(«) + giT(x-，))    (A.2)

这里，= g(xw) = ▽/(xW)为 /(X)在    的梯度.

求出第it+1次迭代值，+1>:

(A.3)

其中，凡是搜索方向，取负梯度方向^=-V/(jcw)，人是步长，由一维搜索确 定，即人使得

/(xw +^pj = nun ,(xw + Ip J    (A.4)

梯度下降法算法如下：

算法A.1 (梯度下降法)

输入：目标函数/⑻，梯度函数g(;t) = V/(x)，计算精度e;

输出:/(x)的极小点X*.

⑴取初始值xweR”，置是=0

(2)    计算/(/，

(3)    计算梯度g*=g(xw)，当||gt||<f时，停止迭代，令x*=xw:否则， 令几=1(^)，求々，使

##### 附录B牛顿法和拟牛顿法

牛顿法(Newton method)和拟牛顿法(quasi Newton method)也是求解无约 束最优化问题的常用方法，有收敛速度快的优点.牛顿法是迭代算法，每一步需 要求解目标函数的海赛矩阵的逆矩阵，计算比较复杂.拟牛顿法通过正定矩阵近 似海赛矩阵的逆矩阵或海赛矩阵，简化了这一计算过程.

1.牛顿法

考虑无约束最优化问题

nun /(x)    (B.1)

其中x为目标函数的极小点.

假设/CO具有二阶连续偏导数，若第A次迭代值为Xw，则可将/CO在 附近进行二阶泰勒展开：

/(x) = /(xw) + g；(x-xw )+~(x-xw)T H(xwXx-xW)    (B.2)

这里，&=•?(#) = ▽/(#)是/(X)的梯度向量在点#的值，//(#)是的 海赛矩阵(Hesse matrix)

"(x)



(B.3)

在点#的值.函数/CO有极值的必要条件是在极值点处一阶导数为0,即梯度 向量为0.特别是当丑(/，是正定矩阵时，函数/co的极值为极小值.

牛顿法利用极小点的必要条件

其中/ft=H(xw).这样，式(B.5)成为

gt+//t(x<i+,>-xw) = 0

| 因此，                                                       |      | x(t+n =xw-H；lgk | (B.8)  |
| ------------------------------------------------------------ | ---- | ---------------- | ------ |
| 或者                                                         |      | /i+I)=x(«+p*     | (B.9)  |
| 其中，                                                       |      | HkPk=~8k         | (B.10) |
| 用式(B.8)作为迭代公式的算法就是牛顿法.算法B.1 (牛顿法)输入：目标函数/C0，梯度g(x) = V/(x),海赛矩阵精度要求。 |      |                  |        |



输出：/(X)的极小点X*.

(1)    取初始点，)，置fc = o

(2)    计算=g(xw)

(3)    若||g*||<e,则停止计算，得近似解x*=xw

(4)    计算均=H(xw),并求a

Hkpk=—gt



(5)    置

(6)    置* =女 + 1,转(2).    ■

步骤(4)求凡，pk=-H^gk,要求Hf,计算比较复杂，所以有其他改进

的方法.

2.拟牛顿法的思路

在牛顿法的迭代中，需要计算海赛矩阵的逆矩阵7T1，这一计算比较复杂，考 虑用一个《阶矩阵g* =g(；vw)来近似代替//卩1二/rW、.这就是拟牛顿法的基 本想法.

先看牛顿法迭代中海赛矩阵满足的条件.首先，满足以下关系.在 式(B.6)中取* =，+1)，即得

(B.11)

记h =‘广&，式=^+1>-沪，则

yk=HkSk    (B.12)

式(B.12)或式(B.13)称为拟牛顿条件.

如果是正定的(T^1也是正定的)，那么可以保证牛顿法搜索方向几是下

降方向.这是因为搜索方向是由式(B.8)有

x = x(t) + Xpk = xw - ^H^gk    (B.14)

所以/(x)在的泰勒展开式(B.2)可以近似写成：

/(x) = /(xw)-lgtT//；1gi    (B.15)

因巧1正定，故有>0 .当2为一个充分小的正数时，总有/(x)<, 也就是说P*是下降方向.

拟牛顿法将G*作为仔*-1的近似，要求矩阵G*满足同样的条件.首先，每次 迭代矩阵是正定的.同时，满足下面的拟牛顿条件：

G^yk=5k    (B.16)

按照拟牛顿条件选择G*作为好/的近似或选择尽作为巧的近似的算法称 为拟牛顿法.

按照拟牛顿条件，在每次迭代中可以选择更新矩阵G*+1 :

GM=Gk+hGk    (B.17)

这种选择有一定的灵活性，因此有多种具体实现方法.下面介绍Broyden类拟牛 顿法.

\3. DFP ( Davidon-Fletcher-Powell)算法(DFP algorithm )

DFP算法选择Gi+1的方法是，假设每一步迭代中矩阵是由q加上两个附 加项构成的，即

| GM=Gk+Pk+Qk                         | (B.18) |
| ----------------------------------- | ------ |
| 其中巧，a是待定矩阵.这时，          |        |
| +Pkyk+Qkyl[                         | (B.19) |
| 为使满足拟牛顿条件，可使G和込满足： |        |
|                                     | (B.20) |
| Qkyk=-Gicyk                         | (B.21) |

事实上，不难找出这样的巧和込，例如取

这样就可得到矩阵(7*+1的迭代公式:



,sksi

y!Gkyk



(B.22)

(B.23)



G*+1 =Gk +



GtykyrkGk

ylGkyt



(B.24)



称为DFP算法.

可以证明，如果初始矩阵G。是正定的，则迭代过程中的每个矩阵G*都是正 定的.

DFP算法如下：

算法B.2 (DFP算法)

输入：目标函数/⑻，梯度g(;c) = V/⑻，精度要求 输出：/CO的极小点X.

(1)选定初始点xw，取为正定对称矩阵，置女=0 ⑵计算沁=g(；cw).若||私||<£，则停止计算，得近似解否则

转(3)

C3) ipk=-Gkgk

(4)    一维搜索：求4使得

,(xw +^p*) = nun/(xw +Apt)

(5)    置 x1*+1)=jcw+々A

(6)    计算g*+,=g(，+1>),若||g*+1||<£,则停止计算，得近似解Z=#+1); 否则，按式(B.23)算出(7*+1

(7)    置灸=* + 1，转(3).    ■

\4. BFGS ( Broyden-Fletcher-Goldfarb -Shanno)算法(BFGS algorithm ) BFGS算法是最流行的拟牛顿算法.

可以考虑用64逼近海赛矩阵的逆矩阵/T1，也可以考虑用巧逼近海赛矩阵 这时，相应的拟牛顿条件是

BMSk=yt    (B.25)

可以用同样的方法得到另一迭代公式.首先令

B^=Bi+Pi+Qk

考虑使Pt和込满足:



= Bksk + pksk + QA



(B.27)



pk8k=yk

QA=-BA



(B.28)

(B.29)



找出适合条件的和込，得到BFGS算法矩阵尽+|的迭代公式:



BM=Bk +

BkS肩 Bk



(B.30)

可以证明，如果初始矩阵S。是正定的，则迭代过程中的每个矩阵尽都是正 定的.

下面写出BFGS拟牛顿算法.

算法B.3 (BFGS算法)

输入：目标函数/(x), g(x) = V/(x),精度要求 输出：/(x)的极小点/.

(1)    选定初始点取5。为正定对称矩阵，置女=0

(2)    计算g*=g(xw).若||gt||<£，则停止计算，得近似解:否则 转⑶

(3)    由 Bkpk = -g* 求出

(4)    一维搜索：求&使得

\5. Broyden 类算法(Broyden’s algorithm )

我们可以从BFGS算法矩阵私的迭代公式(B.30)得到BFGS算法关于的迭 代公式.事实上，若记Q =5；', Gm =B二，那么对式(B.30)两次应用Sherman-Morrison 公式①即得

①Sherman-Morrison公式1假设Z是n阶可逆矩阵，a, v是n维向量，且j + l/vT也是可逆矩阵，则

(^+«vT)-'



称为BFGS算法关于G*的迭代公式.

由DFP算法的迭代公式(B.23)得到的记作GDn'，由BFGS算法Gt的 迭代公式(B.31)得到的G*+1记作Gotqs，它们都满足方程拟牛顿条件式，所以它 们的线性组合

Gk+l= aGDrv +(1-a)GBFGS    (B.32)

也满足拟牛顿条件式，而且是正定的.其中名1.这样就得到了一类拟牛顿 法，称为Broyden类算法.

##### 附录C拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性(Lagrange duality)将原始 问题转换为对偶问题，通过解对偶问题而得到原始问题的解.该方法应用在许多 统计学习方法中，例如，最大熵模型与支持向量机.这里简要叙述拉格朗日对偶 性的主要概念和结果.

1.原始问题

假设/(JC)，c/x),    是定义在R”上的连续可微函数.考虑约束最优化问题

因为若某个*•使约束c,⑻>0，则可令a,    若某个J•使A»0 ,则可令我

復队(X) +oo :而将其余各,外均取为0.

反地，如果x满足约束条件式(C.2)和式(C.3)，则由式(C.5)和式(C.4)可 知,40c) = /(x).因此，

..,f/(x), :v满足原始问题約束 仲)=1+00，其他

(C.7)



所以如果考虑极小化问题

min^,(x) = min max.oL(x,a,j3)    (C.8)

它是与原始最优化问题(C.1)〜(C.3)等价的，即它们有相同的解.问题 称为广义拉格朗日函数的极小极大问题.这样一来，就把原

始最优化问题表示为广义拉格朗日函数的极小极大问题.为了方便，定义原始问 题的最优值

p' =min0p(x)

称为原始问题的值.

2.对偶问题 定义

0D (a,fi) = TmaL(x,a,J3) 再考虑极大化* (a, /?) =    i(x，a,勿，即



(C11)



问题^灼称为广义拉格朗日函数的极大极小问题. 可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题:

m^.0D (a,/3) = max. min L(x，a，j3)

s.t. af >0，i = i,2，...，k

称为原始问题的对偶问题.定义对偶问题的最优值

称为对偶问题的值.

(C.9)



(C.10)



(C.12)

(C.13)

(C.14)



3.原始问题和对偶问题的关系 下面讨论原始问题和对偶问题的关系.

定理C.1若原始问题和对偶问题都有最优值，则

d. = max^minL{x,min i^a^L(x，a,奶=p.

证明由式(C.12)和式(C.5),对任意的《，彡和*，有

OD{a,p) = minZ,(x,«r,/7)^ L{x,a,fi)    maiioL(x,a,JT) = 0p(x)

即

物，加物)

由于原始问题和对偶问题均有最优值，所以，

maxo^D(a,^)^ min^p(x)



d* = maxomin£(x,a,)ff) min maxoZ(x,a,^) = p*

(C.15)

(C.16)

(C.17)

(C.18)

(C.19)



推论C.l设x*和a*,，分别是原始问题(C.1)〜(C.3)和对偶问题(C.12)〜 (C.13)的可行解，并且'=//，则Z和分别是原始问题和对偶问题的最 优解.

在某些条件下，原始问题和对偶问题的最优值相等，d'=p.这时可以用解 对偶问题替代解原始问题.下面以定理的形式叙述有关的重要结论而不予证明.

定理C.2考虑原始问题(C.1)〜(C.3)和对偶问题(C.12) - (C.13).假设函 数/(X)和c,.00是凸函数，是仿射函数；并且假设不等式约束qCc)是严格 可行的，即存在*，对所有f有c,•⑻＜0,则存在;使Z是原始问题的 解，a*,，是对偶问题的解，并且

p" =d' =L(x*,a*,y9*)    (C.20)

定理C.3对原始问题(C.1)〜(C.3)和对偶问题(C.12)〜(C.13)，假设函数 /(x)和c,(x)是凸函数，久(x)是仿射函数，并且不等式约束以幻是严格可行的， 则Z和a*,^分别是原始问题和对偶问题的解的充分必要条件是/，《•，，满足 下面的 Karush-Kuhn-Tucker (KKT)条件：

VxL(x',a,j3') = 0    (C.21)

▽aL(/,a.，,) = 0    (C.22)

▽於(x.，a.，,) = 0    (C.23)

a;c'(x” = O，f = l，U    (C.24)

cXx*)<0,    i = l,2,-,k    (C.25)

a；^0,    i=l,2,--,k    (C.26)

f>j(x') = O    7 = 1,2,-,/    (C.27)

特别指出，式(C.24)称为KKT的对偶互补条件.由此条件可知：若<>0, 则 c,(Z) = O.

奥卡姆剃刀(Occam's razor) 14

半监餐学习(semi-supervised learning) 2 贝叶斯估计(Bayesian estimation〉47，51 比特(bit) 60

边(edge) 191

标注(tagging) 20

不完全数据(incomplete-data) 157

参数空间(parameter space) 6

残差(residual) 148

测试集(test set) 14

测试数据(test data) 2, 4

测试误差(test error) 10

策略(strategy) 2

成对马尔可夫性(pairwise Markov property ) 191 词性标注(part of speech tagging) 21

代价函数(costftmction〉7，65

代理损失函数(surrogate loss function) 115，213

带符号的距离(signeddistance) 98

单元(cell) 38, 47

动态规划 < dynamic programming) 184

对偶算法(dual algorithm) 103 , 226

对偶问题(dual problem) 103，226

对数几率(log odds) 78

对数似然损失函数(log-likelihood loss fimction) 7

对数损失函数(logarithmic loss fonction) 7

对数线性模型(log linear model) 88, 196

多数表决规则(majority voting rule) 40

多项逻辑斯诸回归模型(multi-nominal logistic regression model) 79 多项式核函数(polynomial kernel function〉122

二项逻辑斯滴回归模型(binomial logistic regression model) 78

罚项(penalty term) 9，13

泛化能力(generalization ability) 11，15

泛化误差(generalization error) 15

泛化误差上界(generalization error bound) 15

非监督学习(unsupervised learning) 2

非线性支持向量机(non-linear support vector machine) 95, 1.23 分类(classification) 18 分类器(classifier) 18

分类与回妇树(classification and regression tree, CART) 67 分离超平面(separatinghypeiplane) 26 风险函数(risk fbnction) 8

改进的迭代尺度法(improved iterative scaling, US) 88, 90, 202 概率近似正确(probably approximately correct, PAC) 137 概率图模型(probabilistic graphical model) 191 概率无向图模型(probabilistic undirected graphical model) 191, 193 感知机(perceptron) 25

髙斯核函数(Gaussian kernel fimction) 122

髙斯混合模型(Gaussian mixture model) 162

根结点(root node) 63

估计误差(estimation error) 40

观测变量(observablevariable) 155

观测序列(observation sequence) 171

广义拉格朗日函数(generalized Lagrange fimction) 225 广义期望极大(generalized expectation maximization, GEM) 166 规范化因子(normalization factor) 193

过拟合(over-fitting) 9, 11

海赛矩阵(Hesse matrix) 219

函数间賜(functional margin) 97

合页损失函数(hinge loss fimction) 114

核方法(kernel method) 95

核函数(kernel fimction) 95，116

核技巧(kernel trick) 95, 115, 118

互信息(mutual information) 61

划分(partition) 41，56

回归(regression) 21

基尼指数(Gini index) 68

极大-极大算法 <maximization-maximization algoriflim) 166 极大似然估计(maximum likelihood estimation) 9 几何间隔(geometric margin) 97，98 几率(odds) 78

加法模型(additive model) 144

假设空间(hypothesis space) 2，5，6

间踊(margin) 102

监哲学习(supervised learning) 2，3

剪枝(pruning) 65

交叉验证(cross validation) 14

结点(node) 55，191

结构风险最小化(structural risk minimization, SRM) 9 解码(decoding) 174 近似误差(approximation error) 40 经验风险(empirical risk) 8

经驗风险最小化(empirical risk minimization, ERM) 8

经验炳(empirical entropy) 61

经验损失(empirical loss) 8

经检条件炳(empirical conditional entropy) 6+1

精确率(precision) 19

径向基函数(radial b郎is function) 122

局部马尔可夫性(local Markov property) 191

决策函数(decision function) 5

决策树(decision tree) 55

决策树桩(decision stump) 147

绝对损失函数(absolute loss function) 7

拉格朗日乘子(Lagrange multiplier) 103

拉格朗日对偶性(Lagrange duality) 225

拉格朗日函数(Lagrange function) 103

拉普拉斯平滑(Laplace smoothing) 51

类(class) 18

类标记(class label) 38, 47

留 _交叉验证(leave-one-out cross validation ) 15 逻辑斯缔分布(logistic distribution) 77

逻辑斯谛回妇(logistic regression) 77

马尔可夫随机场(Markov random field) 193 曼哈顿距离(Manhattan distance) 39 模型(model) 2

模型选择(model selection〉11

内部结点(internal node) 55

纳特(nat) 60

拟牛顿法(quasi Newton method) 91, 205, 219 牛顿法(Newton method) 91, 205, 219

欧氏距薄(Euclideandistance) 39

判别方法(discriminative approach) 17

判别模型(discriminative model) 18

偏置(bias) 25

平方损失函数(quadratic loss fimction) 7

评价准则(evaluation criterion) 2

朴素贝叶斯(naive Bayes) 47

朴素贝叶斯算法(n^Tve Bayes algorithm) 50

期望极大算法(EM 算法)(expectationmaximization algorithm) 155，157, 162，183

期望损失(expected loss) 8

前向分步算法(forward stagewise algorithm) 144

前向-后向算法(forward-backward algorithm) 175

潜在变量(latent variable〉155

强化学习(reinforcement learning) 2

强可学习(strongly learnable) 137

切分变量(splittingvariable) 68

切分点(splitting point) 68

全局马尔可夫性(global Markov property) 191

权值(wei^it) 25

权值向量(weight vector) 25

软间踊最大化(soft margin maximization) 95 弱可学习(weakly learaable) 137

摘(entropy) 60

生成方法(generative approach) 17 生成模型(generative model) 18 实例(instance) 4 势函数(potential function) 194 输出空间(output space) 4 输入交间(input space) 4 数据(data) 1

算法(algorithm) 2

随机梯度下降法(stochastic gradient descent) 28 损失函数(loss function) 1, 65

特异点(outlier) 108

特征函数(feature function〉82

特征空间(feature space) 4

特征向量(feature vector) 4

梯度提升(gradient boosting) 151

梯度下降法(gradientdescent) 217

提升(boosting) 137

提升树(boosting tree) 137，147

提早停止(earlystopping) 213

条件摘 < conditional entropy) 61

条件随机场(conditional random field. CRF) 191 > 194 统计机器学习(statistical machine learning) 1 统计学习(statistical learning) 1 统计学习方法(statistical learning method) 3 统计学习理论(statistical learning theory) 3 统计学习应用(application of statistical learning ) 3 凸二次规划(convex quadratic programming) 95, 100, 109 图(graph) 191

团(clique〉193

完全数据(complete-data) 157

维特比算法(Viterbi algorithm) 184, 207

文本分类(text classification) 20

误差率(error rate) 10

希尔伯特空间(Hilbert space) 118

线性分类模型(linear classification model) 25

线性分类群(linear classifier〉25

线性可分数据集(linearly separable data set) 26

线性可分支持向量机(linear support vector machine in linearly separable case) 95，106 线性链(linear chain) 191

线性链条件随机场(linear chain conditional random field) 191> 194

线性扫描(linear scan〉41

线性支持向量机(linear support vector machine) 95» 106, 109

信息増益(information gain〉59，61

信息增益比(information gain ratio) 63

序列最小最优化(sequential minimal optimization, SMO) 124

学习率(learning rate) 28

训练集(training set) 14

训练数据(training data) 2. 4

训练误差(training error) 10

验证集(validation set) 14

叶结点(leafnode) 55

因子分解(fectorization) 193

隐变量(hidden variable) 155

隐马尔可夫模型(hiddenMarkovmodel, HMM) 171

硬间隔最大化(hard margin maximization) 95, 99

有向边(directed edge) 55

余弦相似度(cosine similarity) 123

预测(prediction) 5, 18

原始问题(primal problem) 103 , 225

再生核希尔伯特空间(reproducing kernel Hilbert space, RKHS) 120 召回率(recall) 19

正定核函数(positive definite kernel function) 118

正则化(regularization) 13

正则化项(rcgularizer) 9, 13

支持向量(support vector) 102, 107

支持向量机(support vector machines I SVM) 95

指示函数(indicator function) 10

指数损失函数(exponential loss fiinction) 145

中位数(median) 41

状态序列(state sequence) 171

准确率(accuracy) 10，19

字符串核函数(string kernel function) 122

最大后验概率估计(maximum posterior probability estimation, MAP) 9

最大间隔法(maximum margin method) 100

最大熵模型(maximum entropy model) 77，80，83

最大团(maximal clique) 193

最速下降法(steepest descent) 217

最小二乘法(least squares) 22

最小二乘回归树(least squares regression tree) 69 0-1 损失函数(<M loss function) 7

AdaBoost 算法(AdaBoost algorithm) 137, 138

Baum-Welch 算法(Baum-Welch algorithm) 183

BFGS 算法(Broyden-Fletcher-Goldfarb-Shanno algorithm, BFGS algorithm) 222 Broyden 类算法(Broyden’s algorithm) 223 C4.5 算法(C4.5 algorithm) 65

DFP 算法(Davidon-Fletcher-Powell algorithm, DFP algorithm) 221

EM 算法(EM algorithm) 157, 162, 183

Z7 函数(尸fiinction) 166

Gram 矩阵(Gram matrix) 34，119

ID3 算法(ID3 algorithm) 63

Jensen 不等式(Jensen inequality) 159

fa/树(fe/tree) 41

KKT (Karush-Kuhn-Tucker)条件(KKT (Karush-Kuhn-Tucker) conditions) 125, 227 女近邻法(仑-nearest neighbor, jfc-NN) 37 L\ 范数(£! norm) 14

上2 范数(Zz norm〉14

Lp距离(Lp distance) 38

Minkowski 距离(Minkowski distance) 38

2 函数(Q function) 158

S 形曲线(sigmoid curve) 77

S"折交叉验证 < 5-fold cross validation ) 15
