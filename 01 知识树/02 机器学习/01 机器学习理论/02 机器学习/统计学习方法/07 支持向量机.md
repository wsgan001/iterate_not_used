##### 第7章支持向量机

支持向量机(support vector machines，SVM)是一种二类分类模型.它的基 本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知 机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器.支持向量机 的学习策略就是间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的问题，也等价于正则化的合页损失函数的最小化问题.支持向 量机的学习算法是求解凸二次规划的最优化算法.

支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机(linear support vector machine in linearly separable case),线性支持向量机(linear support vector machine)及非线性支持向量机(non-linear support vector machine).简单 模型是复杂模型的基础，也是复杂模型的特殊情况.当训练数据线性可分时，通 过硬间隔最大化(hard margin maximization)，学习一^^线性的分类器，即线性可 分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软 间隔最大化＜soft margin maximization),也学习一个线性的分类器，即线性支持 向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧 (kernel trick)及软间隔最大化，学习非线性支持向量机.

当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，核函数 (kernel function)表示将输入从输入空间映射到特征空间得到的特征向量之间的 内积.通过使用核函数可以学习非线性支持向量机，等价于隐式地在髙维的特征 空间中学习线性支持向量机.这样的方法称为核技巧.核方法(kernel method)是 比支持向量机更为一般的机器学习方法.

Cortes与Vapnik提出线性支持向量机，Boser、Guyon与Vapnik又引入核技 巧，提出非线性支持向量机.

本章按照上述思路介绍3类支持向量机、核函数及一种快速学习算法一序 列最小最优化算法(SMO).

###### 7.1线性可分支持向量机与硬间隔最大化 7.1.1线性可分支持向量机

考虑一个二类分类问题.假设输入空间与特征空间为两个不同的空间.输入 空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间.线性可分支 持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的 输入映射为特征空间中的特征向量.非线性支持向量机利用一个从输入空间到特 征空间的非线性映射将输入映射为特征向量.所以，输入都由输入空间转换到特 征空间，支持向量机的学习是在特征空间进行的.

假设给定一个特征空间上的训练数据集

其中，XleX = R", yiey = {+l,-l}, i = l,2,-,N, 4为第1•个特征向量，也称为 实例，只为X,的类标记，当乃=+1时，称七为正例；当只=-1时，称x,为负例， (X，x)称为样本点.再假设训练数据集是线性可分的(见定义2.2).

学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的 类.分离超平面对应于方程+ = 它由法向量w和截距决定，可用(叫幻 来表示.分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类.法 向量指向的一侧为正类，另一侧为负类.

一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确 分开.感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多 个.线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的.

定义7.1 (线性可分支持向量机)给定线性可分训练数据集，通过间隔最大 化或等价地求解相应的凸二次规划问题学习得到的分离超平面为

w*-x+b* =0    (7.1)

以及相应的分类决策函数

/(*) = sign(w. • x + 6.)    (7.2)

称为线性可分支持向量机.

考虑如图7.1所示的二维特征空间中的分类问题.图中“。”表示正例，“x ” 表示负例.训练数据集线性可分，这时有许多直线能将两类数据正确划分.线性可 分支持向量机对应着将两类数据正确划分并且间隔最大的直线，如图7.1所示.

图7J二类分类问题

间隔最大及相应的釣束最优化问题将在下面叙述.这里先介绍函数间隔和几 何间隔的概念.

7.1.2函数间隔和几何间隔

在图7.1中，有A，B，C三个点，表示3个实例，均在分离超平面的正类一 侧，预测它们的类.点J距分离超平面较远，若预测该点为正类，就比较确信预 测是正确的；点C距分离超平面较近，若预测该点为正类就不那么确信；点5介 于点J与C•之间，预测其为正类的确信度也在J与C之间.

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度.在 超平面w. x + 6 = 0确定的情况下,\wx + b\能够相对地表示点jc距离超平面的远 近.而+6的符号与类标记y的符号是否一致能够表示分类是否正确.所以 可用量y(w.x+&)来表示分类的正确性及确信度，这就是函数间隔(fimctional margin)的概念.

定义7.2(函数间隔)对于给定的训练数据集r和超平面(W，*)，定义超平 面(w,h)关于样本点(么乃)的函数间隔为

f(=j,(W.x(+6)    (7.3)

定义超平面(w，*)关于训练数据集r的函数间隔为超平面(w,h)关于r中所有样 本点(a，x)的函数间隔之最小值，即

戸=蜘    (7.4)

函数间隔可以表示分类预测的正确性及确信度.但是选择分离超平面时，只

有函数间隔还不够.因为只要成比例地改变w和6,例如将它们改为加和26,超 平面并没有改变，但函数间隔却成为原来的2倍.这一事实启示我们，可以对分离 超平面的法向量w加某些约束，如规范化，||w||=l,使得间隔是确定的.这时函 数间隔成为几何间隔(geometric margin).

图7.2给出了超平面(w，Z»)及其法向量w.点W表示某一实例七，其类标记 为只=+1.点W与超平面(w，6)的距离由线段给出，记作％.

其中，||w||为讲的么范数.这是点Z在超平面正的一侧的情形.如果点/!在超 平面负的一侧，即乃=-1，那么点与超平面的距离为

一般地，当样本点（&，只）被超平面（w，ZO正确分类时，点&与超平面（w,6）的 距离是

由这一事实导出几何间隔的概念.

定义73（几何间隔）对于给定的训练数据集7和超平面（w，*），定义超平 面（w，6）关于样本点（Xi.y,）的几何间隔为

定义超平面（w,i）关于训练数据集r的几何间隔为超平面（w.h）关于r中所 有样本点（x^）的几何间隔之最小值，即

，=挪    （7.6）

超平面（W，b）关于样本点（Xi,y/）的几何间隔一般是实例点到超平面的带符号 的距离（signed distance）,当样本点被超平面正确分类时就是实例点到超平面的 距离.

从函数间隔和几何间隔的定义（式（7.3）〜式（7.6））可知，函数间隔和几何间 隔有下面的关系：

如果||w||=l，那么函数间隔和几何间隔相等.如果超平面参数*和6成比例地改 变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变.

7.1.3间隔最大化

支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔 最大的分离超平面.对线性可分的训练数据集而言，线性可分分离超平面有无穷 多个(等价于感知机)，但是几何间隔最大的分离超平面是唯一的.这里的间隔 最大化又称为硬间隔最大化(与将要讨论的训练数据集近似线性可分时的软间隔 最大化相对应).

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着 以充分大的确信度对训练数据进行分类.也就是说，不仅将正负实例点分开，而 且对最难分的实例点(离超平面最近的点)也有足够大的确信度将它们分开.这 样的超平面应该对未知的新实例有很好的分类预测能力.

1.最大间隔分离超平面

下面考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平 面.具体地，这个问题可以表示为下面的约束最优化问题：

Z

s.t.



max

(7.9)



![img](2012.4e2a-47.jpg)



(7.10)

即我们希望最大化超平面(w,6)关于训练数据集的几何间隔y,约束条件表示的 是超平面(w, 6)关于每个训练样本点的几何间隔至少是y.

考虑几何间隔和函数间隔的关系式(7.8),可将这个问题改写为

max

w,b



IMI



(7.11)



s.t.只(w.z,+Z»)彡戶，i=l,2,-,AT    (7.12)

函数间隔f的取值并不影响最优化问题的解.事实上，假设将w和*按比例 改变为;hv和26,这时函数间隔成为Af.函数间隔的这一改变对上面最优化问 题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一 个等价的最优化问题.这样，就可以取^=1.将，=1代入上面的最优化问题，注

意到最大化;^和最小化^||w|r是等价的，于是就得到下面的线性可分支持向 M 2

量机学习的最优化问题

呼)|l|w||2

(7.13)

(7.14)



s.t.乃(w.x,+!»)—1彡 0，i = l，2,…，N

这是—个凸二次规划(convex quadratic programming)问题. 凸优化间题是指约束最优化问题

min    /(w)    C7.15)

s.t.    g,(w)<0,    (7.16)

及(w) = 0,    j = l,2,-,Z    (7.17)

其中，目标函数/(w)和约束函数g:(w)都是ir上的连续可微的凸函数，约束函 数及(w)是R”上的仿射函数®.

当目标函数/(w)是二次函数且约束函数g,(w)是仿射函数时，上述凸最优化 问题成为凸二次规划问题.

如果求出了约束最优化问题(7.13)〜(7.14)的解，那么就可以得到最 大间隔分离超平面w'x+zr =0及分类决策函数/(;t) = SignO/.;c+矿)，即线性可 分支持向量机模型.

综上所述，就有下面的线性可分支持向量机的学习算法——最大间隔法 (maximum margin method).

算法7.1 (线性可分支持向量机学习算法——最大间隔法)

输入：线性可分训练数据集r={(A,乃)：(々，；)；2)，.",(么，外)}，，其中，人£ X = W, =    i = i，2,…，N •,

输出：最大间隔分离超平面和分类决策函数.

(1)    构造并求解约束最优化问题：

s.t. y,(w.xf+ 6)-1彡0，； = 1，2,…,JV

求得最优解w*，6*.

(2)    由此得到分离超平面：

w* >x+b* =0

分类决策函数

,(x) = sign(w* .x + h*)

2.最大间隔分离超平面的存在唯一性

线性可分训练数据集的最大间隔分离超平面是存在且唯一的.

定理7.1 (最大间隔分离超平面的存在唯一性)若训练数据集r线性可分，则

可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一.

①/W称为仿射函数，如果它满足/(Jt) = a.x+A, «gR", Z>eR , xeR'

证明(1)存在性

由于训练数据集线性可分，所以算法7.1中的最体化问题(7.13)〜(7.14)—定 存在可行解.又由于目标函数有下界，所以最优化问题(7.13)〜(7.14)必有解，记 作(W).由于训练数据集中既有正类点又有负类点，所以(w,6) = (0,6)不是最 优化的可行解，因而最优解必满足w* #0 .由此得知分离超平面的存在性.

(2)唯一性

首先证明最优化问题(7.13) ~ (7.14)解中mZ的唯一性.假设问题(7.13)- (7.14) 存在两个最优解和(O2*).显然||u(|| = ||<|| = c，其中c是一个常数.令 w=w1_+^2, b=^~^-,易知(w,6)是问题(7.13)〜(7.14)的可行解，从而有

c^||w||^|||w；||+^||w； || = c

上式表明，式中的不等号可变为等号，即|| w||= j|| wf ||+|||W；||,从而有< =Aw；, |A| = 1.若义=-1，则w = 0, (w，Z»)不是问题(7.13)〜(7.14)的可行解，矛盾.因 此必有A = 1,即

心w;

由此可以把两个最优解和分别写成(w*，6f)和(w\b；).再证 A* = K •设4和 < 是集合｛*, |乃=+1｝中分别对应于和(w*,6；)使得问题的 不等式等号成立的点，< 和;< 是集合｛*,1只=-1｝中分别对应于(w*，2O和 (v/，*2*)使得问题的不等式等号成立的点，则由Z<=-+    62* =

■'全(w. •《+ w. • <)，得

A' -*2 =~[(W •«_<) +W. .(«)]

又因为

w* •x,2+l\ >l = w*«x,+6f w..x: + 6; >l = w*«^+6j

所以，w*.C<-<) = 0.同理有= 0.因此，

由=<和可知，两个最优解0«)和«，~)是相同的，解的唯一性 得证.

由问题(7.13) - (7.14)解的唯一性即得分离超平面是唯一的.

(3)分离超平面能将训练数据集中的两类点完全正确地分开.

由解满足问题的约束条件即可得知.    ■

3.支持向量和间隔边界

在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点 的实例称为支持向量(support vector).支持向量是使约束条件式(7.14)等号成立 的点，即

yi(yv-xi +6)-1 = 0

对乃=+1的正例点，支持向量在超平面

: w«x + Z> = l

上，对只=-1的负例点，支持向量在超平面

H2 : w«x + h = —1

上.如图7.3所示，在和开2上的点就是支持向量.

注意到什和平行，并且没有实例点落在它们中间.在叫与/^之间形成 一条长带，分离超平面与它们平行且位于它们中央.长带的宽度，即什与7/2之

间的距离称为间隔(margin).间隔依赖于分离超平面的法向量w，等于—.

1卜||

什和//2称为间隔边界.

在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用.如果 移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至 去掉这些点，则解是不会改变的.由于支持向量奄确定分离超平面中起着决定性 作用，所以将这种分类模型称为支持向量机.支向量的个数一般很少，所以支 持向量机由很少的“重要的”训练样本确定.

例7.1数据与例2.1相同.已知一个如图7.4所示的训练数据集，其正例点 是x:=(3,3)t, x2=(4,3)T.负例点是七=(1，1)T,试求最大间隔分离超平面.

解按照算法7.1，根据训练数据集构造约束最优化问题： min 全(wX) s.t. 3w, +3w2 +6>1

4w, +3w2 +b^l -wl-w2-b^i

求得此最优化问题的解叫=%=+，b = -2.于是最大间隔分离超平面为

Ixa)+Ixw_2 = 0 2 2

其中，*,=(3,3)7与七=(1>1)7为支持向景.    ■

7.1.4学习的对偶算法

为了求解线性可分支持向量机的最优化问题(7.13)〜(7.14),将它作为原始 最优化问题，应用拉格朗日对偶性(参阅附录C)，通过求解对偶问题(dual problem)得到原始问题(primal problem)的最优解，这就是线性可分支持向量 机的对偶算法(dual algorithm).这样做的优点，~是对偶问题往往更容易求解； 二是自然引入核函数，进而推广到非线性分类问题.

首先构建拉格朗日函数(Lagrange fiinction).为此，对每一个不等式约束(7.14) 引进拉格朗日乘子(Lagrangemultiplier) at ^0.    i    =    l,2,---,N ,定义拉格朗日函数：

1    N    N

L(yv,b,a)=-\\w\^ -^a^w    x, +b)+^a,    (7.18)

L    M    (=1

其中，》=咏，叫，…而)T为拉格朗日乘子向量•

根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：

max mui L(w,b,a)

所以，为了得到对偶问题的解，需要先求Z(w，6，a)对w，Z»的极小，再求对a的极大.

(1)求

将拉格朗日函数£(w，6，a)分别对求偏导数并令其等于0.

N

VwL(yv,b,a) = w - [    = 0

Z=1

N

ViL(w,b,a) = '^alyl=0

(7.19)

/=]

N

La^=°    (7.20)

/=i

将式(7.19)代入拉格朗日函数(7.18),并利用式(7.20),即得

£(w，6’a)+^+ga(

6)+

N

s.t. 2«,Z=0

i=l

a,^0, i=l,2,—,N

将式(7.21)的目标函数由求极大转换成求极小，就得到下面与之等价的对偶 最优化问题：

、N N    N

min    (7-22)

a *■ 1=1 7=1    (=1

N

s，t.    =0    (7.23)

i=l

«i>o，i = l,2,-,N    (7.24)

考虑原始最优化问题(7.13)〜(7.14)和对偶最优化问题(7.22) - (7.24),原 始问题满足定理C.2的条件，所以存在使V是原始问题的解，a'tp' 是对偶问题的解.这意味着求解原始问题(7.13)〜(7.14)可以转换为求解对偶问 题(7.22)〜(7.24).

对线性可分训练数据集，假设对偶最优化问题(7.22)〜(7.24)对c的解为 a'= «，》/，•••，》//，可以由a*求得原始最优化问题(7.13) ~ (7.14)对(w，6)的 解w*，y.有下面的定理.

| 定理7.2存在下标J, w*,b't           | 设a* = (ofX，".，a;)T是对偶最优化问题(7.22) ~ (7.24)的解，则 使得a; >0，并可按下式求得原始最优化问题(7.13) — (7.14)的解 |        |
| ---------------------------------- | ------------------------------------------------------------ | ------ |
|                                    | N<=1                                                         | (7.25) |
|                                    | Nb' =yJ-^«；yi{xi>xJ)/-I                                     | (7.26) |
| 证明根据定理C.3, KKT条件成立，即得 |                                                              |        |
|                                    | NW，*.，a.) = w. - XX 乃 x, = 0                              | (7.27) |

N

VA£(w*,6*,a*) =    =0

/=i

a'(yi(w* 'Xt +A*)-l) = 0, i-l,2,---,N W.x,+6.)-l>0, i = l,2,-,N a*^Q, i = 1,2,—,7V

由此得

其中至少有一个a/ > 0 (用反证法，假设a* = 0 ,由式(7.27)可知w" = 0,而w* = 0 不是原始最优化问题(7.13)〜(7.14)的解，产生矛盾)，对此y有 ^y(w* •Xj +i*)-l = 0

(7.28)



将式(7.25)代入式(7.28)并注意到y/ = 1，即得

N

»=i

由此定理可知，分离超平面可以写成

N

^a'y^x-x^+b* =0    (7.29)

i-i

分类决策函数可以写成

/(x) = sign(ga»x,)+fc.)    (7.30)

这就是说，分类决策函数只依赖于输入*和训练样本输入的内积.式(7.30)称为 线性可分支持向量机的对偶形式.

综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题(7.22)〜 (7.24)的解a*;再利用式(7.25)和式(7.26)求得原始问题的解w,b' 从而得到 分离超平面及分类决策函数.这种算法称为线性可分支持向量机的对偶学习算法， 是线性可分支持向量机学习的基本算法.

算法7.2 (线性可分支持向量机学习算法)

输入:线性可分训练集7 = {(6,71)，(^2，>>2)，".,('^，外)}，其中;^；1> = R", yte

输出：分离超平面和分类决策函数.

(1)    构造并求解约束最优化问题

Y N N    N

N

s-t. £a,y,=o

<=i

a, >0，i = l,2,—

求得最优解a* =«,a2*，…，a/)T.

(2)    计算

w’=Ea,W

i=i

并选择的一个正分量a/>0,计算

N

i=i

(3)求得分离超平面

w* +    =0

分类决策函数：

f (x) = sign(w* • x + 6*)    ■

在线性可分支持向量机中，由式(7.25)、式(7.26)可知，w*和6*只依赖于训

练数据中对应于a,*>0的样本点Cc,，Z)，而其他样本点对v/和6*没有影响.我 们将训练数据中对应于a/ >0的实例点七e R”称为支持向量.

定义7.4 (支持向量)考虑原始最优化问题(7.13) - (7.14)及对偶最优化问 题(7.22) - (7.24),将训练数据集中对应于af >0的样本点(x^y,)的实例七e R” 称为支持向量.

根据这一定义，支持向量一定在间隔边界上.由KKT互补条件可知， a».W)-l) = 0，i = \,2,-,N

对应于af>0的实例a,有

乃+ 6*)-1 = 0

或

w. • x, + 6. = ±1

即;c, 一定在间隔边界上.这里的支持向量的定义与前面给出的支持向量的定义是 —致的.

例7.2训练数据与例7.1相同.如图7.4所示，正例点是％, =(3,3)T, x2 =(4,3)T, 负例点是；c, =(1,1)T,试用算法7.2求线性可分支持向量机.

解根据所给数据，对偶问题是

I AT JV    N

/»1 !=\    1=1

=去(18<^ +25a^ +2«32 +42a,a2 -I2a,a3 -14a2o^)-c^ ~a2 -珥 s.t. a; + a2 - a3 = 0

a^O, i = 1,2,3

解这一最优化问题.将叫=叫+a2代入目标函数并记为 s(a„a2) = 4a^+ —c^ + 10c^a2 - 2^ -2a2

对叫，％求偏导数并令其为0,易知^(叫，％)在点取极值，但该点不满足 约束条件a2多0,所以最小值应在边界上达到.

当0=0时，最小值＜0,吾)=-吉：当％=0时，最小值＜去，0) = -|.于

是啦我)在q =-,a2 =0达到最小，此时％ =叫+a2 =y.

4    4

这样，巧-:^:^对应的实例点久七是支持向量.根据式(7.25)和式(7.26) 计算得

..1 wi =w2 =-

y =-2

分离超平面为

-xw+-xm-2 = 0 2 2

分类决策函数为

仲)=一(|，+(2)-2)    .

对于线性可分问题，上述线性可分支持向量机的学习(硬间隔最大化)算法是 完美的.但是，训练数据集线性可分是理想的情形.在现实问题中，训练数据集往 往是线性不可分的，即在样本中出现噪声或特异点.此时，有更一殷的学习算法.

###### 7.2线性支持向量机与软间隔最大化

7.2.1线性支持向量机

线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，因 为这时上述方法中的不等式约束并不能都成立.怎么才能将它扩展到线性不可分 问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化.

假设给定一个特征空间上的训练数据集

t=- ■    ?w)}

其中，x＜e^ = R", yt€y={+l,-l}, i = i,2，…，N，x,为第 I•个特征向量，y,为 A的类标记.再假设训练数据集不是线性可分的.通常情况是，训练数据中有一 些特异点(outlier)，将这些特异点除去后，剩下大部分的样本点组成的集合是线 性可分的.

线性不可分意味着某些样本点(久乃)不能满足函数间隔大于等于1的约束条 件(7.14).为了解决这个问题,可以对每个样本点(x,,y,)引进一个松弛变量$彡0, 使函数间隔加上松弛变量大于等于1.这样，约束条件变为

同时，对每个松弛变量支付~个代价目标函数由原来的|||w||2变成 1    N

去 l|w||2+c£$    (7.31)

厶    i=\

这里，C>0称为惩罚参数，一般由应用问题决定，C值大时对误分类的惩罚增 大，<7值小时对误分类的惩罚减小.最小化目标函数(7.31)包含两层含义:使jll叫|2

尽董小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数.

有了上面的思路，可以和训练数据集线性可分时一样来考虑训练数据集线性不

可分时的线性支持向量机学习问题.相应于硬间隔最大化，它称为软间隔最大化. 线性不可分的线性支持向量机的李习问题变成如下凸二次规划(convex

quadratic programming)问题(原始问题)：

1    N

nun    -||W||2+c£^    (7.32)

s.t.    yi(w>xl+b)^l-^i,    f=l，2，".，7V    (7.33)

$ 彡 0, i = l,2,-,N    (7.34)

原始问题(7.32)〜(7.34)是一个凸二次规划问题，因而关于(W，Z»，^)的解是存 在的.可以证明w的解是唯一的，但6的解不唯一，A的解存在于•一个区间⑴L

设问题(7.32)〜(7.34)的解是w*, y,于是可以得到分离超平面W*.x+** =0 及分类决策函数/(x) = sign(w -X + H-称这样的模型为训练样本线性不可分时 的线性支持向量机，简称为线性支持向量机.显然，线性支持向量机包含线性可 分支持向量机.由于现实中训练数据集往往是线性不可分的，线性支持向量机具 有更广的适用性.

下面给出线性支持向量机的定义.

定义7.5(线性支持向量机)对于给定的线性不可分的训练数据集，通过求 解凸二次规划问题，即软间隔最大化问题(7.32)〜(7.34)，得到的分离超平面为

w’.x + 6*=0    (7.35)

以及相应的分类决策函数

f(x) = sign(w* .x + b.)

(7.36)



称为线性支持向量机.

7J.2学习的对偶算法

| 原始问题(7.32)〜(7.34)的对偶问题是 |                          |        |
| ---------------------------------- | ------------------------ | ------ |
| min                                | N N    N/=] y«i    w     | C7.37) |
| s.t.                               | NE«;z=oi=i               | (7.38) |
|                                    | O^a^C, i = l,2,---,N     | (7.39) |
| 原始最优化问题(7.32)-              | ~ (7.34)的拉格朗日函数是 |        |



对偶问题是拉格朗日函数的极大极小问题.首先求Z(w，^，£r，/z)对的 极小，由

N

V ^(w, 6,么 a，//) = w - E «1納=o (-1

N

VtL(w,b,^,a,jU)= -2^=0

i=l

V{L(w,b,^,a,^) = C-ai -/z, =0 得

N

w = Za^jxj    (7.41)

i=l

N

Z^=°    (7.42)

(=i

Cn,=o    (7.43)

将式(7.41)〜(7.43)代入式(7.40)，得

i(w,z»^,Qr,/z)=七•*/)+§ a'

再对求a的极大，即得对偶问题：

-Xj) + ^a,    (7.44)

/-I y-l    i=l

(7.45)



C — Uf —/Zy = 0 a, >0

//,彡0，f = l，2，".，J\r



(7.46)

(7.47)

(7.48)



将对偶最优化问题(7.44)〜(7.48)进行变换：利用等式约束(7.46)消去从， 从而只留下变量q ,并将约束(7.46) - (7.48)写成



(7-49)



再将对目标函数求极大转换为求极小，于是得到对偶问题(7.37)〜(7.39).

可以通过求解对偶问题而得到原始问题的解，进而确定分离超平面和决策函

数.为此，就可以定理的形式叙述原始问题的最优解和对偶问题的最优解的关系. 定理73设a*=(«；，a2*，…，a/)T是对偶问题(7.37)〜(7.39)的一个解，若

存在eZ的一个分量<,0 < a/ < C,则原始问题(7.32)〜(7.34)的解w,b*可按 下式求得：



![img](2012.4e2a-50.jpg)



(7.50)



N

b' =yJ-Xyial,(xi.xJ')



(7.51)



证明原始问题是凸二次规划问题，解满足KKT条件.即得

N

= w. -X 心 x< =0



(7.52)



= =0

/=i



V(L{w,b',^,a,n') = C-a -/z*=0

(Ji (w. • x, + 6*) -1 +《•)= 0



(7.53)



«；=0

y,(w .x, +6*)-1+^* >0

o

/z/ > 0, i = l,2,-,N



(7.54)



由式(7.52)易知式(7.50)成立.再由式(7.53) - (7.54)可知，若存在a;，

0<a/ <C ,则乃(w*.*,+6*)-1 = 0 .由此即得式(7.51).    ■

由此定理可知，分离超平面可以写成

Af

Sa»x<) + 6.=0    (7.55)

/-I

分类决策函数可以写成

/(x) = sign(^X乃(x.x,) + d.)    (7.56)

式(7.56)为线性支持向量机的对偶形式.

综合前面的结果，有下面的算法.

算法73 (线性支持向量机学习算法)

瑜入:训练数据集r = {(x,，乃)，(&，乃)，...，(〜，知)}，其中，xleX = R", y,e

输出：分离超平面和分类决策函数.

(1)选择惩罚参数00,构造并求解凸二次规划问题

s.t.

i=i

OWC: i = l,2，".,N 求得最优解a* =«，a2*，…，a/)T .

N

(2)    计算冰*=£00^

w

选择a*的一个分量a/适合条件0<a/<C,计算

N

/=!

(3)    求得分离超平面

w*-x + b* =0

分类决策函数:

f(x} = sign(w*-x+b')    ■

步骤(2)中，对任一适合条件0 < a/ < C的a/，按式(7.51)都可求出6*，但是由 于原始问题(7.32) - (7.34)对6的解并不唯」11],所以实际计算时可以取在所有

符合条件的样本点上的平~值.

7.2.3支持向量

在线性不可分的情况下，将对偶问题(7.37)〜(7.39)的解</ =(«；，

中对应于a/>0的样本点(x,,y,)的实例x,称为支持向量(软间隔的支持向量).如 图7.5所示，这时的支持向量要比线性可分时的情况复杂一些.图中，分离超平 面由实线表示，间隔边界由虚线表示，正例点由“。”表示，负例点由“X”表示.图 中还标出了.实例x,到间隔边界的距离A.

软间隔的支持向量或者在间隔边界上，或者在间隔边界与分离超平面之 间，或者在分离超平面误分一侧.若a:<C,则$=0,支持向量'恰好落在间 隔边界上：若a:=C, 0<^<1,则分类正确，a在间隔边界与分离超平面之间： 若a;=C, 6=1，则七在分离超平面上：若<=<7, ^>1,则人位于分离超 平面误分一侧.

7.2.4合页损失函数

对于线性支持向量机学习来说，其模型为分离超平面>/.* + 6*=0及决策函 数/U) = sign(w*^ + y)i其学习策略为软间隔最大化，学习算法为凸二次规划.

线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：

玄[1 -    +h)]+ +21] w ||2    (7.57)

/=>i

目标函数的第1项是经验损失或经验风险，函数

L(y(w.x+6)) = [1 - y(w-x+(7.58) 称为合页损失函数(hinge loss function).下标“ + ”表示以下取正值的函数.

这就是说，当样本点(易，乃)被正确分类且函数间隔(确信度)乃(W&+6)大于1 时，损失是0,否则损失是1_只0巧+的，注意到在图7.5中的实例点'被正确 分类，但损失不是0.目标函数的第2项是系数为2的w的么范数，是正则化项.



定理7.4线性支持向量机原始最优化问题：

1    if

nun -||w[|2 +C^    (7.60)

s.t.    i=l,2,--,N    (7.61)

《>0, i =    (7.62)

等价于最优化问题

"51    +^Hwll2    (7.63)

证明可将最优化问题(7.63)写成问题(7.60) - (7.62).令

1-Z(w，x( + 的=会，6 > 0    (7.64)

则乃(w.x,+Z»)>l.于是w，6，f满足约束条件(7.61)〜(7.62).由式(7.64)有， [l-y,(w.x，+h)]+ =K]+ =克，所以最优化问题(7.63)可写成

N

rain +^\\w\\2

W-B i=i

若取2 = ；^,则 2C

1& 祐"W||2+C§^)

与式(7.60)等价.

反之，也可将最优化问题(7.60)〜(7.62)表示成问题(7.63).    ■

合页损失函数的图形如图7.6所示，横轴是函数间隔片州^ +幻，纵轴是损

失.由于函数形状像一个合页，故名合页损失函数.

图中还画出04损失函数，可以认为它是二类分类问题的真正的损失函数，而

合页损失函数是0-1损失函数的上界.由于0-1损失函数不是连续可导的，直接 优化由其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失 函数的上界（合页损失函数）构成的目标函数.这时的上界损失函数又称为代理 损失函数（surrogate loss function）.

图7.6中虚线显示的是感知机的损失函数［乃（《^+ +6）］+.这时,当样本点（Xl，yi） 被正确分类时，损失是0,否则损失是-相比之下，合页损失函数 不仅要分类正确，而且确信度足够髙时损失才是0.也就是说，合页损失函数对 学习有更髙的要求.

###### 7.3非线性支持向量机与核函数

对解线性分类问题，线性分类支持向釐机是一种非常有效的方法.但是，有 时分类问题是非线性的，这时可以使用非线性支持向量机.本节叙述非线性支持 向量机，其主要特点是利用核技巧（kerneltrick）.为此，先要介绍核技巧.核技 巧不仅应用于支持向量机，而且应用于其他统计学习问题.

7.3.1龈巧

1.非线性分类问题

非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题.先看 —个例子：如7.7左图，是一个分类问题，图中“•”表示正实例点，“X”表示负 实例点.由图可见，无法用直线（线性模型）将正负实例正确分开，但可以用一 条楠圆曲线（非线性模型）将它们正确分开.

一般来说，对给定的一个训练数据集7 =肛，乃），（&，八），…，（~山）}，其中， 实例X属于输入空阒，xteX = R",对应的标记有两类Zey = {-1，+1}, f = l, 1，…，N.如果能用R”中的一•个超曲面将正负例正确分开，则称这个问题为非线 性可分问题.

非线性问题往往不好求解，所以希望能用解线性分类问题的方法解决这个问 题.所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解 变换后的线性问题的方法求解原来的非线性问题.对图7.7所示的例子，通过变 换，将左图中椭圆变换成右图中的直线，将非线性分类问题变换为线性分类问题.

设原空间为足cR\x =    e 新空间为2cR2, z =(2(1),/2))T€ 2：,

定义从原空间到新空间的变换(映射)：

z = (^) = ((x(1>)2，(x(2))2)T

经过变换z=^(x),原空间AfcR2变换为新空间2cR2,原空间中的点相应地 变换为新空间中的点，原空间中的椭圆

w1(x°))2+w2(x(2>)2+h = O

变换成为新空间中的直线

.w,z(,) +w2zm +6 = 0

在变换后的新空间里，直线w/0 +    + 6 = 0可以将变换后的正负实例点正确

分开.这样，原空间的非线性可分问题就变成了新空间的线性可分问题.

上面的例于说明，用线性分类方法求解非线性分类问题分为两步：首先使用 一个变换将原空间的数据映射到新空间：然后在新空间里用线性分类学习方法从 训练数据中学习分类模型.核技巧就属于这样的方法.

核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间R”或离散集合)对应于一个特征空间(希尔伯特空间W),使得在输 入空间R”中的超曲面模型对应于特征空间W中的超平面模型(支持向量机).这 样，分类间题的学习任务通过在特征空间中求解线性支持向量机就可以完成.

2.核函数的定义

定义7.6 (核函数)设A•是输入空间(欧氏空间R"的子集或离散集合)，又

设W为特征空间(希尔伯特空间)，如果存在一个从A•到W的映射

(7.65)

使得对所有函数尤Cc，z)满足条件

尺(*，Z) =舛xM⑺    (7.66)

则称尺(JC，Z)为核函数，扒X)为映射函数，式中诊00 •舛Z)为舛JC)和扒2)的内积.

核技巧的想法是，在学习与预测中只定义核函数尤(X，Z)，而不显式地定义映 射函数吵.通常，直接计算尺UZ)比较容易，而通过4>(X)和多⑺计算A：Cc，z)并 不容易.注意，0是输入空间R”到特征空间打的映射，特征空间W—般是髙维 的，甚至是无穷维的.可以看到，对于给定的核A：(jc,z),特征空间开和映射函 数岁的取法并不唯~，可以取不同的特征空间，即便是在同~特征空间里也可以 取不同的映射.

下面举一个简单的例子来说明核函数和映射函数的关系.

例73假设输入空间是R\核函数是尺(x,幻= (k)2,试找出其相关的特 征空间W和映射^x): R2 — W .

解取特征空间 W = R3,记jc = (;cw,x(2>)t，2 = (?'),z(«)t,由于 (x.z)2    +x(2)?2))2 =(x(,)z(,))2 +2xmzwxwzw +(x(2)?2))2

所以可以取映射

彻= ((x(V,万Aw,(x(2>)2)T

容易验证於(x) •卢⑺=(x • 2)2 = K(x,z).

仍取W = R3以及

4>(x) = ^((x0))2 - (x(2))2,2x(,)x(2),(x(,))2 +(x(2>)2)T

同样有岁(x)• ^(z) = (x«z)2 =K(x,z).

还可以取7< = 114和

於(x) = ((xw)\xmxm,xwxi2\(xm)2y    ■

3.核技巧在支持向量机中的应用

我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数 (分离超平面)都只涉及输入实例与实例之间的内积.在对偶问题的目标函数 (7.37)中的内积可以用核函数【(6七)=<^)•列来代替.此时对偶问题 的目标函数成为

1 N If    N

『⑻=奶尺(七，〜)_ £a'    (7-67)

1=1 y-i    i=i

同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为

/⑻=sign〔X    ) •咐)+ 6*^ = sign^ a'y^x,, x) + 6*)    (7.68)

这等价于经过映射函数0将原来的输入空间#换到一•个新的特征空间，将输 入空间中的内积&变换为特征空间中的内积叭x；｝.叭X)，在新的特征空间里 从训练样本中学习线性支持向量机.当映射函数是非线性函数时，学习到的含有 核函数的支持向量机是非线性分类模型.

也就是说，在核函数7C0r，z)给定的条件下，可以利用解线性分类问题的方法 求解非线性分类问题的支持向量机.学习是隐式地在特征空间进行的，不需要显 式地定义特征空间和映射函数.这样的技巧称为核技巧，它是巧妙地利用线性分 类学习方法与核函数解决非线性问题的技术.在实际应用中，往往依赖领域知识 直接选择核函数，核函数选择的有效性需要通过实验验证.

73.2膝核

已知映射函数诊，可以通过穴;C)和夕00的内积求得核函数K(x,z).不用构造 映射诊co能否直接判断一个给定的函数尤什，幻是不是核函数？或者说，函数 尺(X，Z)满足什么条件才能成为核函数？

本节叙述正定核的充要条件.通常所说的核函数就是正定核函数(positive definite kernel function).为证明此定理先介绍有关的预备知识.

假设K(x,z)是定义在Yx Y上的对称函数，并且对任意的x,,x2,-,xMe A*. AXxj)关于jq，x2，…，a：„的Gram矩阵是半正定的.可以依据函数尤(x，z)，构成一 个希尔伯特空间(Hilbert space),其步骤是：首先定义映射吵并构成向量空间＜S; 然后在＜5上定义内积构成内积空间；最后将＜S完备化构成希尔伯特空间.

1.定义映射，构成向量空间＜5

先定义映射

(7.69)

根据这一映射，对任意叫eR, f = l,2,-,m,定义线性组合

=    (7.70)

Ml

考虑由线性组合为元素的集合＜S.由于集合5对加法和数乘运算是封闭的，所以

•S构成一个向量空间.

2.在《5上定义内积，使其成为内积空间 在5上定义一个运算*:对任意/，ge«S,

| ，(.)=£掛，太,)(=i                                         | (7.71) |
| ---------------------------------------------------------- | ------ |
| 於)=i>A(.，zy)'•1                                          | (7.72) |
| 定义运算*                                                  |        |
|                                                            | (7-73) |
| 证明运算*是空间5的内积.为此要证：(1) (cf)*g = c(J*g) , ceR | (7.74) |
| (2) (f+g~)*h = f *h + g*h , he. S                          | (7.75) |
| (3) f*g = g*f                                              | (7.76) |
| ⑷ /*/>0,                                                   | (7.77) |
| /*/=o»/=o                                                  | (7.78) |

其中，(1)〜(3)由式(7.70)〜式(7.72)及K(x,z)的对称性容易得到.现证⑷ 之式(7.77) •由式(7.70)及式(7.73)可得：

<./=!

由Gram矩阵的半正定性知上式右端非负，即/*/>0.

再证(4)之式(7.78).充分性显然.为证必要性，首先证明不等式：

l/*g|2^(/*/)(g*g)    (7.79)

设/，ge<S, <ieR,贝!J/+/lge5, -了是，

(，+知)*(，+ Ag)>0

/*/ + 2A(/ *g') + A\g*g)^0 其左端是A的二次三项式，非负，其判别式小于等于0，即

于是式(7.79)得证.现证若/*/ = 0,贝IJ/ = 0.事实上，若

/(•) = ^＞尺(4)

/=1

则按运算*的定义式(7.73),对任意的;ce AS有

尺(• 4 * /=玄=/CO

i»l

于是，

|/(x)|2=|/c(.,x)*/|2    (7.80)

由式<7.79)和式(7.77)有

I 夂(•，*) * /12< (尺(•，为 * 尺(•，x))(/ * /) = K{x,x){f^f) 由式(7.80)有

此式表明，当/*/ = 0时，对任意的*都有|/ooi=o.

至此，证明了 *为向量空间＜5的内积.赋予内积的向量空闾为内积空间.因

此5是一个内积空间.既然*为《5的内积运算，那么仍然用.表示，即若 /(.)=笔》人(•，*)，客(.)=土#，('，力)

则

f.g = hLa々jK(xt，z)    (7-81)

(=i y=i

3.将内积空间《S完备化为希尔伯特空间

现在将内积空间《S完备化.由式(7.81)定义的内积可以得到范数

ii/ii=77v    (7.82)

因此，＜S是一个赋范向量空间.根据泛函分析理论,对于不完备的赋范向量空间5， 一定可以使之完备化，得到完备的赋范向量空间—个内积空间，当作为一个 赋范向量空间是完备的时候，就是希尔伯特空间.这样，就得到了希尔伯特空 间

这一希尔伯特空间称为再生核希尔伯特空间(reproducing kernel Hilbert space, RKHS).这是由于核犮具有再生性，即满足

X(.,x)./ = /(x)    (7.83)

及

K(.,x).^(.,z) = ^(x,z)    (7.84)

称为再生核.

4.正定核的充要条件

定理7.5(正定核的充要条件)设尺:；^义->11是对称函数，则夂Oc，2)为正 定核函数的充要条件是对任意易eY, / = 1,2,A；(x，Z)对应的Gram矩阵：

尤= l>(W)k    (7.85)

是半正定矩阵.

证明必要性.由于尤Cr，z)是;VxY上的正定核，所以存在从;V到希尔伯特 空间W的映射於，使得

K{x，z、= ♦⑻.叭 z、

于是，对任意…，•¥„，构造2C(x,z)关于x,，x2,…人的Gram矩阵

[〜]咖=[A-(xj>xy)]mxra

对任意cpC2，…,c„eR，有

Z cicjK(xi>xj)=iicicj(^(xi)'^.xj))

\>j=i    i,j-i

=〔Ec>(x<)).(Ee/Kxy))=|^^(七)| 卜0

表明尺Cr，z)关于jfpXWm的Gram矩阵是半正定的.

充分性.已知对称函数尺卜^对任意;^，〜…，;^^"，A?(x,z)关于XpA，…,x„

的Gram矩阵是半正定的.根据前面的结果，对给定的7C(;c，Z)，可以构造从;T到 某个希尔伯特空间打的映射：

(7.86)

由式(7.83)可知，

[(•，x).y=，(x)

并且

K{.tx).K{.,z) = K{x,z)

由式(7.86)即得

K(x,z) = ^(x)-^(z)

表明尺(x，z)是Afxi上的核函数.    ■

定理给出了正定核的充要条件，因此可以作为正定核，即核函数的另一定义.

定义7.7(正定核的等价定义)设;VcR',尺(jc，z)是定义在;上的对称 函数，如果对任意X, e AS i = l,2,-,m,    对应的Gram矩阵

尺=［卯w)L,    (7'87)

是半正定矩阵，则称尤是正定核.

这一定义在构造核函数时很有用.但对于一个具体函数7C0c，Z)来说，检验它 是否为正定核函数并不容易，因为要求对任意有限输入集｛Xp%，…，验证尺对 应的Gram矩阵是否为半正定的.在实际间题中往往应用已有的核函数.另外，

由Mercer定理可以得到Mercer核(Mercer Kernel)［11］，正定核比Mercer核更具 一般性.下面介绍一些常用的核函数.

7.3.3常用核函数

1.多项式核函数(polynomial kernel function )

K(x,2)= (x-z + l)p    (7.88)

对应的支持向量机是一个p次多项式分类器.在此情形下，分类决策函数成为

/(x) = sign〔芸 a:乃(x"* + l)p +Z>*1    (7.89)

2.高斯核函数(Gaussian kernel function )

[(x,z) = exp| -



(7.90)

对应的支持向量机是高斯径向基函数(radial basis fiinction)分类器.在此情形下, 分类决策函数成为

/(•«) = sign



+b*



(7.91)



3.字符串核函数(string kernel function)

核函数不仅可以定义在欧氏空间上,还可以定义在离散数据的集合上.比如， 字符串核是定义在字符串集合上的核函数.字符串核函数在文本分类、信息检索、 生物信息学等方面都有应用.

考虑一个有限字符表字符串S是从中取出的有限个字符的序列，包括 空字符串.字符串s的长度用|表示，它的元素记作冲>(2)…刈叫).两个字符 串S和r的连接记作对.所有长度为《的字符串的集合记作工”，所有字符串的集 合记作 =    .

考虑字符串s的子串《•给定一个指标序列f = (H-，k), 1=^ <&<••• <k j|» s 的子串定义为m =s(/) = J(QX4),,，S(^|)» 其长度记作/(o = L -i, +1.如 果f是连续的，则/(O=|m|;否则，/(/)>|«|.

假设<S是长度大于或等于《字符串的集合，s是《$的元素.现在建立字符串 集合5到特征空间的映射氏Cs).'表示定义在2："上的实数空间，其 每一维对应一个字符串《e ，映射甙(s)将字符串s对应于空间及"'的一个向量， 其在《维上的取值为

[氏(吼=[f    (7.92)

这里，O<K1是一个衰减参数，表示字符串f的长度，求和在s中所有与M相 同的子串上进行.

例如，假设2•为英文字符集，n为3, «S为长度大于或等于3的字符串的集合. 考虑将字符集映射到特*征空间开3.好3的一维对应于字符串osrf.这时，字 符串“Nasdaq”与“lass das”在这一•维上的值分别是[冉(Nasdaq)^ =乂3和 [^(lassndas)](Brf=22s (□为空格).在第1个字符串里，是连续的子串.在 第2个字符串里，ost/是长度为5的不连续子串，共出现2次.

两个字符串s和f上的字符串核函数是基于映射么的特征空间中的内积：

k„(s,t)= X [^(5)]b[^(0L = Z Z    (7.93)

xer1    ueZ" (lj):j(i)=z(»=u

字符串核函数夂(^0给出了字符串*和《中长度等于》的所有子串组成的特征向 量的余弦相似度(cosine similarity).直观上，两个字符串相同的子串越多，它们 就越相似，字符串核函数的值就越大.字符串核函数可以由动态规划快速地计算.

73.4非线性支持向量分类机

如上所述，利用核技巧，可以将线性分类的学习方法应用到非线性分类问题 中去.将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶 形式中的内积换成核函数.

定义7.8 (非线性支持向量机)从非线性分类训绣集，通过核函数与软间隔 最大化，或凸二次规划(7.95)〜(7.97),学习得到的分类决策函数

/CO = sign〔^X>^(x, 〜) + *•)    (7.94)

称为非线性支持向量，尺Ct，z)是正定核函数.

下面叙述非线性支持向量机学习算法.

算法7.4 (非线性支持向量机学习算法)

输入:训练数据集『：你，^)，^，;^)，…，^，：^)｝，其中七6无=『，只€ y = ｛-l，+l｝，f = l,2,"•，沁

输出：分类决策函数.

(1)    选取适当的核函数尺Oc，z)和适当的参数C,构造并求解最优化问题

]N    N    N

呼    ~Xa>    (7-95)

1-1 >1    W

N

s-t- £a,z =°    (7.96)

i=l

O^a^C, j = 1,2,…，TV    (7.97)

求得最优解》• =(«•••，</.

(2)    选择a*的一个正分量0<«；<C,计算

b' =yJ-^a；ylK(xl-xJ')

i=l

(3)    构造决策函数：

/(x) = sign    a； y.K(x • x,) + & J    ■

当尺Uz)是正定核函数时，问题(7.95)〜(7.97)是凸二次规划问题，解是存在的.



###### 7.4序列最小最优化算法

本节讨论支持向量机学习的实现问题.我们知道，支持向量机的学习问题可 以形式化为求解凸二次规划问题.这样的凸二次规划问题具有全局最优解，并且 有许多最优化算法可以用于这一问题的求解.但是当训练样本容量很大时，这些 算法往往变得非常低效，以致无法使用.所以，如何髙效地实现支持向量机学习 就成为一个重要的问题.目前人们已提出许多快速实现算法.本节讲述其中的序 列最小最优化(sequential minimal optimization, SMO)算法，这种算法 1998 年 由Platt提出.

SMO算法要解如下凸二次规划的对偶问题：

1 N N    N

min    恥尺(七，~) —(7-98)

°    M 1=1    1=1

N

s.t. ^0!^,=0    (7.99)

i=i

O^at^C, ，= 1,2，…，2V    (7.100)

在这个问题中，变量是拉格朗日乘子，一个变量对应于一个样本点(xnx);变 量的总数等于训练样本容量TV .

SMO算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此 最优化问题的KKT条件(Karush-Kuhn-Tucker conditions),那么这个最优化问题 的解就得到了.因为KKT条件是该最优化问题的充分必要条件.否则，选择两 个变量，固定其他变量，针对这两个变量构建一个二次规划问题.这个二次规划 问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始 二次规划问题的目标函数值变得更小.重要的是，这时子问题可以通过解析方法 求解，这样就可以大大提髙整个算法的计算速度.子问题有两个变量，一个是违 反KKT条件最严重的那~个，另一个由约束条件自动确定.如此，SMO算法将 原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的.

注意，子问题的两个变量中只有一个是自由变量.假设叫，叫为两个变量， 珥，叫,…，％固定，那么由等式约束(7.99)可知

N

i=2

如果a2确定，那么％也随之确定.所以子问题中同时更新两个变量.

整个SMO算法包括两个部分：求解两个变量二次规划的解析方法和选择变

量的启发式方法.

7.4.1两个变量二次规划的求解方法

不失一般性，假设选择的两个变量是《i，a2,其他变量《// = 3,4,…，JV)是固 定的.于是SMO的最优化问题(7.98)〜(7.100)的子问题可以写成：

^(Qf1»«2)= 7A：n^    +yly2Kna1a2

2 2

ff    N

-X«i+«2)+ y,a^y>a<Kn +    (7.101)

s.t. a,y,+a2y2=-艺糾=f    (7.102)

i»3

i = l,2    (7.103)

其中，2^=K(xp'),f,J = l，2,…，TV, f是常数，目标函数式(7.101)中省略了不 含叫，《2的常数项.

为了求解两个变量的二次规划问题(7.101)〜(7.103)，首先分析约束条件，然 后在此约束条件下求极小.

由于只有两个变量(Ol，a2)，约束可以用二维空间中的图形表示(如图7.8 所示).

C(2=C

a^=0

y2^aj-a2=k





yi=y2^a\+a2=k 图7.8二变董优化问题图示

不等式约束(7.103)使得(叫，《2)在盒子[0，C]x[0，C]内，等式约束(7.102)使 («l,a2)在平行于盒子[0，C]x[0，C]的对角线的直线上.因此要求的是目标函数在 一条平行于对角线的线段上的最优值.这使得两个变量的最优化问题成为实质上 的单变量的最优化问题，不妨考虑为变量《2的最优化问题.

假设问题(7.101)〜(7.103)的初始可行解为最优解为并 且假设在沿着约束方向未经剪辑时叫的最优解为

由于<"需满足不等式约束(7.103),所以最优值《厂的取值范围必须满足

条件

L^ce^^H

其中，I与"是所在的对角线段端点的界.如果只*>«2 (如图7.8左图所 示)，则

L = maK(0,a^-o^u), H = min(C,C+c^u-c^u)

如果Z=y2 (如图7.8右图所示)，则

L = maxCO.flf1* +^-C), H = min(C，o^ + ofu)

下面，首先求沿着约束方向未经剪辑即未考虑不等式约束(7.103)时叫的最

| 优解cC■咖；           | 然后再求剪辑后《2的解我们用定理来叙述这个结果.为了叙         |         |
| --------------------- | ------------------------------------------------------------ | ------- |
| 述简单，记            | N1-1                                                         | (7.104) |
| 令                    | Ei =g(x,)-y.    +    ，i = l，2                              | (7.105) |
| 当i = l，2时，定理7.6 | £,为函数gCO对输入的预测值与真实输出x之差.最优化问题(7.101)〜(7.103)沿着约束方向未经剪辑时的解是 |         |
|                       | ^new•脚 _^ild \| ^2(^1 _五2)                                 | (7.106) |
| 其中，                | rj=Kn+K22-2Kl2= ^(x,)- <P(x2)\|\|2                           | (7.107) |

少(X)是输入空间到特征空间的映射，£；，z = l,2 ,由式(7.105)给出.

经剪辑后》2的解是

H,    ce^w-'aK>H

(=^單，(7.108) L,    《嘛< Z,

由《^"求得<*是

«T =< +yty2« ~ar)    (7.109)

证明引进记号

vi =    ) = g(x() - ^ajyjK^Xj )-b, / = 1,2

目标函数可写成

ffr(al,a2)=~Klta^ +^^22c^ +y\y2Kno^a2



-(«i+«2) + W«i + y2v2a2 由《^1=?-«^2及少/=1，可将《,表示为 代入式(7.110)，得到只是《2的函数的目标函数：

(7.110)



^oc2)=^Ku(g-a2y2)2 +-^(^ +y2Kn^-a2y2)a1

_(d 少2)乃 ~«2 +vt(.s-a2y2')+y2v2a2

对％求导数

dw    v

= Kxxaz + Kna2- 2Kna2 -^ii^2 + Kn^2 + Zy2-l-v,y2+ y2v2

令其为0,得到

(尺 11 +尺22    = )^2(^2 ~~}\ +S•尺U -5^12+1 ~^2)

=>*2卜-yt+$Kn -$Kl2 +〔跑)-芬jyjr/S" -* -〔g⑷-Z w、- J

将代入，得到

(Kn+K22    =y2((/Tn +Kn-2Kn)o^y2+y2-yi +g(x,)-g(x2))

= (^, +Ka-2Ki2»2(E、-E2)

将?代入，于是得到

0^-^=0^ +



y状-e2)



要使其满足不等式约束必须将其限制在区间［z，/f］内，从而得到<«的表达 式(7.108).由等式约束(7.102),得到的表达式(7.109).于是得到最优化问 题(7.101)〜(7.103)的解(ar’O.    ■

7.4.2变量的选择方法

SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT 条件的.

1.第1个变量的选择

SMO称选择第1个变量的过程为外层循环.外层循环在训练样本中选取违 反KKT条件最严重的样本点，并将其对应的变量作为第1个变量.具体地，检 验训练样本点(xf,Z.)是否满足KKT条件，即

a, =0<=>^(x()^l

0<ai<C<^ylg(.xl) = l



(7.112)



(7.113)



N

其中，8(Xi) = ajyjK(x,,Xj) + b.

该检验是在f范围内进行的.在检验过程中，外层循环首先遍历所有满足条 件0<a,<C的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT 条件.如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否 满足KKT条件.

\2.    第2个变量的选择

SMO称选择第2个变量的过程为内层循环.假设在外层循环中已经找到第1 个变量叫，现在要在内层循环中找第2个变量第2个变量选择的标准是希望 能使《2有足够大的变化.

由式(7.106)和式(7.108)可知，是依赖于|马-£2|的，为了加快计算速 度，一种简单的做法是选择叫，使其对应的|£,-£2|最大.因为o,已定，乓也确 定了.如果£,是正的，那么选择最小的乓作为五2;如果尽是负的，那么选择最 大的作为£2.为了节省计算时间，将所有£，值保存在一个列表中.

在特殊情况下，如果内层循环通过以上方法选择的％不能使目标函数有足够 的下降，那么采用以下启发式规则继续选择》2.遍历在间隔边界上的支持向量点， 依次将其对应的变量作为叫试用，直到目标函数有足够的下降.若找不到合适的 «2,那么遍历训练数据集；若仍找不到合适的则放弃第1个O,,再通过外层 循环寻求另外的

\3.    计算阈值6和差值乓

在每次完成两个变量的优化后，都要重新计算阈值当0<<*<(7时，由 KKT条件(7.112)可知：

N

Xaiy<Kn+b=yi

(-1

于是，

N

bT =y^a,yiKn-«r^n-o^y2K2X    (7.114)

i=3

由尽的定义式(7.105)有

式(7.114)的前两项可写成：

J, -X即A =-耳 +<\Kn +o^y2K2l +bM

代入式(7.114),可得

### .ir=-五,-城-o-施(《r    (7.H5)

同样，如果0<«厂<(7,那么，

br =-e2 -y^ar-o-^(ar-o+^old    (7.116)

如果<*，<*同时满足条件0<«T<C，i = l,2,那么6^=拉™.如果 oT.C*是0或者C,那么Zf"■和巧以及它们之间的数都是符合KKT条件的阈 值，这时选择它们的中点作为6'

在每次完成两个变量的优化之后，还必须更新对应的乓值，并将它们保存在 列表中.乓值的更新要用到值，以及所有支持向量对应的a,:

Er =Yy戶J叫，屮『-y：    (7.117)

### s

其中，S是所有支持向量'的集合.

7.4.3 SMO 算法

算法7.5 (SMO算法)

输入：训练数据集r =似,艽)，(4，y2)，•“，(〜，外)}，其中，xleX = R", y.e y = {-l，+l}，i = K2，…，N ,精度e;

输出：近似解冷.

(1) ^-* = 0；

(2)    选取优化变量解析求解两个变量的最优化问题(7.101)〜 (7.103)，求得最优解《^+1),£^+1〉，更新为

(3)    若在精度£范围内满足停机条件

N

Za^=°

i=l

i =、2,…,N

\>1，{^|«;.=0} yt -g(x,)=• = 1， {xjo<a；. <c}

.彡 1， {x< \at=C\

其中，

N

g(x,) = ^a^jK^x^x^+b y=i

则转(4):否则令Ar = * + 1,转(2);

(4)取汝= #+1).    ■

###### 本章概要

\1.    支持向量机最简单的情况是线性可分支持向量机，或硬间隔支持向量 机.构建它的条件是训练数据线性可分.其学习策略是最大间隔法.可以表示为 凸二次规划问题，其原始最优化问题为

s.t. y,(w>xi +Z»)-1^0, i = l,2,---,N 求得最优化问题的解为w*, h*,得到线性可分支持向量机，分离超平面是

w* • x+6* = 0

分类决策函数是

/(x) = sign(w* ♦x+ft*)

最大间隔法中，函数间隔与几何间隔是重要的概念.

线性可分支持向量机的最优解存在且唯一.位于间隔边界上的实例点为支持 向量.最优分离超平面由支持向量完全决定.

二次规划问题的对偶问题是

J N N    N

•气)

(=i j=i    i=i

N

St. ^0^=0 i=i

a,彡 0，i = l，2,…，N

通常，通过求解对偶问题学习线性可分支持向量机，即首先求解对偶问题的 最优值a*,然后求最优值和y,得出分离超平面和分类决策函数.

\2.    现实中训练数据是线性可分的情形较少，训练数据往往是近似线性可分 的，这时使用线性支持向量机，或软间隔支持向量机.线性支持向量机是最基本 的支持向量机.

对于噪声或例外，通过引入松弛变量会，使其“可分”，得到线性支持向量 机学习的凸二次规划问题，其原始最优化问题是

1    , N

s.t.    +6)彡 1-$，i = l,2,---,N

奕彡0， i = l,2,-,N

求解原始最优化问题的解得到线性支持向量机，其分离超平面为 w .x+y=o

分类决策函数为

/(x) = sign(w*-x + b*)

线性可分支持向量机的解v/唯一但y不唯一.

对偶问题是

1 N N    N

呼 2^Jia，ajy，yj(Xi'X^~^a，

N

s.t. 2a^=()

(=i

/ = 1,2,-,JV

线性支持向量机的对偶学习算法，首先求解对偶问题得到最优解a*,然后求 原始问题最优解V和ZT,得出分离超平面和分类决策函数.

对偶问题的解a*中满足af >0的实例点$称为支持向量.支持向量可在间隔 边界上，也可在间隔边界与分离超平面之间，或者在分离超平面误分一侧.最优 分离超平面由支持向量完全决定.

线性支持向量机学习等价于最小化二阶范数正则化的合页函数

N

艺[l-X(w.x,+6)]+ +A||w||2

<=i

3.非线性支持向量机

对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个髙 维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机.由于在 线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例与实 例之间的内积，所以不需要显式地指定非线性变换，而是用核函数来替换当中的 内积.核函数表示，通过一个非线性转换后的两个实例间的内积.具体地，K{x,z） 是一个核函数，或正定核，意味着存在一个从输入空间/V到特征空间W的映射

对任意x，ze；t，有

对称函数尺（x，z）为正定核的充要条件如下：对任意i =    任意正

整数m,对称函数对应的Gram矩阵是半正定的.

所以，在线性支持向量机学习的对偶问题中，用核函数7C（x，z）替代内积，求 解得到的就是非线性支持向量机

/（x） = signf^ar*y（A：（x,x（） +    |

\4. SMO算法

SMO算法是支持向量机学习的一种快速算法，其特点是不断地将原二次规 划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到 所有变量满足KKT条件为止.这样通过启发式的方法得到原二次规划问题的最 优解.因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数 很多，但在总体上还是髙效的.

###### 继续阅读

线性支持向量机（软间隔）由Cortes与Vapnik提出同时，Boser, Guyon与 Vapnik又引入核技巧，提出非线性支持向量机［2］. Drucker等人将其扩展到支持 向量回归Pl. Vapnik Vladimir在他的统计学习理论w—书中对支持向量机的泛化 能力进行了论述.

Platt提出了支持向量机的快速学习算法SMO151, Joachims实现的SVM Light, 以及Chang与Lin实现的LIBSVM软件包被广泛使用.®

原始的支持向量机是二类分类模型，又被推广到多类分类支持向量机以 及用于结构预测的结构支持向量机［81.

关于支持向量机的文献很多.支持向量机的介绍可参照文献［9〜12］.核方法 被认为是比支持向量机更具一般性的机器学习方法.核方法的介绍可参考文献 [13 〜15].

习 题

1.1比较感知机的对偶形式与线性可分支持向量机的对偶形式.

1.2 已知正例点X, =(1，2)T, x2 =(2,3)t, x3 =(3,3)t,负例点x4 =(2,1)t, x, =(3,2)t, 试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔 边界及支持向量.

1.3线性支持向量机还可以定义为以下形式，

J    N

恕 i1卜1|2+C§6

s.t.乃(vv.x,+6)彡1-6， f = l，2，."，+_Y 会＞0，i = \,2,--,N

试求其对偶形式.

1.4证明内积的正整数幕函数：

K(x,z) = (x-z)p 是正定核函数，这里p是正整数，x,zeR".

###### 参考文献

[1]    Cortes C, Vapnik V. Support-vector networks. Machine Learning, 1995,20

[2]    BoserBE, GuyonIM, V即nik VN. A training algorithm for optimal margin classifiers. In: Haussler D, ed. Proc of the 5th Annual ACM Workshop on COLT. Pittsburgh, PA, 1992, 144-152

[3]    Drucker H, Burges CJC, Kaufman L, Smola A, Vapnik V. Support vector regression machines. In: Advances in Neural Information Processing Systems 9, NIPS 1996. MIT Press, 155-161

[4]    Vapnik Vladimir N. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag, 1995 (中译本：张学工，译.统计学习理论的本质.北京：淸华大学出版社，2000)

[5]    Platt JC. Fast training of support vector machines using sequential minimal optimization. Microsoft Research, <http://research.microsoft.com/apps/pubsZ?id=68391>

[6]    Weston JAE, Watkins C. Support vector machines for multi-class pattern recognition. In: Proceedings of the 7th European Symposium on Articial Neural Networks. 1999

[7]    Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based machines. Journal of Machine Learning Research, 2001,2 (Dec): 265-292

[8]    Tsochantaridis I, Joachims T, Hofmann T, Altun Y. Large margin methods for structured and interdependent output variables. JMLR, 2005,6: 1453-1484

[9]    Burges JC. A tutorial on support vector machines for pattern recognition. Bell Laboratories, Lucent Technologies. 1997

[10]    Cristianini N, Shawe-Taylor J. An Introduction to Support Vector Machines and Othre Kemer-Based Learning Methods. Cambridge University Press. 2000 (中译本：李国正，等译.支持向 量机导论.北京：电子工业出版社，2004)

[11]    邓乃扬，田英杰.数据挖掘中的新方法一支持向量机.北京：科学出版社，2004

[12]    邓乃扬，田英杰.支持向量机——理论，算法与拓展.北京：科学出版社，2009

[13]    Scholkpf B, Smola AJ. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002

[14]    Herbrich R. Learning Kernel Classifiers, Theory and Algorithms. The MIT Press, 2002

[15]    Hofinann T, Scholkopf B, Smola AJ. Kernel methods in machine learning. The Annals of Statistics, 2008, 36(3): 1171-1220
