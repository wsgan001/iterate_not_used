##### 第3章ft近邻法

灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的输入为实例的特征向量，对应于特征空间 的点；输出为实例的类别，可以取多类.*近邻法假设给定一个训练数据集，其 中的实例类别已定.分类时，对新的实例，根据其A个最近邻的训练实例的类别， 通过多数表决等方式进行预测.因此，近邻法不具有显式的学习过程.*近邻 法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型灸 值的选择、距离度量及分类决策规则是*近邻法的三个基本要素.灸近邻法1968年 由Cover和Hart提出.

本章首先叙述近邻算法，然后讨论*近邻法的模型及三个基本要素，最后 讲述A近邻法的一个实现方法一JW树，介绍构造妃树和搜索松树的算法.

###### 3.1    近邻算法

先近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数 据集中找到与该实例最邻近的*个实例，这A个实例的多数属于某个类，就把该 输入实例分为这个类.下面先叙述*近邻算法，然后再讨论其细节.

算法3.1 (々近邻法)

输入：训练数据集

^={(Xi»yi)Xx2，y2),—,(xN,yN')}

其中，为实例的特征向量，_W = {cpc2，…，〜}为实例的类别，f = 1，2,…，況；实例特征向量X;

输出：实例;c所属的类

(1)    根据给定的距离度量，在训练集r中找出与x最邻近的*个点，涵盖这 个点的;t的邻域记作M(xJ:

(2)    在M(;r)中根据分类决策规则(如多数表决)决定*的类别

少= argraax [ I{y,=cJ), i = l,2,-,Nt j = \,2,-,K    (3.1)

1

式(3.1)中，J为指示函数，即当时J为1，否则7为0.    ■

k近邻法的特殊情况是* = 1的情形，称为最近邻算法.对于输入的实例点(特

征向量)；C,最近邻法将训练数据集中与最邻近点的类作为X的类.

是近邻法没有显式的学习过程.

###### 3.2 A近邻模型

k近邻法使用的模型实际上对应于对特征空间的划分.模型由三个基本要素—— 距离度量、A值的选择和分类决策规则决定.

3^.1模型

*近邻法中，当训练集、距离度量（如欧氏距离）、值及分类决策规则（如 多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定.这相 当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的 类.这一事实从最近邻算法中可以看得很清楚.

特征空间中，对每个训练实例点*,距离该点比其他点更近的所有点组成一 个区域，叫作单元（cell）.每个训练实例点拥有一个单元，所有训练实例点的单 元构成对特征空间的_个划分.最近邻法将实例&的类y,作为其单元中所有点的 类标记（class label）.这样，每个单元的实例点的类别是确定的.图3.1是二维 特征空间划分的一个例子.

o    x（l）

图3.1    *近邻法的模型对应特征空间的一个划分

3.2.2距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映.*近邻模型的特 征空间一般是n维实数向量空间R”.使用的距离是欧氏距离,但也可以是其他距离， 如更一般的 Zy距离（2^ distance）或 Minkowski 距离（Minkowski distance）.

设特征空间A•是n维实数向量空间R"，x„XjeX, Xl=(^,x}2),-,x^)T, Xj=^,x^,-,x^)\ xp〜的么距离定义为

Lp(x„x}) = ^1 x<° - l^y    (3.2)

这里.当p = 2时，称为欧氏距离(Euclideandistance)，即 i

咕，少⑽

当p = 1时，称为曼哈顿距离(Manhattan distance)，即

A(x(,x7) = Jl^,-xJ/)|    (3.4)

1=1

当p=oo时，它是各个坐标距离的最大值，即

A.(-K,»x/) = max|x<(0 -x^\    (3.5)

图3.2给出了二维空间中p取不同值时，与原点的~距离为1 (~=1)的 点的图形.

下面的例子说明，由不同的距离度量所确定的最近邻点是不同的.

例3.1已知二维空间的3个点$ = (1，1)T，& = (5,1)T，X, = (4,4)T，试求在p

取不同值时，距离下x,的最近邻点

解因为*和七只有第二维上值不同，所以p为任何值时，.而 £1(x,,x3) = 6 , I2(x,,x3) = 4.24, Z3(x1>x3) = 3.78, I4(*,,j^) = 3.57

于是得到：p等于1或2时，x2是X,的最近邻点：p大于等于3时，x,是*的 最近邻点.    ■

3.2 J A值的选择

k值的选择会对*近邻法的结果产生重大影响.

如果选择较小的&值，就相当于用较小的邻域中的训练实例进行预测，“学 习”的近似误差(approximationerror)会减小，只有与输入实例较近的(相似的) 训练实例才会对预测结果起作用.但缺点是“学习”的估计误差(estimation error) 会增大，预测结果会对近邻的实例点非常敏感［2\如果邻近的实例点恰巧是噪声， 预测就会出错.换句话说，A值的减小就意味着整体模型变得复杂，容易发生过 拟合.

如果选择较大的*值，就相当于用较大邻域中的训练实例进行预测.其优点 是可以减少学习的估计误差.但缺点是学习的近似误差会增大.这时与输入实例 较远的(不相似的)训练实例也会对预测起作用，使预测发生错误.1值的增大 就意味着整体的模型变得简单.

如果A = 那么无论输入实例是什么，都将简单地预测它属于在训练实例 中最多的类.这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不 可取的.

在应用中，*值_般取一个比较小的数值.通常采用交叉验证法来选取最优 的女值.

3.2.4分触策规则

k近邻法中的分类决策规则往往是多数表决，即由输入实例的*个邻近的训 练实例中的多数类决定输入实例的类.

多数表决规则(majority voting rule)有如下解释：如果分类的损失函数为0-1 损失函数，分类函数为

/:R”

那么误分类的概率是

###### P(K*/(x»=i-p(r=/(^))

对给定的实例xe    其最近邻的*个训练实例点构成集合y/x).如果涵盖％(x)

的区域的类别是那么误分类率是

要使误分类率最小即经验风险最小，就要使/(乃=9)旱大，所以多数表决 规则等价于经验风险最小化.

###### 3.3 A近邻法的实现：似树

实现*近邻法时,主要考虑的问题是如何对训练数据进行快速*近邻捜索.这 点在特征空间的维数大及训练数据容量大时尤其必要.

灸近邻法最简单的实现方法是线性扫描（linear scan）.这时要计算输入实例与 每一个训练实例的距离.当训练集很大时，计算非常耗时，这种方法是不可行的.

为了提髙*近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少 计算距离的次数.具体方法很多，下面介绍其中的树（fa/tree）方法 3.3.1构造W树

kd树是一种对*维空间中的实例点进行存储以便对其进行快速检索的树形 数据结构.紐树是二叉树，表示对A维空间的一个划分（partition）.构造记树相 当于不断地用垂直于坐标轴的超平面将A维空间切分，构成一系列的i维超矩形区 域.fa/树的每个结点对应于一个*维超矩形区域.

构造fc/树的方法如下：构造根结点，使根结点对应于Jfc维空间中包含所有实 例点的超矩形区域；通过下面的递归方法，不断地对A维空间进行切分，生成子结 点.在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确 定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩 形区域切分为左右两个子区域（子结点）：这时，实例被分到两个子区域.这个 过程直到子区域内没有实例时终止（终止时的结点为叶结点）.在此过程中，将 实例保存在相应的结点上.

通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位 数（median）®为切分点，这样得到的W树是平衡的.注意，平衡的似树搜索时 的效率未必是最优的.

下面给出构造紀树的算法.

算法3.2 （构造平衡树）

输入：维空间数据集r={x,，ww}，

其中七二^1），;^2），…，^））7, z = l，2,…，况；

输出：记树.

（1）开始：构造根结点，根结点对应于包含r的龙维空间的超矩形区域.

选择x（1＞为坐标轴，以r中所有实例的坐标的中位数为切分点，将根结点 对应的超矩形区域切分为两个子区域.切分由通过切分点并与坐标轴x（1＞垂直的 超平面实现.

①    W树是存储*■维空间数据的树结构，这里的々与龙近邻法的A•意义不同，为了与习质一致，本书仍用虹树的名称.

②    一组败据按大小顾序排列起来，处在中间位a的一个数或最中间两个数的平均值.

由根结点生成深度为1的左、右子结点：左子结点对应坐标/^小于切分点 的子区域，右子结点对应于坐标#＞大于切分点的子区域.

将落在切分超平面上的实例点保存在根结点.

〈2)重复：对深度为_/的结点，选择;^为切分的坐标轴，/ = y(mod幻+1, 以该结点的区域中所有实例的;^坐标的中位数为切分点，将该结点对应的超 矩形区域切分为两个子区域.切分由通过切分点并与坐标轴垂直的超平面 实现.

由该结点生成深度为_/ + 1的左、右子结点：左子结点对应坐标/0小于切分 点的子区域，右子结点对应坐标^＞大于切分点的子区域.

将落在切分超平面上的实例点保存在该结点.

(3)直到两个子区域没有实例存在时停止.从而形成fa/树的区域划分.■

例3.2给定一个二维空间的数据集：

r = {(2,3)\(5,4)\(9,6)\(4,7)\(8,1)\(7,2/}

构造一个平衡fo/树®.

解根结点对应包含数据集r的矩形，选择x(1)轴，6个数据点的X(1)坐标的

图3.3特征空间划分

33.2搜索W树

下面介绍如何利用fe/树进行A近邻搜索.可以看到，利用记树可以省去对

大部分数据点的搜索，从而减少搜索的计算量.这里以最近邻为例加以叙述，同 样的方法可以应用到*近邻.

给定一个目标点，搜索其最近邻.首先找到包含目标点的叶结点；然后从该叶 结点出发，依次回退到父结点；不断査找与目标点最邻近的结点，当确定不可能存 在更近的结点时终止.这样搜索就被限制在空间的局部区域上，效率大为提髙.

包含目标点的叶结点对应包含目标点的最小超矩形区域.以此叶结点的实例 点作为当前最近点.目标点的最近邻一定在以目标点为中心并通过当前最近点的 超球体的内部(参阅图3.8).然后返回当前结点的父结点，如果父结点的另一子 结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例 点.如果存在这样的点，将此点作为新的当前最近点.算法转到更上一级的父结 点，继续上述过程.如果父结点的另一子结点的超矩形区域与超球体不相交，或 不存在比当前最近点更近的点，则停止搜索.

下面叙述用松树的最近邻捜索算法.

算法3«3 (用/W树的最近邻搜索)

输入：已构造的fc/树；目标点

输出：x的最近邻.

(1)    在fc/树中找出包含目标点的叶结点：从根结点出发，递归地向下访问 fa/树.若目标点;V当前维的坐标小于切分点的坐标，则移动到左子结点，否则移 动到右子结点.直到子结点为叶结点为止.

(2)    以此叶结点为“当前最近点

(3)    递归地向上回退，在每个结点进行以下操作：

(a)    如果该结点保存的实例点比当前最近点與离目标点更近，则以该实例点 为“当前最近点”.

(b)    当前最近点一定存在于该结点一个子结点对应的区域.检查该子结点的 父结点的另一子结点对应的区域是否有更近的点.具体地，检査另一子结点对应 的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超 球体相交.

如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动 到另~个子结点.接着，递归地进行最近邻搜索；

如果不相交，向上回退.

（4）当回退到根结点时，搜索结束.最后的“当前最近点”即为的最近 邻点.    ■

如果实例点是随机分布的，紀树搜索的平均计算复杂度是，这里况 是训练实例数.奴树更适用于训练实例数远大于空间维数时的ft近邻捜索.当空 间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描.

下面通过一个例题来说明搜索方法.

例3.3给定一个如图3.5所示的似树，根结点为W，其子结点为5 , C等.树 上共存储7个实例点；另有一个输入目标实例点S,求5的最近邻.

解首先在fc/树中找到包含点S的叶结点D （图中的右下区域），以点Z）作为 近似最近邻.真正最近邻~定在以点S为中心通过点D的圆的内部.然后返回结 点Z）的父结点在结点5的另一子结点F的区域内搜索最近邻.结点F的区域 与圆不相交，不可能有最近邻点.继续返回上一级父结点J，在结点d的另一子结 点C的区域内搜索最近邻.结点C的区域与圆相交;该区域在圆内的实例点有点£， 点£:比点D更近，成为新的最近邻近似.最后得到点£是点5的最近邻.    ■

###### 本章概要

1.女近邻法是基本且简单的分类与回归方法.*近邻法的基本做法是：对给 定的训练实例点和输入实例点，首先确定输入实例点的个最近邻训练实例点，然

后利用这A个训练实例点的类的多数来预测输入实例点的类.

\2.    *近邻模型对应于基于训练数据集对特征空间的一个划分.*近邻法中， 当训练集、距离度量、it值及分类决策规则确定后，其结果唯一确定.

\3.    Jfc近邻法三要素：距离度量、&值的选择和分类决策规则.常用的距离度 量是欧氏距离及更一般的距离.A值小时，*近邻模型更复杂：A值大时，是近 邻模型更简单.A值的选择反映了对近似误差与估计误差之间的权衡，通常由交 叉验证选择最优的常用的分类决策规则是多数表决，对应于经验风险最小化.

\4.    女近邻法的实现需要考虑如何快速搜索*个最近邻点.fa/树是一种便于 对*维空间中的数据进行快速检索的数据结构.树是二叉树，表示对A维空间 的一个划分，其每个结点对应于A维空间划分中的一个超矩形区域.利用紀树可 以省去对大部分数据点的搜索，从而减少搜索的计算量.

###### 继续阅读

k近邻法由Cover与Hart提出近邻法相关的理论在文献［2,3］中己有论 述.*近邻法的扩展可参考文献［4］. W树及其他快速搜索算法可参见文献［5］.关 于走近邻法的介绍可参考文献［2］.

习 题

3.1参照图3.1，在二维空间中给出实例点，画出*为1和2时的灸近邻法构成的 空间划分，并对其进行比较，体会女值选择与模型复杂度及预测准确率的关系.

3.2利用例题3.2构造的奴树求点* = (3,4.5/的最近邻点.

3.3参照算法3.3,写出输出为的是近邻的算法.

###### 参考文献

[1]    Cover T, HartP. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 1967

[2]    Hastie T, TibshiraniR, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2001 (中译本：统计学习基础一数据挖掘、推理与预測.范明，柴玉梅， 昝红英等译.北京：电子工业出版社，2004)

[3]    Friedman J. Flexible metric nearest neighbor classification. Technical Rqx)rt, 1994

[4]    Weinberger KQ, Blitzer J, Saul LK. Distance metric learning for large margin nearest neighbor classification. In： Proceedings of the NIPS. 2005

[5]    Samet H. The Design and Analysis of Spatial Data Structures. Reading, MA: Addison-Wesley, 1990
