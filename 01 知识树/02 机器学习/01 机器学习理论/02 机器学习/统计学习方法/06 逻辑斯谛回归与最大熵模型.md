##### 第6章逻辑斯谛回归与最大滴模型

逻辑斯谛回归(logistic regression)是统计学习中的经典分类方法.最大熵是 概率模型学习的一个准则,将其推广到分类问题得到最大熵模型(maximum entropy model).逻辑斯谛回归模型与最大熵模型都属于对数线性模型.本章首先介绍逻 辑斯谛回归模型，然后介绍最大熵模型，最后讲述逻辑斯谛回归与最大熵模型的 学习算法，包括改进的迭代尺度算法和拟牛顿法.

###### 6.1逻辑斯谛回归模型

6.1.1逻辑斯谛分布

首先介绍逻辑斯诗分布(logistic distribution).

定义6.1(逻辑斯谛分布〉设尤是连续随机变量，尤服从逻辑斯谛分布是指 X具有下列分布函数和密度函数：

式中，//为位置参数，



e-(x->O'y

Z>0为形状参数.



(6.1)

(6.2)



逻辑斯谛分布的密度函数/00和分布函数的图形如图6.1所示.分布函 数属于逻辑斯谛函数，其图形是一条S形曲线(sigmoid curve).该曲线以点&，£) 为中心对称，即满足

F(-x+/z)-| = -F(x -") + 去

曲线在中心附近增长速度较快，在两端增长速度较慢.形状参数7的值越小，曲 线在中心附近增长得越快.

/(x)    F(X)

图6.1逻辑斯谛分布的密度函数与分布函数

6.1.2二项逻辑斯谛回归模型

二项逻辑斯谛回归模型(binomial logistic regression model)是一种分类模型， 由条件概率分布表示，形式为参数化的逻辑斯谛分布.这里，随机变量尤 取值为实数，随机变量7取值为1或0.我们通过监督学习的方法来估计模型参数.

定义6.2 (逻辑斯谛回归模型)二项逻辑斯谛回归模型是如下的条件概率 分布：

p(y=i|x)=



l + exp(w-x+Z>)



p(r=o|x)=



\+exp(w-x+b')



(6.3)

(6.4)



这里，xeR”是输入，Fe {0,1}是输出，weR”和6eR是参数， 量，h称为偏置，w.j:为w和的内积.



w称为权值向



对于给定的输入实例,按照式(6.3)和式(6.4)可以求得= 和 p(r=o|x).逻辑斯谛回归比较两个条件概率值的大小，将实例x分到概率值较 大的那一类.

有时为了方便，将权值向量和输入向量加以扩充，仍记作W, 即 x = (x(1),x(2),-,xw,1)t.这时，逻辑斯谛回归模型如下：



尸(y = l|x) =



exp(w.x)



l+exp(w.x)



(6.5)



p(y=o|x)=



l

l + exp(w.x)



(6.6)



现在考査逻辑斯谛回归模型的特点.一个事件的几率(odds)是指该事件发 生的概率与该事件不发生的概率的比值.如果事件发生的概率是p,那么该事件



的几率是该事件的对数几率(logodds)或logit函数是

logit(p) = logT^

对逻辑斯谛回归而言，由式(6.5)与式(6.6)得

log



p(y=i|x)

i-p(y=i|x)



这就是说，在逻辑斯谛回归模型中，输出r=i的对数几率是输入的线性函数. 或者说，输出r=i的对数几率是由输入x的线性函数表示的模型，即逻缉斯谛回 归模型.

换一个角度看，考虑对输入进行分类的线性函数WX,其值域为实数域.

注意，这里JteRw.weR"41.通过逻辑斯谛回归模型定义式(6.5)可以将线性函

数wu转换为概率:



尸(r=i|x)=



exp(w.x) l + exp(w«x)



这时，线性函数的值越接近正无穷，概率值就越接近1;线性函数的值越接近负 无穷，概率值就越接近0 (如图6.1所示).这样的模型就是逻辑斯谛回归模型.

6.1.3模型参数估计

逻辑斯谛回归模型学习时，对于给定的训练数据集7 =收，只)，(;^&)，一， (.xN,yN} },其中，x，eR”，乃e{O，l}，可以应用极大似然估计法估计模型参数，从 而得到逻辑斯谛回归模型.

设：    p(y=i|x)=^(x), p(y=o|x)=i-^(x)

似然函数为

n闹r[i-苽u)广 1-1

对数似然函数为

N

[(W) =    log^) + (1 - ^)log(l- ^(x,))]

(=1

=2[只 log,    +1°g(1_ 苽(〜))]

(=i L i—龙(七)    」

N

=E[乃(w.易)-lo80 + exp(w.x,)]

)=i

对£(w)求极大值，得到w的估计值.

这样，问题就变成了以对数似然函数为目标函数的最优化问题.逻辑斯谛回 归学习中通常采用的方法是梯度下降法及拟牛顿法.

假设w的极大似然估计值是必，那么学到的逻辑斯谛回归模型为

p(y = l|x)=



exp(w.x) l + exp(w»x)



P(K = 0|x) =



1

l + exp(w.x)



6.1.4多项逻辑斯谛回归

上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类.可以将其推 广为多项選辑斯谛回归模型(multi-nominal logistic regression model),用于多类

分类.假设离散型随机变董r的取值集合是｛1，2,…,那么多项逻辑斯谛回归 模型是

P(r = *|x) = —:X『(VX)_, k = \,2,'-,K-l    (6.7)

l + ^exiXwj.x)

*=i

P(Y=K\x)^    - (6.8)

l + ^exp(wA.x)

*=i

这里，xeR^^WjeR"*1.

二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归.

###### 6.2最大嫡模型

最大熵模型(maximum entropy model)由最大嫡原理推导实现.这里首先叙述 一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式.

6.2.1最大熵原理

最大熵原理是概率模型学习的一个准则.最大熵原理认为，学习概率模型时， 在所有可能的概率模型(分布)中，熵最大的模型是最好的模型.通常用约束条 件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模 型集合中选取嫡最大的模型.

假设离散随机变量义的概率分布是P(；O,则其炳(参照5.2J节〉是

H(P) = ~XP(x)logP(x)    (6.9)

熵满足下列不等式： 式中，是Y的取值个数，当且仅当义的分布是均匀分布时右边的等号成 立.这就是说，当Z服从均匀分布时，熵最大.

直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约 束条件•在没有更多信息的情况下，那些不确定的部分都是“等可能的”.最大 熵原理通过熵的最大化来表示等可能性.“等可能”不容易操作，而熵则是一个 可优化的数值指标.

首先，通过一个简单的例子来介绍一下最大熵原理®.

例6.1假设随机变量义有5个取值｛A，B，C，D，E｝，要估计取各个值的概率 P(A),P(B),P(C),P(D),P(E).

解这些概率值满足以下约束条件：

P(A) + P(B) + P(C) + P(D) + P(£) = l

满足这个约束条件的概率分布有无穷多个.如果没有任何其他信息，仍要对概率 分布进行估计，一个办法就是认为这个分布中取各个值的概率是相等的：

P(A) = P(B) = P(C) = P(D) = P(£) = -

等概率表示了对事实的无知.因为没有更多的信息，这种判断是合理的.

有时，能从一些先验知识中得到一些对概率值的约束条件，例如：

3

尸⑷十尸(5)=台

P(A) + P(B) + P(C) + P(D) + P(E) = 1

满足这两个约束条件的概率分布仍然有无穷多个.在缺少其他信息的情况下，可 以认为d与5是等概率的，C, 2)与£是等概率的，于是，

3

P(^) = P(5) = —

P(O = P(D) = P(E) = 1-

如果还有第3个约束条件：

p(^)+p(O=i P(J) + P(5) = A

PG4) + P(B) + P(C) + P(D) + P(£) = l

可以继续按照满足约束条件下求等概率的方法估计概率分布.这里不再继续讨 论.以上概率模型学习的方法正是遵循了最大熵原理. ■

图6.2提供了用最大熵原理进行概率模型选择的几何解释.概率模型集合沪 可由欧氏空间中的单纯形(simplex) ®表示，如左图的三角形(2-单纯形).一个 点代表~个模型，整个单纯形代表模型集合.右图上的一条直线对应于一个约束 条件，直线的交集对应于满足所有约束条件的模型集合.一般地，这样的模型仍 有无穷多个.学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则 给出最优模型选择的一个准则.

概率摸型空间

图6.2



满足约束条件的模型集合

概率模型集合



6.2.2最大摘模型的定义

最大摘原理是统计学习的一般原理，将它应用到分类得到最大熵模型. 假设分类模型是一个条件概率分布P(r|JQ, JfeAfeR”表示输入，表

示输出，A■和;V分别是输入和输出的集合.这个模型表示的是对于给定的输入 以条;(牛概率尤)输出y.

▲定一个训练数据集

学习的目标是用最大熵原理选择最好的分类模型.

首先考虑模型应该满足的条件.给定训练数据集，可以确定联合分布

的经验分布和边缘分布/XZ)的经验分布，分别以戶cv，r)和戶(jv)表示.这里，

P(X = X,Y = y) = ^=^-=y^ N

#### P(%=x)=r<^)

N

其中，V(Z = ;c,y = y)表示训练数据中样本(jc，y)出现的频数，v(Z = x)表示训练 数据中输入;c出现的频数，7V表示训练样本容量.

用特征函数.(feature function) /(x，>>)描述输入jc和输出y之间的某--个事 实.其定义是

f(x,y) =



fl, :V与y满足某_事实 !0，否则



它是一个二值函数®,当;c和y满足这个事实时取值为1，否则取值为0. 特征函数/(x,y)关于经验分布戶(U)的期望值，用£#(/)表示.

EP(f) = ^P(.x,y)f{x,y)

特征函数/(u)关于模型p(y | x)与经验分布戶(尤)的期望值，用Ep(f)表示.

如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即 尽(/) = &(/)    (6.10)

或

x)f(x，y) = ^P(x,j)/(x,j)    (6.11)

我们将式(6.10)或式(6.11)作为模型学习的约束条件.假如有《个特征函数 i = l,2,-,n,那么就有《个约束条件.

定义6.3 (最大熵模型)假设满足所有约束条件的模型集合为

C^{PeP\Ep(fi) = Ep(fl), i = 1,2,-,«}    (6.12)

定义在条件概率分布p(y I z)上的条件熵为

ff(P) = -^P(.x)P{y I x)\ogl\y | x)    (6.13)

*.y

则模型集合C中条件熵//(P)最大的模型称为最大熵模型.式中的对数为自然 对数.

6.2.3最大熵模型的学习

最大熵模型的学习过程就是求解最大熵模型的过程.最大熵模型的学习可以 形式化为约束最优化问题.

对于给定的训练数据集卜奴久从^从…办^^^以及特征函数/^办 f = 1，2,,最大熵模型的学习等价于约束最优化问题：

晋    //(P) = -£ 戶(x)/V I x) log / VIX)

s.t.    Ep{ff) = Ep{ft), i = l，2，“.，《

Z 戶(J|x) = l

按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：

蛙-//(P) = IP(jf)_P(j|j：)logP(y|jc)    (6.14)

s.t. Ep(J'l)-Ep(fl') = 0, i = l,2,--,n 2>(小)=1

(6.15)

(6.16)



求解约束最优化问题(6.14)〜(6.16)，所得出的解，就是最大熵模型学习的 解.下面给出具体推导.

这里，将约束最优化的原始问题转换为无约束最优化的对偶问题®.通过求 解对偶问题求解原始问题.

首先，弓I进拉格朗日乘子如叫,％，…，w„，定义拉格朗日函数

Z(P，w)吾-H(P) + w0〔1 一 £ 戶(m) +    (么(/) - £P(/))

=^P(x)P(>-| *)log/V | x) +w。〔1 -芩P(y | X))

\+ Zw/f    ，少)-[A工)尸 CFI ^)Z(x,j)|    (6.17)

###### <=>    J

最优化的原始问题是

mn max Z(P9w)    (6.18)

对偶问题是

max nun£(P,w)    (6.19)

由于拉格朗日函数2：(P，w)是P的凸函数，原始问题(6.18)的解与对偶问题 (6.19)的解是等价的.这样，可以通过求解对偶问题(6.19)来求解原始问题(6.18).

首先，求解对偶问题(6.19)内部的极小化问题L(P,w) . ram L{P, w)是w的 函数，将其记作

^(w) = imn£(P,w) = L(Pw,w)    (6.20)

P(w)称为对偶函数.同时，将其解记作

Pw = argnun £(P, w) = Pw(y \ x)    (6.21)

具体地，求£(P,w)对PCvIX)的偏导数

dL(P, w) dP(y\x)

ZA^)^logP(y |x)+l-w0 -gw(/(x,y)

令偏导数等于0,在戶(;c)>0的情况下，解得

z „    X exp

，W = exP^w,/(x,y) + »0-lJ=二^。)

由于lP(y»=l，得

ACr I:)=j^yexp〔 J 叫你J))    (6.22)

其中，

zwOO = fexp(其 w,/(x：y))    (6.23)

Zw(x)称为规范化因子；/(x,y)是特征函数；w,是特征的权值.由式(6.22)、式(6.23) 表示的模型Pw=Pw(j|；c)就是最大熵模型.这里，w是最大嫡模型中的参数向量.

之后，求解对偶问题外部的极大化问题

max *F(w)    (6.24)

将其解记为w*，即

w* =argmax !F(w)    (6.25)

这就是说，可以应用最优化算法求对偶函数!F(w)的极大化，得到V,用来 表示这里，P*=Pw.=Pw.(j|;r)是学习到的最优模型(最大熵模型).也 就是说，最大熵模型的学习归结为对偶函数的极大化.

例6.2学习例6.1中的最大熵模型.

解为了方便，分别以只，乃，乃，h，ys表示儿S, C，D和五，于是最大熵模 型学习的最优化问题是

min -，=玄尸⑻log尸⑻

i=i

st. P(y,) + P(y2) = PU) + P(y2)=

/=i    /=i

引进拉格朗日乘子wD，Wl,定义拉格朗日函数 £(P,W) = ^U)logPU)+ w^PCFl) + P(J2)-^ + w0^PU)-l')

根据拉格朗日对偶性，可以通过求解对偶最优化问题得到原始最优化问题的 解，所以求解

max nun L(P, w)

首先求解£(P，w)关于7＞的极小化问题.为此，固定％，叫，求偏导数： ai(p，w)

冲(乃)一 弘(P，w)_

dL(P，w) dP(y3)" dL(P,w) _ dP(y4) dL(P,w)



= l + logP(yl) + wI+w0



= l + logPCv2) + w1+w0



i = l + logP(y3) + w0



\- = l + logPCv4) + w0



• = l + log 尸(js) + w0



令各偏导数等于0,解得



P(yi)=P(y2)=e^-1 戶(j3)=/W=尸⑻：^一1

于是，

nunL(P,w) = L(Pw,w) = -2e_ W -3e’-i-^w,-w0 再求解Z(Pw,W)关于w的极大化问题：

max L(Pw,w) =    -3e'^-' ~^-w0

分别求Z(Pw，w)对we，Wl的偏导数并令其为0,得到

=—

20

e-1*-1 =—

30

于是得到所要求的概率分布为

尸⑻吻2)=盖 P(y3)=P(y,)=P(ys)=^

6.2.4极大似然估计

从以上最大熵模型学习中可以看出，最大熵模型是由式(6.22)、式(6.23)表示的 条件概率分布.下面证明对偶函数的极大化等价于最大熵模型的极大似然估计.

已知训练数据的经验概率分布户(尤,7),条件概率分布P(F| I)的对数似然 函数表示为

W) = logEPV I    = 27Cv，y)iog/协)

当条件概率分布IX)是最大熵模型(6.22)和(6.23)时,对数似然函数为 ^(Pw) = ZAx,j)logP(y|x)

=芬戶(U)其    ~^P{x,y)\ogZv{x)

= LAx，j0^>,/0c，>0-ZAx)lOg4(x)    (6.26)

*,y    m    x

再看对偶函数.由式(6.17)及式(6.20)可得 w=Xp(xWy\ x)iogpw(^|x)

*.y

+ZWJ X^(x»yVi(x>y) ~^P(.x)Pw(.y I I

最后一步用到¥/Xy|；0 = l.

比较式(6.26)和式(6.27),可得

F(w) = Z/Pw)

既然对偶函数fOv)等价于对数似然函数，于是证明了最大熵模型学习中 的对偶函数极大化等价于最大熵模型的极大似然估计这一事实.

这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶 函数极大化的问题.

可以将最大熵模型写成更一般的形式.



(6.28)



其中，

z«(x) = expf (x,y) j    (6.29)

y    J

这里，xe R”为输入，/e {1，2,…,A}为输出，we R”为权值向量，f^y), i = l,2,-,n 为任意实值特征函数.

最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型 (log linear model).模型学习就是在给定的训练数据条件下对模型进行极大似然 估计或正则化的极大似然估计.

###### 6.3模型学习的最优化算法

逻辑斯谛回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化 问题，通常通过迭代算法求解.从最优化的观点看，这时的目标函数具有很好的 性质.它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优 解.常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法.牛顿法 或拟牛顿法一般收敛速度更快.

下面介绍基于改进的迭代尺度法与拟牛顿法的最大熵模型学习算法.梯度下 降法参阅附录A.

63.1改进的迭代尺度法

改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习 的最优化算法.

已知最大熵模型为

其中，

###### 2w(x)=Xexpf

/ V '=i    J

对数似然函数为

£(w)=Z^X> y^wif^x> y) - 2 戶(x) los z* ⑶

目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值必.

ns的想法是：假设最大熵模型当前的参数向董是w=(Wl，叫，…，hJt，我们

希望找到一个新的参数向量W +彡=(W, +我，w2 +么，• • •,％ +式)T,使得模型的对数 似然函数值增大.如果能有这样一种参数向量更新的方法那么就 可以重复使用这一方法，直至找到对数似然函数的最大值.

对于给定的经验分布模型参数从W到W + A对数似然函数的改变

量是

£(w+<y) -Z(w) = ^PCx.^logP^^lx) -X^(x,^)logPw(j|x)

*.y    *，y

x,y    i=i    *    ZwW

利用不等式

-loga>l-a, a>0 建立对数似然函数改变量的下界：

于是有

L(w+S)—L(w) > A(S | w)

即J(J|w)是对数似然函数改变量的一个下界.

如果能找到适当的J使下界J(5|w)提高，那么对数似然函数也会提高.然

而，函数必中的是一个向量，含有多个变量，不易同时优化.us试图一 次只优化其中一个变量<5；，而固定其他变量i*j.

为达到这一目的,ns进一步降低下界义⑷w).具体地,ns引进一个量/*(*，力，

因为/是二值函数，故/*Cc，y)表示所有特征在(x，W出现的次数.这样，A{S\yv)

可以改写为

A{S\w) = ^P(x,y)    >») +1 - Z 戶(•今⑽ I *)哪(广0^)其

(6.30)

利用指数函数的凸性以及对任意/,有且这-事实，根 / (x,y)    tf/*(x，y)

据Jensen不等式，得到

哪〔$微，H噠慰响〜))

于是式(6.30)可改写为

(6.31)

记不等式(6.31)右端为

糊中芬知崎<U(w)+1 - E加)5X(y Ix)奴舞分响广⑽))

于是得到

L(w + <5)-L(w)^ B(J|w)

这里，5(5| w)是对数似然函数改变量的一个新的(相对不紧的)下界.

求B(万|w)对<5；的偏导数：

初S 叫=X^yVi(x>y)~    I •y)/(x，y)exp(<V*(uO)

aoi *.y    *    y

(6-32)

在式(6.32)里，除<5；外不含任何其他变量.令偏导数为0得到

J^P(x)Pw(j | x)/(x,>>)exp(<yz/*(x,>»)) = Ep(f,)    (6.33)

*>y

于是，依次对衣求解方程(6.33)可以求出炙

这就给出了一种求w的最优解的迭代算法，即改进的迭代尺度算法ns.

算法6.1 (改进的迭代尺度算法ns〉

输入：特征函数义，/2，…，X;经验分布户(m，模型忍(/⑷

输出：最优参数值最优模型

⑴对所有fe｛l，2,…，《｝，取初值w,=0

(2)对每｛1,2, •,«｝：

(a)令么是方程

X^(x)^(yl x)/t(x> y) exp(^/#(X, y)) = Ep(/,)

的解，这里，

i=l

(b)更新叫值：w, <-w,+<yz

(3)如臬不是所有w,都收敛，重复步(2).

这一算法关键的一步是⑻，即求解方程(6.33)中的衣.如果/*(x，jf)是常数, 即对任何有/*(;c，;f) = A/，那么《可以显式地表示成

(6.34)

如果/#Cc，y)不是常数，那么必须通过数值计算求<5；.简单有效的方法是牛顿 法.以g(<5；.) = 0表示方程(6.33),牛顿法通过迭代求得<5；，使得g(<5；) = 0.迭代 公式是

《h 纖    (6.35)

只要适当选取初始值由于么的方程(6.33)有单根，因此牛顿法恒收敛，而 且收敛速度很快.

6.3.2拟牛顿法

最大熵模型学习还可以应用牛顿法或拟牛顿法.参阅附录B.

对于最大熵模型而言，

梯度，

g(W)



![img](2012.4e2a-40.jpg)



其中

= 2I x)/(x,j) -£//), i = i,2,-,n

相应的拟牛顿法BFGS算法如下.

算法6.2 (最大熵模型学习的BFGS算法〉

输入：特征函数Z，/2,经验分布戶(x，y),目标函数/(w)，梯度g(w) = V/(w),精度要求£;

输出：最优参数值V;最优模型<.紗|勾.

(1)    选定初始点ww,取5。为正定称矩阵，置k = 0

(2)    计算私=容0^).若!|g*||<e,则停止计算，得州.=眞,否则转(3)

(3)    由Bkpk = -gk求出凡

(4)    一维搜索：求人使得

/(w(t> + Aa) = nun/(ww +Apk)

(5)    置 #+1)=#)+'凡

(6)    计算g*+1=g(M戸),若U+1||<£,则停止计算，得w.=w⑽；否则， 按下式求出5*+|:

» _ D , y*y*T ^ksks；Bk

其中，



yk=gM-§k > «y* =w(*+*)-w(*)

(7) Sk=k+l,转(3).



###### 本章概要

1.逻辑斯谛回归模型是由以下条件概率分布表示的分类模型.逻辑斯谛回 归模型可以用于二类或多类分类.

P(Y^k\x) = ~    , k = l)2,...,K-i

l+^exp(wt.x)

P(Y = K\x} =



l + ^exp( wk-x)



这里，为输入特征，w为特征的权值.

逻辑斯谛回归模型源自逻辑斯谛分布，其分布函数F(x)是S形函数.逻辑斯

谛回归模型是由输入的线性函数表示的输出的对数几率模型•

2.最大摘模型是由以下条件概率分布表示的分类模型.最大熵模型也可以

用于二类或多类分类.

Zw(x) = Eexp〔其《＞，＞＞))

其中，Zw(jc)是规范化因子，乂为特征函数，w,为特征的权值.

\3.    最大熵模型可以由最大熵原理推导得出.最大熵原理是概率模型学习或 估计的一个准则.最大熵原理认为在所有可能的概率模型(分布)的集合中，熵 最大的模型是最好的模型.

最大熵原理应用到分类模型的学习中，有以下约束最优化问题：

min -H(P) = £P(x)P(j| x)logP(j|x)

*,y

s.t. P(Z)-P(/) = O, i = l,2,--,n

^P(y\x) = l y

求解此最优化问题的对偶问题得到最大熵模型.

\4.    逻辑斯谛回归模型与最大嫡模型都属于对数线性模型.

\5.    逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计，或正则化 的极大似然估计.逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优 化问题.求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法.

###### 继续阅读

逻辑斯谛回归的介绍参见文献［1］,最大熵模型的介绍参见文献［2, 3］.逻辑 斯谛回归模型与朴素贝叶斯模型的关系参见文献［4］，逻辑斯谛回归模型与 AdaBoost的关系参见文献［5］,逻辑斯谛回归模型与核函数的关系参见文献［6］.

6.1确认逻辑斯谛分布属于指数分布族.

6.2写出逻辑斯谛回归模型学习的梯度下降算法.

6.3写出最大熵模型学习的DFP算法.（关于一般的DFP算法参见附录B）

###### 参考文献

[1]    Berger A, Della Pietta SD, Pietra VD. A maximum entropy approach to natural language processing. Computational Linguistics, 1996,22（1）, 39-71

[2]    Berger A. The In^roved Iterative Scaling Algraithm: A Gentle Introduction. ht^>y/www.cs.cmu.edu/ afs/cs/user/abergerAvww/ps/scaling.ps

[3]    Hastie T, Tibshirani R. Friedman J. The Elements of Statistical Learning: Data Mining,

Inference, and Prediction. Springer-Verlag. 2001 （中译本：统计学习基础-数据挖掘、推

理与预测.范明，柴玉梅，昝红英等译.北京：电子工业出版社，2004）

[4]    Mitchell TM. Machine Learning. McGraw-Hill Conyjanies，Inc. 1997 （中译本：机器学习.北京: 机械工业出版社，2003）

[5]    Collins M, Schapire RE, Singer Y. Logistic Regression, AdaBoost and Bregman Distances. Machine Learning Journal, 2004

[6]    Canu S, SraolaAJ. Kernel method and exponential family. Neurocomputing, 2005, 69:714-720
