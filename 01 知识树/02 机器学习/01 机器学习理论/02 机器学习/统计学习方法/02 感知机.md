##### 第2章感知机

感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量， 输出为实例的类别，取+1和-1二值.感知机对应于输入空间(特征空间)中将 实例划分为正负两类的分离超平面，属于判别模型.感知机学习旨在求出将训练 数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度 下降法对损失函数进行极小化，求得感知机模型.感知机学习算法具有简单而易 于实现的优点，分为原始形式和对偶形式.感知机预测是用学习得到的感知机模 型对新的输入实例进行分类.感知机1957年由Rosenblatt提出，是神经网络与支 持向量机的基础. ’

本章首先介绍感知机模型；然后叙述感知机的学习策略，特别是损失函数： 最后介绍感知机学习算法，包括原始形式和对偶形式，并证明算法的收敛性.

###### 2.1感知机模型

定义2.1 (感知机)假设输入空间(特征空间)是；V£R”，输出空间是 ^ = {+1,-1}-输入表示实例的特征向量，对应于输入空间(特征空间〉的 点；输出ye y表示实例的类别.由输入空间到输出空间的如下函数

/(x) = sign(w.x+i)    (2.1)

称为感知机.其中，冰和/»为感知机模型参数，we R"叫作权值(weight)或权 值向量(weight vector)，6e R叫作偏置(bias)，w.jc表示w和x的内积.sign 是符号函数，即

.,、f+1, x^O

S，gnW = U x<0    ⑽

感知机是一种线性分类模型，属于判别模型.感知机模型的假设空间是定义 在特征空间中的所有线性分类模型(linear classification model)或线性分类器 (linear classifier),即函数集合= w-x + b}.

感知机有如下几何解释：线性方程

w>x+b = 0    (2.3)

对应于特征空间R”中的-■个超平面S,其中w是超平面的法向量，*是超平面的 截距.这个超平面将特征空间划分为两个部分.位于两部分的点(特征向量)分 别被分为正、负两类•因此，超平面S称为分离超平面（separating hyperplane）， 如图2.1所示.

感知机学习，由训练数据集（实例的特征向量及类别）

其中，x,e^ = Rn,乃ey = {+l，-l}，i = l，2,…，N,求得感知机模型（2.1），即求 得模型参数％ 6.感知机预测，通过学习得到的感知机模型，对于新的输入实例 给出其对应的输出类别.

###### 2.2感知机学习策略

2.2.1数据集的线性可分性

定义2.2 （数据集的线性可分性）给定一个数据集

其中，XieX = R", Z.e> = {+1,-1}, i = lt2,-,N,如果存在某个超平面5 w.x+6 = 0

能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 乃=+1的实例f，有w.x,+6>0，对所有只=-1的实例f,有w.x,+6<0,则称 数据集r为线性可分数据集（linearly separable dataset）;否则，称数据集T线性 不可分.

2.2.2感知机学习策略

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集

正实例点和负实例点完全正确分开的分离超平面•为了找出这样的超平面，即确 定感知机模型参数需要确定一个学习策略，即定义(经验)损失函数并将 损失函数极小化.

损失函数的一个自然选择是误分类点的总数.但是，这样的损失函数不是参 数w，&的连续可导函数，不易优化.损失函数的另一个选择是误分类点到超平面 S的总距离，这是感知机所采用的.为此，首先写出输入空间R1•中任一点气到超 平面S的距离：

h|w-x°+A|

这里,M是VV的£2范数.

其次，对于误分类的数据乃)来说，

-y,(w>xi +b)>0

成立.因为当w.x, + 6>0时，yt --1,而当w.x,+6<0时，yt = +1.因此，误 分类点&到超平面S的距离是

这样，假设超平面S的误分类点集合为A/，那么所有误分类点到超平面5的 总距离为

①第7章中会介绍少(wx+*)称为样本点的函数间隔.

##### -缸*Il+i)

不考虑；^，就得到感知机学习的损失函数®.

IM

给定训练数据集

T ~    乃)，…，

其中，xteX = Rn, j<ey = {+l,-l}, i = l,2,-,N.感知机sign(wx+6)学习的 损失函数定义为

L(w,b)=~Yiyi(w.xl+b)    (2.4)

x,eU

其中A/为误分类点的集合.这个损失函数就是感知机学习的经验风险函数. 显然，损失函数I(w，6)是非负的.如果没有误分类点，损失函数值是0.而

且，误分类点越少，误分类点离超平面越近，损失函数值就越小._个特定的样 本点的损失函数：在误分类时是参数w，6的线性函数，在正确分类时是0.因此， 给定训练数据集T ,损失函数£(w，h)是w，&的连续可导函数.

感知机学习的策略是在假设空间中选取使损失函数式(2.4)最小的模型参数 w,b,即感知机模型.

###### 2.3感知机学习算法

感知机学习问题转化为求解损失函数式(2.4)的最优化问题，最优化的方法 是随机梯度下降法.本节叙述感知机学习的具体算法，包括原始形式和对偶形式， 并证明在训练数据线性可分条件下感知机学习算法的收敛性.

2.3.1感知机学习算法的原始形式

感知机学习算法是对以下最优化问题的算法.给定一个训练数据集

其中，x,eX = Rn,    =    i = l,2,-,N ,求参数使其为以下损

失函数极小化问题的解

= y^w-Xf+b)    (2.5)

其中M为误分类点的集合.

感知机学习算法是误分类驱动的，具体采用随机梯度下降法(stochastic gradient descent).首先，任意选取一个超平面wD，hfl,然后用梯度下降法不断地 极小化目标函数(2.5).极小化过程中不是一次使从中所有误分类点的梯度下降， 而是一次随机选取一个误分类点使其梯度下降.

假设误分类点集合3/是固定的，那么损失函数Z(w,6)的梯度由 VwL(w，6) = —[Vi

x,eU

^bL{w,b) = -'^yi

给出.

随机选取一个误分类点{xt,y,),对w，h进行更新：

w^w+rjy^    p.6)

b^-b + rjy,    (2.7)

式中?7(0<7<1)是步长，在统计学习中又称为学习率(learningrate).这样，通过 迭代可以期待损失函数L(w，6)不断减小，直到为0.综上所述，得到如下算法：

算法2.1 (感知机学习算法的原始形式)

输入:训练数据集r = {(x,，只)，(x2，y2 )，• • •,(〜，外)}，其中x, e A* = R”，乃e y = {—1»+1}，i = l，2,…，JV;学习率7(0<叮彡 1);

输出：w,b ；感知机模型/(x) = sign(w，x+6).

(1)    选取初值w0,么

(2)    在训练集中选取数据&，只)

(3)    如果乃(w.xf+6)<0

W^-w+TJy^, b h + J]y,

(4)转至(2〉，直至训练集中没有误分类点.    ■

这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面 的错误一侧时，则调整的值，使分离超平面向该误分类点的~侧移动，以减 少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类.

算法2.1是感知机学习的基本算法，对应于后面的对偶形式，称为原始形 式.感知机学习算法简单且易于实现.

例2.1如图2.2所示的训练数据集，其正实例点是x,=(3,3)t, x2=(4,3)t, 负实例点是x,=(l,l)T,试用感知机学习算法的原始形式求感知机模型f(x) = sign(w.x + Z>).这里，w=(w(1)，w(2〉)T，x = (x(1),a：(2))t .

解构建最优化问题：

minL(w,Z>) = -^ y,(w'X + b) 按照算法2.1求解w, 6. 77 = 1.

(1)    取初值wo=O，bo=O

(2)    对x, =(3,3)T，j»,(w0.x, +Z>„) = 0,未能被正确分类，更新 w，6

wi =wo +^i-xi = (3»3)T » =b0+yj =1

得到线性模型

wcx + bt =3xm+3xw+l

(3)    对Ia，显然，j,(w,•x,+61)>0,被正确分类，不修改w，Z>;

对 Xj=(l，l)T，>>3(w, .^+h,)<0,被误分类，更新w，6.

w2=w, +y3Xj= (2,2)T，b2=b,+y3=G

得到线性模型

w2^x + b2=2xw + 2x(2)

如此继续下去，直到

w7=(1，1)t，b7=-3

w7-x+6t =x(i) +x(2)-3

对所有数据点只(％.易+*7)>0,没有误分类点，损失函数达到极小. 分离超平面为    x[1](#bookmark2) [2](#bookmark3)+xm~3 = 0

感知机模型为    /(x) = sign(x(,)+xw-3)

迭代过程见表2.1.

表2.1例2.1求解的迭代过程

迭代次数    误分类点    w    b    w.x+b

012345678



^rv^l/of^wlf1)T (3(2c-(o,(3,(2,c-o



这是在计算中误分类点先后取，JC3，X3，X,，X, , &，4得到的分离超平面和感 知机模型.如果在计算中误分类点依次取^1，々，;^，巧,七,*3，七，;^,%1，七，;^，那 么得到的分离超平面是2x(1) +/2)-5 = 0.

可见，感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以 不同.

2.3.2算法的收敛性

现在证明，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限 次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型.

为了便于叙述与推导，将偏置6并入权重向量w,记作w=(wt,6)t，同样也将 输入向量加以扩充，加进常数1，记作i = (xT，l)T.这样，xgR"+1, weR"+1.显 然，w-x = w>x + b .

定理2.1 (Novikoff)设训练数据集7 = {(久只)，(;^2)，".，氏,;^)}是线性 可分的，其中 x, e；V = R”，yiey = {-l,+l}, i=l,2,-,N, BO

(1)    存在满足条件1^1 = 1的超平面wopt.x = WopI.x+6opt=0将训练数据集完 全正确分开：且存在y>0：对所有i' = l，2,…，

-xj =    .X, +bopt')^r    (2.8)

(2)    令7? = ^||和|，则感知机算法2.1在训练数据集上的误分类次数满足 不等式

证明(1)由于训练数据集是线性可分的，按照定义2.2,存在超平面可将训 练数据集完全正确分开，取此超平面为！^+    使|<| = 1.由

于对有限的f = l，2，.",7V,均有

少凡：•xi) = yi(wopt.xl +*呼)>0

所以存在

X=nun{^(wOI)t-x,+6op,)}

使

只(氏p. •足)=y,(wopt-xi+6opt)^z

(2)感知机算法从汍=0开始，如果实例被误分类，则更新权重.令丸_,是

第A;个误分类实例之前的扩充权重向量，即

KwU卜,)T

则第fc个误分类实例的条件是

若(Wj是被化误分类的数据，则冰和&的更新是

w* <-w*-i+W/ bk<-bk-i+ny,

即

氏=氏-|+喊

下面推导两个不等式：

⑴

由式(2.11)及式(2.8)得

A •必呻=K +

由此递推即得不等式(2.12)

A •    > O喇 +7Z>w*_2 •必呼+2tjy»k”r

■    \M^krj2R2

由式(2,11)及式(2.10)得

ip*ii2=m+27jA-i-^+^hir 叫 im2+”w

^k-2||2+2^/?2^...

^krj2R2

(2.10)

(2.11)

(2.12)

(2.13)



结合不等式(2.12)及式(2.13)即得

krjr ＜也• ＜叫|必41卜呼卜 k2f^kR2

定理表明，误分类的次数灸是有上界的，经过有限次搜索可以找到将训练数 据完全正确分开的分离超平面.也就是说，当训练数据集线性可分时，感知机学 习算法原始形式迭代是收敛的.但是例2.1说明，感知机学习算法存在许多解，这 些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序.为了得到 唯一的超平面，需要对分离超平面增加约束条件.这就是第7章将要讲述的线性 支持向量机的想法.当训练集线性不可分时，感知机学习算法不收敛，迭代结果 会发生震荡.

2.3.3感知机学习算法的对偶形式

现在考虑感知机学习算法的对偶形式.感知机学习算法的原始形式和对偶形式 与第7章中支持向量机学习算法的原始形式和对偶形式相对应.

对偶形式的基本想法是，将w和*表示为实例;c,和标记只的线性组合的形 式，通过求解其系数而求得W和* .不失一般性，在算法2.1中可假设初始值w0,b0 均为0.对误分类点通过

w<—

b<-b+Jjyt

逐步修改w,h，设修改”次，则W，*关于(xpZ)的增量分别是和这里 «； = ntrj.这样，从学习过程不难看出，最后学习到的w，6可以分别表示为

N

C2.14)

i=i

N

b = ^y>    (2.15)

这里，巧彡0, i = l,2,-,N,当？7 = 1时，表示第Z•个实例点由于误分而进行更新 的次数.实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分 类.换句话说，这样的实例对学习结果影响最大.

下面对照原始形式来叙述感知机学习算法的对偶形式.

算法2.2 (感知机学习算法的对偶形式)

输入：线性可分的数据集/^似，^)，^^^)，…,^，;^)}，其中x,sR"，乃e {-!,+!}» : = 1，2, "，JV;学习率z； (O<77^1)；

输出：a,b；感知机模型/(x) = sign



,Xj-x+b .



其中 a=(«i,a2，"，aw)T. (1) a<-0, 6<-0



(2)在训练集中选取数据(x,，＞0



![img](2012.4e2a-14.jpg)



(3)如果y,    x'.x, + 6 <0



a, a,+tj b^-b + rjy!



(4)转至(2)直到没有误分类数据.    ■

对偶形式中训练实例仅以内积的形式出现.为了方便，可以预先将训练集中

实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵



G =[x, •xJ\NxN



例2.2数据同例2.1，正样本点是x, =(3,3)t，x2=(4,3)t，负样本点是Xj = (1,1)T,试用感知机学习算法对偶形式求感知机模型.

解按照算法2.2，

(1)    取％=0, / = 1,2,3» 6 = 0,叮=1

(2)    计算Gram矩阵



•18 21 6" 21 25 7 6    7    2



(3)误分条件



![img](2012.4e2a-15.jpg)





参数更新



a, <-«;.+!, b^-b + y,



(4)    迭代.过程从略，结果列于表2.2.

(5)



w = 2x, + 0x2 - 5x3 = (1,1)T 6 = -3



分离超平面



xo)+x(2)_3 = 0



| 感知机模型 | /(x) = sign(xm+x<2)-3) | ■    |      |      |      |      |      |
| ---------- | ---------------------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 表2.2      | 例2.2求解的迭代过程    |      |      |      |      |      |      |
| k          | 0                      | 1 2  | 3    | 4    | 5    | 6    | 7    |
|            |                        | X[   | A    | X}   | Xl   |      | *3   |
|            | 0                      | 1 1  | 1    | 2    | 2    | 2    | 2    |
|            | 0                      | 0 0  | 0    | 0    | 0    | 0    | 0    |
|            | 0                      | 0 1  | 2    | 2    | 3    | 4    | 5    |
| b          | 0                      | 1 0  | -1   | 0    | -1   | -2   | -3   |

对照例2.1,结果一致，迭代步骤也是互相对应的.

与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解.

###### 本章概要

1.感知机是根据输入实例的特征向量;c对其进行二类分类的线性分类模型:

/(x) = sign(w >x + b)

感知机模型对应于输入空间(特征空间)中的分离超平面w.x + 6 = 0.

2.感知机学习的策略是极小化损失函数：

min £( w, Z») = - ^ y, (w • ^/ + *)

损失函数对应于误分类点到分离超平面的总距离.

\3.    感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有 原始形式和对偶形式.算法简单且易于实现.原始形式中，首先任意选取一个超 平面，然后用梯度下降法不断极小化目标函数.在这个过程中一次随机选取~个 误分类点使其梯度下降.

\4.    当训练数据集线性可分时，感知机学习算法是收敛的.感知机算法在训 练数据集上的误分类次数it满足不等式：

当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同 的初值或不同的迭代顺序而可能有所不同.

###### 继续阅读

感知机最早在 1957 年由 Rosenblatt 提出Novikoff[2]，Minsky 与 Papert[3] 等人对感知机进行了一系列理论研究.感知机的扩展学习方法包括口袋算法 (pocket algorithm) t4\ 表决感知机(voted perception) 带边缘感知机(perceptron with margin)[6].关于感知机的介绍可进一步参考文献[7, 8].

习 题

2.1 Minsky与Papert指出：感知机因为是线性模型，所以不能表示复杂的函数， 如异或(XOR).验证感知机为什么不能表示异或.

2.2模仿例题2.1，构建从训练数据集求解感知机模型的例子.

2.3证明以下定理：样本集线性可分的充分必要条件是正实例点集所构成的凸壳® 与负实例点集所构成的凸壳互不相交.

###### 参考文献

[1]    Rosenblatt F. The Perceptron: A probabilistic model for information storage and organization in the Brain. Cornell Aeronautical Laboratory. Psychological Review, 1958,65 (6): 386-408

[2]    Novikoff AB. On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, Polytechnic Institute of Brooklyn, 1962, 12, 615-622

[3]    Minsky ML, Papert SA. Perceptrons. Cambridge, MA: MIT Press. 1969

[4]    Gallant SI. Perceptron-based learning algoritiuns. IEEE Transactions on Neural Networks, 1990,

1(2): 179-191

[5]    Freund Y, Schapire RE. Large margin classification using the perceptron algorithm. In:

Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT，98).

ACM Press, 1998

[6]    Li YY, Zaragoza H, Herbrich R, Shawe-Taylor J, Kandola J. The Perceptron algorithm with uneven margins. In: Proceedings of the 19th International Conference on Machine Learning. 2002,379-386

[7]    Widrow B, Lehr MA. 30 years of additive neural networks: Perceptron, madaline, and backpropagation. Proc. IEEE, 1990,78(9): 1415-1442

[8]    Cristianioi N, Shawe-Taylor J. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000

②设集合5 c R”是由R”中的*个点所组成的集合，即S =    定义S的凸壳conv(S)为

conv(S) = |x=X^,-|SA
