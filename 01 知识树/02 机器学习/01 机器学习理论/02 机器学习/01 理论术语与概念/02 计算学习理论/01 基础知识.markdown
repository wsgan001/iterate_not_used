---
author: evo
comments: true
date: 2018-05-24 11:51:03+00:00
layout: post
link: http://106.15.37.116/2018/05/24/ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86/
slug: ml-%e8%ae%a1%e7%ae%97%e5%ad%a6%e4%b9%a0%e7%90%86%e8%ae%ba-%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86
title:
wordpress_id: 6555
categories:
- 人工智能学习
tags:
- Machine Learning
---

<!-- more -->

[mathjax]

**注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。**


# ORIGINAL






  1. 《机器学习》周志华




# TODO






  * aaa





* * *





# INTRODUCTION






  * aaa




# 介绍一下计算学习




## 什么是计算学习理论？


计算学习理论 (computational learning theory) 研究的是关于通过 “计算 ” 来进行 “学习” 的理论，即关于机器学习的理论基础。

其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。**嗯利害，关于计算学习有更系统的书吗？想系统总结下。**



给定样例集 \(D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}，\(x_i\in \mathcal{X}\) ，本章主要讨论二分类问题，若无特别说明，\(y_i\in \mathcal{Y}=\{-1,+1\}\) 。假设 \(X\) 中的所有样本服从一 个隐含未知的分布 \(\mathcal{D}\) ， \(D\) 中所有样本都是独立地从这个分布上采样而得，即独立同分布(independent and identically distributed，简称 i.i.d. ) 样本。**如果没有这个独立同分布的条件会怎样？**

令 \(h\) 为从 \(\mathcal{X}\) 到 \(\mathcal{Y}\) 的一个映射，其泛化误差为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fljIe55lIA.png?imageslim)

y) , (12.1)

>1 m

E(h; D) = - 53(心)一讲). (12.2)

i=l

由于p是r的独立同分布采样，因此的经验误差的期望等于其泛化误 差.在上下文明确时，我们将五(Zi;P)和S(h;D)分别简记为五㈨和E(h).令 e为E{h)的上限，即E{h) e;我们通常用e表示预先设定的学得模型所应满 足的误差要求,亦称“误差参数”.

本章后面部分将研究经验误差与泛化误差之间的逼近程度.若Zi在数据集 D上的经验误差为0,则称h与D 一致，否则称其与D不一致.对任意两个映 射如，如e疋4 乂可通过其“不合”(disagreement)来度量它们之间的差别：

= Px^h^x) 如(®)) • (12.3)

我们会用到几个常用不等式：

• Jensen不等式：对任意凸函数/(re),有

/(E⑷)(，⑷)•

(12.4)

Hoeffding不等式[Hoeffding, 1963]:若列,扔,…，为m个独立随机变 量，且满足0 < % < 1,则对任意e〉0,有

(I m 1 m \

P ( rn 52 _ W E(^)e I exp(-2me2),

e ] 2exp(—2me2).

2=1

171 2=1

lib i "Cz

2=1 2=1

(12.5)

(12.6)

McDiarmid 不等式[McDiarmid, 1989]:若 吻,...,xm 为 m 个独立随 机变量，且对任意1 < < < m，函数/满足 SUp \f ，. • •，— f (*^1，• • •，1)怎i+1, • • • , *^m) | , ①1 ,•••，況m, 则对任意e〉0,有 P (/ (以，…，a;m) - E (/ (町，…，xm)) > e) exp (^^)，(12.7) e) 2exp ^^^2^ - (12.8)





















* * *





# COMMENT
