# 有限假设空间



TODO

- 这一章也没有整理




12.3有限假设空间
12.3.1可分情形

可分情形意味着目标概念c属于假设空间W，B卩c e    给定包含m个样

例的训练集D，如何找出满足误差参数的假设呢？

容易想到一种简单的学习策略：既然I?中样例标记都是由目标概念c赋予 的，并且C存在于假设空间？/中，那么，任何在训练集.1?上出现标记错误的假 设肯定不是目标概念C.于是，我们只需保留与D —致的假设，剔除与D不一 致的假设即可.若训练集D足够大，则可不断借助I?中的样例剔除不一致的假 设，直到丸中仅剩下一个假设为止，这个假设就是目标概念G通常情形下，由 于训练集规模有限,假设空间并中可能存在不止一个与I? 一致的“等效”假 设,对这些等效假设，无法根据D来对它们的优劣做进一步区分.

到底需多少样例才能学得目标概念C的有效近似呢？对PAC学习来说，只 要训练集79的规模能使学习算法£以概率1 - 5找到目标假设的e近似即可.

我们先估计泛化误差大于e但在训练集上仍表现完美的假设出现的概率. 假定/I的泛化误差大于e,对分布:P上随机采样而得的任何样例什，有

P(h(x)=y) = l-P(h(x)^y)
=1-雌)

< 1-e .    (12.10)
由于D包含m个从P独立同分布采样而得的样例，因此，h与D表现一 致的概率为

P((九(®i) = 2/1)A ... A (Zi(a?m) = ym)) = (l-P(h(x)
< (1 - e)m .    (12.11)
我们事先并不知道学习算法会输出对中的哪个假设，但仅需保证泛化 误差大于e,且在训练集上表现完美的所有假设出现概率之和不大于5即可：

P(hen：丑⑻〉e 八 E{h) = 0)< 間(1 - e)m
< \H\e~m€ ,    (12.12)
令式(12.12)不大于d,即
\n\e~me S 5    (12.13)
可得

m    (In \H\ + In —).    (12.14)
由此可知，有限假设空间丸都是PAC可学习的，所需的样例数目如 式(12.14)所示，输出假设/I的泛化误差随样例数目的増多而收敛到0,收敛速 率为0(^).

12.3.2不可分情形

对较为困难的学习问题，目标概念0往往不存在于假设空间W中.假定对 于任何hen, E(h) 0,也就是说，宄中的任意一个假设都会在训练集上出现 或多或少的错误.由Hoeffding不等式易知：

引理12.1若训练集D包含m个从分布P上独立同分布采样而得的样 例，0 < e < 1,则对任意heH,有

P(E(h) - E(h) > e)彡 exp(-2me2) 5    (12.15)

P{E{h) - E(h)》e) < exp(-2me2) ,    (12.16)

P(|五(")-五(")|》e) < 2exp(—2me2) .    (12.17)

推论12.1若训练集D包含m个从分布P上独立同分布采样而得的样 例，0 < e < 1，则对任意heK式(12.18)以至少1 — 5的概率成立：

忌㈨-宏(h) +    •    (12.18)

推论12.1表明，样例数目m较大时，/i的经验误差是其泛化误差很好的近 似.对于有限假设空间丸，我们有

定理12.1若n为有限假设空间，0 < d < 1，则对任章hen,有

P(\E(h)- E(h)\    1 - <5 •    (12-19)

证明令加,h2,...,hw表示假设空间宄中的假设，有

P(3hen： \E(h)-E(h)\ >e)

=P^(\Ehl - E/ll | > e) V ... V (\Ehm ~Ehw \ > e)^)

hen

由式(12.17)可得

5^ P(|E(/i) — E(h)\ > e) < 2\H\exp(—2me2) 5 hen •

于是，令(J = 2|7£|exp(—2me2)即可得式(12.19).

即在宄的所有假设中找 出最好的一个.


显然，当0穿并时5学习算法£无法学得目标概念6的€近似.但是，当 假设空间并给定时，其中必存在一个泛化误差最小的假设，找出此假设的e 近似也不失为一个较好的目标.対中泛化误差最小的假设是argmii^g坷h)， 于是，以此为目标可将PAC学习推广到c拿W的情况，这称为“不可知学 习”(agnostic learning).相应的，我们有

定义 12.5 不可知 PAC 可学习(agnostic PAC learnable):令 m 表 示从分布P中独立同分布采样得到的样例数目，0 < 6, d < 1,对所 有分布2?，若存在学习算法£和多项式函数polyG，•,•，•)，使得对于任何 poly(l/e, l/^size(«)5size(c)), £能从假设空间W中输出满足式(12.20)的

假设/I:

P(五⑻—忠 E(h，、彡 e)彡 1 - d ,    (12.20)

则称假设空间兴是不可知PAC可学习的.

与PAC可学习类似，若学习算法公的运行时间也是多项式函数 poly(l/e, 1/4 size⑻，size(c))，则称假设空间対是高效不可知PAC可学习 的，学习算法公则称为假设空间宄的不可知PAC学习算法，满足上述要求的 最小m称为学习算法£的样本复杂度.

1




## REF

1. 《机器学习》周志华
