# PCA 学习

TODO

- 这一章也没有整理。



12.2 PAC学习

计算学习理论中最基本的是概率近似正确(Probably Approximately Correct,简称PAC)学习理论[Valiant, 1984]. “概率近似正确”这个名字 看起来有点古怪,我们稍后再解释.

令c表示“概念” (concept),这是从样本空间1到标记空间J的映射，它 决定示例:r的真实标记2/，若对任何样例Or,?/)有= ^/成立，则称c为目 标概念；所有我们希望学得的目标概念所构成的集合称为“概念类”(concept class),用符号C表示.

学习算法公的假设空间 不是1.3节所讨论的学习 任务本身对应的假设空间.


给定学习算法公，它所考虑的所有可能概念的集合称为“假设空 间”(hypothesis space),用符号表示.由于学习算法事先并不知道概念 类的真实存在，因此7Z和C通常是不同的，学习算法会把自认为可能的目标概

念集中起来构成況，对& e宄，由于并不能确定它是否真是目标概念，因此称为 “假设”(hypothesis).显然，假设九也是从样本空间Y到标记空间0；的映射.

若目标概念c G丸，则況中存在假设能将所有示例按与真实标记一致的方 式完全分开，我们称该问题对学习算法£是“可分的”(separable)，亦称“一 致的”(consistent);若c矣H，则沢中不存在任何假设能将所有示例完全正 确分开，称该问题对学习算法£是“不可分的” (non-separable),亦称“不一 致的” (non-consistent).

参见1.4节.


一般来说，训练样例越 少，采样偶然性越大.


给定训练集P，我们希望基于学习算法£学得的模型所对应的假设/I尽可 能接近目标概念匕读者可能会间：为什么不是希望精确地学到目标概念0呢? 这是由于机器学习过程受到很多因素的制约，例如我们获得的训练集I?往往仅 包含有限数量的样例，因此，通常会存在一些在I?上“等效”的假设，学习算 法对它们无法区别；再如，从分布P采样得到I?的过程有一定偶然性，可以想 象即便对同样大小的不同训练集，学得结果也可能有所不同.因此，我们是希 望以比较大的把握学得比较好的模型，也就是说，以较大的概率学得误差满足 预设上限的模型；这就是“概率” “近似正确”的含义.形式化地说，令5表示 置信度，可定义：

定义 12.1 PAC 辨识(PAC Identify):对 0 < €，d < 1，所有 cEC 和分布 P，若存在学习算法氡其输出假设/i e対满足
P(E(h) < e) > 1 — d，    (12.9)
则称学习算法il能从假设空间対中PAC辨识概念类C.

这样的学习算法£能以较大的概率(至少1 -幻学得目标概念c的近似 (误差最多为e).在此基础上可定义：

样例数目m与误差e、 置信度<5、数据本身的复 杂度size(sc)、目标概念的 复杂度size⑷都有关.


定义12.2 PAC可学习(PAC Learnable):令m表示从分布D中独立同 分布采样得到的样例数目，0 < < 1，对所有分布P，若存在学习算法£和多 项式函数 poly(-5 •，•，•)，使得对于任何 m 彡 poly(l/e51/5, size⑻，size(c)), £ 能 从假设空间丸中PAC辨识概念类C，则称概念类C对假设空间并而言是PAC 可学习的，有时也简称概念类C是PAC可学习的.

对计算机算法来说,必然要考虑时间复杂度，于是：

定义12.3 PAC学习算法(PAC Learning Algorithm):若学习算法£使 概念类C为PAC可学习的，且;C的运行时间也是多项式函数poly(l/e,lA size(£c),size(c)),则称概念类 C 是高效 PAC 可学习(efficiently PAC learnable) 的，称为概念类C的PAC学习算法.

假定学习算法处理每个样本的时间为常数，则£的时间复杂度等价于样 本复杂度.于是，我们对算法时间复杂度的关心就转化为对样本复杂度的关心：

定义12.4样本复杂度(Sample Complexity):满足PAC学习算法41所 需的m > poly(l/e5 l/d3size(aj),size(c))中最小的m,称为学习算法£的样本 复杂度.

显然，PAC学习给出了一个抽象地刻画机器学习能力的框架，基于这个框 架能对很多重要问题进行理论探讨，例如研究某任务在什么样的条件下可学得 较好的模型？某算法在什么样的条件下可进行有效的学习？需多少训练样例才 能获得较好的模型？

.PAC学习中一个关键因素是假设空间7/的复杂度.宄包含了学习算法£ 所有可能输出的假设，者在PAC学习中假设空间与概念类完全相同，即并= 这称为“恰PAC可学习”(properly PAC learnable);直观地看,这意味着学习 算法的能力与学习任务“恰好匹配”.然而，这种让所有候选假设都来自概念 类的要求看似合理，但却并不实际，因为在现实应用中我们对概念类C通常一 无所知，更别说获得一个假设空间与概念类恰好相同的学习算法.显然，更重要 的是研究假设空间与概念类不同的情形，即況一 a —般而言，并越大5其包含 任意目标概念的可能性越大，但从中找到某个具体目标概念的难度也越大.間 有限时，我们称并为“有限假设空间”，否则称为“无限假设空间”.



## REF

1. 《机器学习》周志华
