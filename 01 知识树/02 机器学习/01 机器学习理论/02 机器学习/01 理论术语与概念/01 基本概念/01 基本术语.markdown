# 基础术语


TODO

* <span style="color:red;">还是有些散，要重新整理一下。</span>


OK，这里，我们介绍一些机器学习的术语


## 模型

一般用 “模型” 泛指从数据中学得的结果。有的文章使用 “模型” 来指代全局性的结果（比如一棵决策树），而用 “模式” 来指代指局部性结果（例如一条规则）。<span style="color:red;">还是有些不清楚，要确认下。</span>


## 数据集、记录、示例、样本


要进行机器学习，首先要有数据。

例如假定我们收集了一批关于西瓜的数据：

* (色泽=青绿；根蒂=蜷缩；敲声=浊响)
* (色泽=乌黑；根蒂=稍蜷；敲声=沉闷)
* (色泽=浅白；根蒂=硬挺；敲声=清脆)
* ……


那么这里的每对括号里面就是一条记录，这组记录的集合称为一个 “数据集”  (data set)，其中每条记录是关于一个事件或对象 (这里是一个西瓜) 的描述，称为一个 “示例” (instance) 或 “样本”  (sample)。

注意：有的时候整个数据集会被称为一个 “样本”，因为它可看作对样本空间的一个采样，我们可以通过上下文来判断 “样本” 是指单个示例还是数据集。**会****有这样的情况吗？**

## 属性、特征、属性值、样本空间

这里的 “色泽”、“根蒂”、 “敲声”，等，都是反应样本在某方面的表现或性质的事项，这些被称为 “属性” (attribute) 或 “特征”  (feature)。

属性上的取值，例如 “青绿”、 “乌黑”，被称为 “属性值” (attribute value)。

属性张成的空间称为 “属性空间” (attribute space)、“样本空间” (sample space) 或“输入空间”。例如我们把“色泽”、 “根蒂”、 “敲声”作为三个坐标轴，则它们张成 一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置。由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个 “特征向量” (feature vector)。<span style="color:red;">嗯，也就是说一个样本可以看作一个特征向量。</span>

## 数据集的表示、示例的表示、属性的表示、维数

一般地，令 $D =\{x_1,x_2,\cdots ,x_m\}$  表示包含m个示例的数据集，每个示例由 $d$ 个属性描述 (例如上面的西瓜数据使用了 $3$ 个属性)，则每个示例 $x_i =(x_{i1};x_{i2};\cdots ;x_{id};)$ 是 $d$ 维样本空间 $\mathcal{X}$ 中的一个向量，$x_i\in \mathcal{X}$ 其中 $x_{ij}$ 是 $x_i$ 在第 $j$ 个属性上的取值(例如上述第 $3$ 个西瓜在第$2$个属性上的值是“硬挺”)，$d$ 称为样本 $x_i$ 的 “维数” (dimensionality)。


## 学习、训练、训练集

从数据中学得模型的过程称为 “学习” (learning) 或 “训练” (training)， 这个过程通过执行某个学习算法来完成。训练过程中使用的数据称为 “训练数据” (training data) ，其中每个样本称为一个 “训练样本” (training sample)，训练样本组成的集合称为 “训练集”  (training set)。


## 假设、真实、学习器

学得模型对应了关于数据的某种潜在的规律，因此亦称 “假设” (hypothesis)。这种潜在规律自身，则称为 “真相” 或 “真实” (ground-truth)，学习过程就是为了找出或逼近真相。

有的时候将模型称为 “学习器” (learner)，可看作是学习算法在给定数据和参数空间上的实例化，因为学习算法通常有参数需要设置，使用不同的参数值、训练数据，将产生不同的模型。

## 标记、样例

如果希望学得一个能帮助我们判断没剖开的是不是 “好瓜” 的模型，仅有前面的示例数据显然是不够的。要建立这样的关于 “预测” (prediction) 的模型，我们需获得训练样本的 “结果 ”信息，例如：

* ((色泽=青绿;根蒂=蜷缩;敲声=浊响)，好瓜)

这里的关于示例结果的信息 “好瓜” 就被称为 “标记” (label)。（将“label”译为 “标记” 而非 “标签”，是考虑到英文中 “label” 既可用作名词、也可用作动词）

而拥有了标记信息的示例，则称为 “样例” (example)。若将标记看作对象本身的一部分，则 “样例” 有时也称为 “样本” 。**注意这里关于样例和样本的说明。**


## 样例的表示、标记空间


—般地，用 $(x_i,y_i)$ 表示第 i 个样例，其中 $y_i\in \mathcal{Y}$ 是示例 $x_i$ 的标记，$\mathcal{Y}$ 是所有标记的集合，亦称 “标记空间” (label space) 或 “输出空间”。


## 分类、回归


如果我们想要预测的值是：

* 离散值，例如 “好瓜”、“坏瓜” ，则此类学习任务称为 “分类” (classification)
* 连续值，例如西瓜成熟度 0.95、0.37，则此类学习任务称为 “回归” (regression)

二分类、多分类

对只涉及两个类别的 “二分类” (binary classification) 任务，通常称其中一个类为 “正类” (positive class)， 另一个类为 “反类” (negative class) 或 “负类” 。

涉及多个类别时，则称为 “多分类” (multi-class classification) 任务。


## 预测任务的表示、输入空间、输出空间

一般地，预测任务是希望通过对训练集 ${(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}$ 进行学习，建立一个从输入空间 $\mathcal{X}$ 到输出空间 $\mathcal{Y}$ 的映射 $f:\mathcal{X}\mapsto \mathcal{Y}$。


* 对于分类任务
    * 二分类任务，通常令 $\mathcal{Y}=\{-1,+1\}$ 或 $\{0,1\}$。
    * 多分类任务 $|\mathcal{Y}|>2$ 。**这个是什么意思？**
* 对于回归任务，$\mathcal{Y}=\mathbb{R}$，$\mathbb{R}$ 为实数集。




## 测试、测试样本、预测标记

学得模型后，使用其进行预测的过程称为 “测试” (testing)，被预测的样本称为 “测试样本” (testing sample)，亦称 “测试示例” (testing instance) 或 “测试例” 。例如：

在学得 $f$ 后，对测试例 $\mathbf{x}$ 我们可以得到它的预测标记 $y=f(\mathbf{x})$。

## 聚类

我们还可以对西瓜做 “聚类” (clustering)，即将训练集中的西瓜分成若干组，每组称为一个 “簇” (cluster)，这些自动形成的簇可能对应一些潜在的概念划分，例如 “浅色瓜” 、“深色瓜”，甚至 “本地瓜” 、“外地瓜”。

这样的学习过程有助于我们了解数据内在的规律，能为更深入地分析数据建立基础。

需说明的是，在聚类学习中，“浅色瓜” 、“本地瓜”这样的概念我们事先是不知道的，而且学习过程中使用的训练样本通常不拥有标记信息，否则标记信息直接形成了簇划分，但也有例外情况，参见13.6节。<span style="color:red;">什么例外情况？要补充到这里。</span>

<span style="color:blue;">在上kaggle 的课程的时候，看到了一个例子，就是想把两个表拼在一起的时候，一个是每5分钟统计的，一个是每1小时统计的，这时候他使用了聚类来把这个1小时统计的都换成了每5分钟统计的。</span>

<span style="color:red;">嗯，这个后续再补充下。</span>

## 监督学习、无监督学习

根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：

* 监督学习 (supervised learning)
* 无监督学习 (unsupervised learning)

分类和回归是前者的代表，而聚类则是后者的代表。

<span style="color:red;">说实话，对于回归这个词还是有点心理忐忑，想更理解回归一些。</span>


## 泛化

需注意的是，机器学习的目标是使学得的模型能很好地适用于 “新样本”，更确切地说，是 “未见示例” (unseen instance)， 而不是仅仅在训练样本上工作得很好。即便对聚类这样的无监督学习任务，我们也希望学得的簇划分能适用于没在训练集中出现的样本。

学得模型适用于新样本的能力，称为 “泛化” (generalization) 能力。具有强泛化能力的模型能很好地适用于整个样本空间。于是，尽管训练集通常只是样本空间的一个很小的采样，我们仍希望它能很好地反映出样本空间的特性，否则就很难期望在训练集上学得的模型能在整个样本空间上都工作得很好。<span style="color:red;">嗯。</span>

## 独立同分布

通常假设样本空间中全体样本服从一个未知 “分布” (distribution)  $\mathcal{D}$ ，我们获得的每个样本都是独立地从这个分布上采样获得的，即 “独立同分布” (independent and identically distributed，简称 i.i.d. )。

一般而言，训练样本越多，我们得到的关于  $\mathcal{D}$ 的信息越多，这样就越有可能通过学习获得具有强泛化能力的模型。 <span style="color:red;">为什么说，训练样本越多，我们得到的关于 $\mathcal{D}$ 的信息越多？</span>

<span style="color:red;">关于这个独立同分布，现实中真的有这样的基本假设存在吗？机器学习中的那些事基于这个独立同分布的？如果不满足独立同分布，是不是有些机器学习算法那就不能使用了？到底怎么判断一些东西是不是独立同分布？关于这个还是不是很清楚。</span>


# REF

1. 《机器学习》周志华
