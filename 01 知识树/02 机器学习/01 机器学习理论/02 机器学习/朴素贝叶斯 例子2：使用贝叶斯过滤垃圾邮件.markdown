---
author: evo
comments: true
date: 2018-05-08 13:00:36+00:00
layout: post
link: http://106.15.37.116/2018/05/08/nb-sample2/
slug: nb-sample2
title:
wordpress_id: 5438
categories:
- 随想与反思
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL






  1.


[第4章 基于概率论的分类方法：朴素贝叶斯](http://ml.apachecn.org/mlia/naive-bayes/)







# TODO






  * **再仔细看下**




# MOTIVE






  * aaa





* * *





# 项目场景


进行电子邮件垃圾过滤。


# 项目要求


进行电子邮件垃圾过滤是朴素贝叶斯的最著名的一个应用。实际上就是使用朴素贝叶斯对电子邮件进行分类。

使用朴素贝叶斯对电子邮件进行分类

完成朴素贝叶斯的一个最著名的应用:。


# 项目数据


链接：https://pan.baidu.com/s/1CNCeOdKcR2ugAU5MXKLp6A 密码：vqaq

里面包含了一些以txt文档形式存放的邮件，ham 文件夹里存放的是正常的邮件，spam 文件夹里村发给的是垃圾邮件。

邮件内容类似：


    Hi Peter,

    With Jose out of town, do you want to
    meet once in a while to keep things
    going and do some interesting stuff?

    Let me know
    Eugene





# 完整代码




    import numpy as np
    import codecs


​
​
    # 将文本内容切分成 word list
    def text_parse(content):
        import re
        list_of_words = re.split(r'\W+', content)
        return [tok.lower() for tok in list_of_words if len(tok) > 2]


​
    # 根据样本中的词汇创建词汇表
    def create_vocabulary_list(data_set):
        vocabulary_set = set([])  # create empty set
        for document in data_set:
            vocabulary_set = vocabulary_set | set(document)  # union of the two sets
        return list(vocabulary_set)


​
​
    #将文本转化成向量
    def convert_doc_to_vec(vocabulary_list, doc):
        doc_vec = [0] * len(vocabulary_list)
        for word in doc:
            if word in vocabulary_list:
                doc_vec[vocabulary_list.index(word)] += 1
        return doc_vec


​
    # 计算出概率值
    def calculate_probility(vec_data, label):
        doc_num = len(vec_data)
        word_num = len(vec_data[0])
        p_1 = sum(label) / float(doc_num)  # 正样本出现概率，sum(label)即其中所有的 1 的个数，
        vec_0 = np.ones(word_num)  # 构造单词出现次数列表 之所以不是zeros 是因为为了应对有生词出现的情况
        vec_1 = np.ones(word_num)
        word_num_0 = 2.0  # 整个数据集单词出现总数   为什么不是0？
        word_num_1 = 2.0

        for i in range(doc_num):
            if label[i] == 1:
                vec_1 += vec_data[i]  # 如果是正样本，对正样本的向量进行加和
                word_num_1 += sum(vec_data[i])  # 计算所有正样本中出现的单词总数   为什么要算这个？为了后面normalize吗？
            else:
                vec_0 += vec_data[i]
                word_num_0 += sum(vec_data[i])

        p_vec_0 = np.log(vec_0 / word_num_0)
        p_vec_1 = np.log(vec_1 / word_num_1)  # 即在正样本类别下，每个单词出现的概率   为什么使用log()？
        return p_vec_0, p_vec_1, p_1


​
    def classify_NB(test_vec, p_vec_not_abusive, p_vec_abusive, p_abusive):
        p1 = sum(test_vec * p_vec_abusive) + np.log(p_abusive)  # element-wise mult
        p0 = sum(test_vec * p_vec_not_abusive) + np.log(1.0 - p_abusive)
        if p1 > p0:
            return 1
        else:
            return 0

    if __name__=="__main__":
        # 将文本提取出来
        doc_list = []
        class_list = []
        for i in range(1, 26):
            word_list = text_parse(open('email/spam/%d.txt' % i,'r').read())
            doc_list.append(word_list)
            class_list.append(1)
            word_list = text_parse(open('email/ham/%d.txt' % i,'r').read())
            doc_list.append(word_list)
            class_list.append(0)
        # 创建词汇表
        vocabulary_list = create_vocabulary_list(doc_list)

        # 创建出训练集和测试集
        train_set = list(range(50))  # 创建训练集
        test_set = []  # 创建测试集
        for i in range(20):
            rand_index = int(np.random.uniform(0, len(train_set)))#随机产生一个索引值
            test_set.append(train_set[rand_index])#从训练集中取出一个到测试集里
            del (train_set[rand_index])
        # 获得训练集的数据向量和label
        train_mat = []
        train_label = []
        for doc_index in train_set:  # train the classifier (get probs) trainNB0
            train_mat.append(convert_doc_to_vec(vocabulary_list, doc_list[doc_index]))
            train_label.append(class_list[doc_index])
        # 计算后面要用到的概率
        p_vec_0, p_vec_1, p_1 = calculate_probility(np.array(train_mat), np.array(train_label))
        print(p_1)
        # 进行测试
        error_count = 0
        for doc_index in test_set:
            word_vector = convert_doc_to_vec(vocabulary_list, doc_list[doc_index])
            if classify_NB(np.array(word_vector), p_vec_0, p_vec_1, p_1) != class_list[doc_index]:
                error_count += 1
                print("classification error", doc_list[doc_index])
        print('the error rate is: ', float(error_count) / len(test_set))


输出：


    0.466666666667
    classification error ['experience', 'with', 'biggerpenis', 'today', 'grow', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', 'ma1eenhancement', 'supplement', 'trusted', 'millions', 'buy', 'today']
    the error rate is:  0.05


没想到，朴素贝叶斯处理这个精确度都这么高，我这里面设的是30个作为train，20个作为test的，错误率只有0.05











* * *





# COMMENT
