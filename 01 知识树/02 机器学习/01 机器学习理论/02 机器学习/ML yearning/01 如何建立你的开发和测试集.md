## Chapter 5、Your development and test sets

**您的开发和测试集**
​	让我们回到我们早期猫图片的那个例子：你运行一个移动app，用户正在上传很多不同事物的图片到该app中。你想自动找到猫的图片。
​	您的团队通过从不同网站上下载猫（positive examples，正样本）和非猫（negative examples，负样本）的图获得一个大的训练集。 他们将数据集按照比例70％/ 30％分成训练集/测试集。 使用这些数据，他们构建了一个在训练集和测试集上都表现很好的的猫检测器。
​	但是当你将这个分类器部署到移动app时，你发现表现真的很糟糕！


​	发生了什么？
​	您发现用户上传的图片与您构建训练集的网站图片有所不同：用户上传的照片使用手机拍摄，这些照片往往分辨率较低，比较模糊，并且采光不好。 由于您的训练集/测试集是由网站图片构建的，您的算法没有很好的兼顾到你所关心的智能手机图片的实际分布。
​	在大数据的时代之前，在机器学习中使用随机的70％/ 30％来分割训练集和测试集是常见的规则。 这种做法可以工作，但在越来越多的应用程序，如训练集的分布（上面例子中的网站图像）不同于你最终关心的分布（手机图像），这是一个坏主意。

​	我们通常定义：

- 训练集 - 学习算法运行在这上面。

- Dev（开发）集 - 用于调整参数，选择特征，以及对学习算法做出其他决定。 有时也称为维持交叉验证集(hold-out cross validation set)。

- 测试集 - 用于评估算法的性能，但不要做出关于使用什么学习算法或参数的任何决定。

  ​你定义一个开发集和测试集，你的团队会尝试很多想法，如不同的学习算法参数，看看什么是最好的。 开发集和测试集能够使你的团队快速看到你的算法做得有多好。

  ​换句话说，开发和测试集的目的是指导你的团队对机器学习系统进行最重要的更改。
  ​所以，你应该做如下事情：

- 选择开发和测试集，以反映您期望在未来获得的数据，并希望做好。

  ​换句话说，您的测试集不应该只是可用数据的30％这么简单，特别是如果您期望您的未来数据（移动app图片）在性质上与您的训练集（网站图像）不同时。

  ​如果您尚未启动移动app，可能还没有任何用户，因此可能无法获取准确反映您未来需要做的更好的数据。 但你可能仍然尝试去靠近它。 例如，请你的朋友拍一些手机图片，并发送给你。 一旦app启动后，您可以使用实际的用户数据更新您的开发集/测试集。
  ​如果你真的没有任何方法来获得接近你期望的未来数据，也许你可以从使用网站图像开始。 但是你应该意识到这将导致系统不能一般化的很好的风险。
  ​我们需要判断去决定多少投资开发好的开发集和测试集。 但是不要假定你的训练分布与你的测试分布是一样的。 尝试选择反映您最终想要表现良好的测试样本，而不是训练遇到的任何数据。


## Chapter 6、Your dev and test sets should come from the same distribution

**你的开发集和测试集应该来自相同的分布**

根据您最大的市场，将猫应用图片数据分为四个区域：（i）美国，（ii）中国，（iii）印度和（iv）其他。要想出一个开发集和一个测试集，我们可以随机分配这两个区域到开发集，另外两个到测试集，对吧？ 比如美国和印度在开发集; 中国和其他在测试集。


​	一旦定义了开发集和测试集，您的团队将专注于提高开发集的性能。 因此，开发集应该反映你最想提高的任务：在所有四个地区都要做得很好，而不只是两个。
​	开发集和测试集的不同分布带来的第二个问题：有一个机会，你的团队将构建一些在开发集件上工作得很好，只是发现它在测试集上做得不好。 我曾经在很多失望和白费的努力中看到这个结果。 避免让这些发生在你身上。
​	例如，假设您的团队开发的系统在开发集上工作的很好，但在测试集上并不如意。 如果你的开发集和测试集合来自相同的分布，那么你会有一个非常明确的诊断哪里出错了：你在开发集上过拟合(overfit)了。显而易见的方法是去获得更多的开发集数据。

​	但是如果开发集和测试集来自不同的分布，那么你的选择是不清晰的。几方面可能会出错：

- 1、在开发集上过拟合。

- 2、测试集比开发集更难。所以你的算法可能做的和预期一样好，因此没有进一步的重大改进的可能了。

- 3、测试集不一定更难，但只是和开发集不同。所以在开发集上表现很好但并不能在测试集上表现一样。这种情况下，之前很多提高开发集性能的努力可能都白费了。

  ​在机器学习应用程序上工作是很艰难的。具有不匹配的开发和测试集引入了关于是否改进开发集分布也提高测试集性能额外的不确定性。具有不匹配的开发和测试集，使得更难找出什么是有效和无效的努力，因此使得更难以确定工作的优先级。
  ​如果你面临的是第三方基准测试 问题，他们的创建者可能会指定开发集和测试集来自不同的分布。相比开发和测试集来自同一分布，此时运气，而不是技术，将对这样的基准的性能有更大的影响。开发一个在一个分布上训练的很好同时能够很好的推广到另一个分布中的学习算法是一个重要的研究问题。但是如果你的目标是在一个特定的机器学习应用中取得进展，而不是研究进展，我建议尝试选择开发集和测试集从相同的分布。这将使您的团队更有效率。




## Chapter 7、How large do the dev/test sets need to be?

**开发集/测试集需要多大？**

​	开发集应该足够大，以检测您尝试的算法之间的差异。例如，如果分类器A具有90.0％的准确度，分类器B具有90.1％的准确度，那么100个样本的开发集将不能检测出这0.1％的差异。与我看到的其他机器学习问题相比，100个样本的开发集太小了。常见的开发集的大小在1,000到10,000个样本之间。有10,000个样本，你将有很好的机会检测到这0.1％的提升。[2]
​	对于成熟和重要的应用，例如广告，网络搜索和产品建议 - 我也看到了很多团队，甚至为提升0.01％积极努力，因为它对公司的利润有直接影响。在这种情况下，开发集可以远大于10,000，以便检测出更小的提升。
​	测试集的大小应该多大？它应该足够大，使得对系统的整体性能有一个高的信心。一个流行的启发式方法是将30％的数据用于测试集。当你有适量的样本，比如100到10,000的样本，它会工作的很好。但在大数据的时代，我们现在有机器学习问题，有时会超过十亿个样本，分配给开发集/测试集的比例一直在缩小，即使开发集 /测试集中的样本绝对数量一直在增长。除了需要评估算法的性能之外，没有必要提供过大的开发集/测试集。

——————————

[2]. 在理论上，我们可以测试算法的变化是否对开发集产生统计上显着的差异。 在实践中，大多数团队不会为此困扰（除非他们出版学术研究论文），我通常没法发现统计意义的测试对测量临时进展是有用的。

## Chapter 8、Establish a single-number evaluation metric for your team to optimize

**为你的团队进行算法优化建立一个单一数字的评估指标**

分类准确率是**单一数字评估指标（single-number evaluation metric）**的示例：你在开发集（或测试集）上运行分类器，然后得到样本正确分类的比例(fraction)单个数字。根据这个指标，如果分类器A获得97%的准确率，而分类器B获得90%的准确率，那么我们认为分类器A更好。

相比之下，查准率（Precision）和查全率（Recall）【3】 就不是一个单一数字的评估指标：它给出了两个数字来评估分类器。拥有多个数字的评估指标使得比较算法更加困难。假设你的算法表现如下： 

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/ahgfj6B6gJ.png?imageslim)

如上所示，两个分类器都没有显而易见地比另一个更好，所以它不能立即引导你选择其中一个。

在开发期间，你的团队会尝试大量关于算法架构、模型参数、特征选择等方面的想法。使用**单一数字的评估指标（single-number evaluation metric）**（如精度）使得你可以根据其在该指标上的表现快速对所有模型进行排序，从而快速决定哪一个是能工作得最好的。

如果你真的即关心查准率（Precision）又关心查全率（Recall），我推荐使用一种标准方法将它们组合成一个单一的数字。例如，可以取Precision和Recall的平均值，最终得到单个数字。或者，你可以计算“F1度量（F1 score）”，这是一种基于其平均值改善的方法，比简单地取平均值效果要好。【4】 

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/0FHEG9CGc5.png?imageslim)

当你面在大量的分类器中进行选择时，使用单一数字的评估指标可以加快你做出决策的能力。所有这些都给出了明确的表现排名，从而给出一个清晰的前进方向。

作为最后一个例子，假如你分别得在四个主要市场（（i）美国，（ii）中国，（iii）印度和（iv）其他地区）跟踪猫分类器的准确率。这里提供了四个指标。通过对这四个数据进行平均或加权平均，最终得到一个单一数字度量。取平均值或加权平均值是将多个指标合并为一个的最常见的方法之一。

———————————————————— 

【3】 猫的分类器的查准率（Precision）是指在开发集（或测试集）中检测出的所有有猫的图片中有多少比例是真正的有猫。它的查全率（Recall）指在开发集（或测试集）中所有真正有猫的图片有多少比例被检测出来了。在高查准率和高查全率之间通常存在权衡。

【4】 如果你想要了解更多关于F1度量（F1 score）的信息，请参阅[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score) 。它是基于Precision和Recall的”几何平均（geometric mean）”定义的，其计算公式为2(1/Precision)+(1/Recall)。



## Chapter 9、Optimizing and satisficing metrics

**优化指标和满足指标**

这里有组合多个评估指标的另一种方法。

假设你同时关心算法的准确率和运行时间。你需要在如下三个分类器中进行选择： 

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/GihI1bj0mj.png?imageslim)

如果将准确率和运行时间通过如下一个公式得到单个评估指标会看起来不太自然，例如：

```
Accuracy − 0.5∗RunningTime
```

你可以这样做：首先，定义一个“acceptable”的运行时间。例如任何运行时间在100ms以内的算法都是可接受的。然后，根据满足运行时间标准的分类器，最大化准确率。这里，运行时间就是一个“satisficing metric”，你的分类器必须要在这个指标上表现地“good enough”就行，这就意味着它最多为100ms。准确率是一个“optimizing metric”。

如果你正在权衡N个不同的标准，例如模型的二进制文件大小（这对于移动app很重要，因为用户不想要下载很大的app）、运行时间和准确率，你可以考虑将其中N-1个标准设置为为“satisficing”指标。也就是说你只需要他们满足特定的值即可。然后将最后一个定义为“optimizing”指标。例如，将二进制文件大小和运行时间设定一个可接受的阈值，并尝试在这些约束条件下不断优化准确率。

作为最后一个例子，假定你正在构建一个硬件设备，该设备使用麦克风监听用户说出的某个特定的“唤醒语（wakeword）”，从而唤醒系统。例如：Amazon Echo监听“Alexa”；苹果Siri监听“Hey Siri”；Android监听“Okay Google”；百度app监听“Hello Baidu”。你同时关心假正例的比率（the false positive rate——当没有人说唤醒语时系统唤醒的频率）和假反例的比率（the false negative rate——当有人说出唤醒语时系统没有唤醒的频率）。该系统性能的一个合理目标是最大限度的减少误报率（optimizing metric），同时满足每24小时操作出现不超过一个假正例（satisficing metric）.

一旦你的团队按照评估指标进行优化，他们将能够取得更快的进展。


## Chapter10、Having a dev set and metric speeds up iterations

**有一个开发集和评估指标来加速迭代**

对于一个新问题，很难事先知道什么方法是最合适的。即使经验丰富的机器学习研究人员通常会尝试许多想法，才能发现令人满意的东西。在构建机器学习系统时，我经常会：

1. 首先有一些如何构建系统的想法（**idea**）
2. 用代码（**code**）来实现这些idea
3. 进行实验（**experiment**），来告诉我的这个idea工作的如何。（通常我的前几个想法并不能work）基于这些学习，回去从而产生更多的idea，并不断迭代。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/GBDIdGBm8d.png?imageslim)


这是一个不断迭代的过程。你循环得越快，你的进展也就越快。这就是 开发/测试集 和评估指标非常重要的原因：每次尝试一个idea时，在开发集上衡量idea的表现，将使你快速判断你是否在朝着正确的方向前进。

相反，如果你没有特定的开发集和评估指标。那么每次团队开发出一个新的猫分类器时，你必须把它移植到你的app中，并体验几个小时来感受一下这个新的分类器性能是否有提升。这将非常慢！此外，如果你的团队将分类器的准确率从95.0%提升到95.1%，你可能无法通过体验app来感受到0.1%的提升。而通过不断积累这些许多个0.1%的改进，你的系统将取得很大的进步。有一个开发集和评估指标，可以使你很快地检测出哪些想法给你的系统带来了小（或大）的提升，因此你可以快速决定哪些想法可以继续改善，哪些可以舍弃。


## Chapter 11、When to change dev/test sets and metrics

**何时更改开发/测试集和评估指标**

当开始一个新项目时，我会试图快速选择开发/测试集 ，因为这样可以给团队制定一个明确的目标。

我通常会要求我的团队在不到一周之内（几乎不会更长）提供一个初始的开发/测试集和评估指标。提出一个不太完美的方案并迅速行动起来，比花过多时间去思考更好。但是一周这个时间线并不适用于成熟的应用。例如，反垃圾邮件（anti-spam）是一个成熟的深度学习应用。我曾经见过一些团队会花费数月时间在已经成熟的系统上，去获得更好的开发/测试集。

如果你之后发现初始的开发/测试集或评估指标与目标有失偏颇，那么使用一切手段快速更改它们。例如，如果在你的开发集和评估指标上分类器A比分离器B表现好，但你的团队认为分类器B在实际产品中表现的更优越，这可能表示你需要更改开发/测试集或评估指标。

有三个主要原因可能会造成开发集/评估指标不正确地把分类器A排得更高：

1. 你需要做得好的实际数据的分布和开发/测试集不同。 
   假设你的初始开发/测试集主要是一些成年猫的照片。你查看猫app，发现用户上传了比预期多很多的幼猫的照片。所以，开发/测试集的数据分布并不能代表你需要做好的实际的数据分布。这种情况下，更新你的开发/测试集，使其更具代表性。 
2. 你已经在开发集上过拟合了。 
   在开发集上反复评估想法的过程导致算法逐渐对开发集“过拟合”。当完成开发后，你将在测试集上评估你的算法。如果你的算法在开发集上的表现远好于在测试集上的表现，这意味着你已经过拟合开发集。这种情况下，更新开发集。 
   如果你需要跟踪团队的进度，你也可以在测试集上定期评估你的系统——每月或每周一次。但不要使用测试集来对算法做任何决定，包括是否回滚到上一周的系统。如果这样做，你将开始过拟合测试集，并且不能再依靠它来完全无偏见的评估系统的性能（你可能会在发表研究论文或做出重要商业决策是使用这个指标）。
3. 评估指标衡量的并不是项目所需要优化的东西。 
   假设对于你的猫app，你的评估指标是分类准确率。当前在该指标下分类器A优于分类器B。但是假设你尝试了这两种算法，发现分类器A会偶尔允许色情图片通过。那么即使分类器A准确率更高，偶尔的色情图片所带来的坏影响也意味着其表现是不可接受的。你需要做什么呢？ 
   这里，该评估指标不能辨别出对产品而言算法B比算法A更好这一事实。所以，你不能再相信该指标能挑选出最佳算法。是时候改变评估指标了。例如，你可以更改评估指标，严厉惩罚色情图片分类错误。我强烈建议你选择一个新的评估指标，并用新的标准来为团队明确定义一个新的目标，而不是在一个不可信的评估指标下处理太长时间，并恢复到手工选择分类器。

在项目中改变开发/测试集和评估指标是很常见的。拥有一个初始的开发/测试集和评估指标能帮助你快速迭代。如果你发现 开发/测试集和评估指标不再使你的团队在正确方向上前进，这不是什么大问题！只需要改变它们，并确保你的团队知道新的方向。



## Chapter 12、Takeaways: Setting up development and test sets

**小结：建立开发集和测试集**

- 从分布中选择开发集和测试集，该分布反映你期望在未来获得什么样的数据，并希望在上面做得很好。这可能和你训练数据的分布不一样。
- 如果可能的话，选择来自同一分布的开发集和测试集。
- 为你的团队选择单一数字的评估指标进行优化。如果你关心多个目标，考虑把它们合并到一个公式中（例如平均多个错误指标），或设定满足指标和优化指标。
- 机器学习是一个高度迭代的过程：在发现你满意的方法之前你可能需要尝试很多的idea。
- 开发/测试集和单一数字评估指标可以帮助你快速评估算法，从而迭代的更快。
- 当开始一个全新的应用时，尝试快速建立开发/测试集和评估指标，最好在一周之内。当然，在成熟应用上花费更长的时间是ok的。
- 当你拥有大量数据时，依据70%：30%的比例划分训练/测试集这一经验性的方法不太适用；开发/测试集可以占远小于30%的数据量。
- 你的开发集应该足够大，以检测出算法准确性有意义的改变，但没必要更大。你的测试集应该足够大，大到能对你的系统整体性能有一个确信的评估。
- 如果你的开发集和评估指标不再使你的团队在正确方向上前进，快速改变它们：（i）如果你过拟合了开发集，去获得更多的开发集数据。（ii）如果你所关心的实际分布和开发/测试集的分布不同，那么去获得新的开发/测试集数据。（iii）如果你的评估指标不再能衡量对你来说最重要的东西，改变评估指标。


## REF

- [machine-learning-yearning](https://github.com/xiaqunfeng/machine-learning-yearning/)
- 《Machine Learning Yearning》 by Andrew NG
