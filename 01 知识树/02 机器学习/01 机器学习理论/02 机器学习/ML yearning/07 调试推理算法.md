## 优化验证测试 The Optimization Verification test

假设你在构建一个语音识别系统。系统通过输入一个音频剪辑A，并对每个可能的输出语句S计算某个`Score_A(S)`来工作。例如，给定输入音频A，你可能尝试去估计`Score_A(S) = P(S|A)`，即正确输出转录语句是S的概率。

给定计算`Score_A(S)`的方法，你仍然需要去寻找最大化它的英语语句S：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/k365KH31f1.png?imageslim)

你如何计算上面的“arg max”？如果英语语言有5w个词汇，那么长度为N的语句就有(5w)^N种可能——太多而无法详尽列举。所以，你需要应用一个近似搜索算法，试图找出优化（最大化）`Score_A(S)`的值S。一个搜索算法的例子是“束搜索(beam search)”，其在搜索的过程中保持只保留K个最高候选人（就本章而言，你不需要了解束搜索的细节）。像这种算法不能保证找到最大化`Score_A(S)`的值S。

假设一个音频剪辑A记录有人说“我爱机器学习”。但你的系统不是正确的转录输出，而是输出错误的“我爱机器人”。有两种出错的可能性：

1. **搜索算法问题**。近似的搜索算法（束算法）未能寻找到最大化`Score_A(S)`的值S。
2. **目标（打分函数）问题**。我们对`Score_A(S) = P(S|A)`的估计不准确。特别地，我们选择的`Score_A(S)`没能识别出“我爱机器学习”是的正确的转录。

根据这些中的哪个是失败的原因，你应该以不同的方式优先考虑你的努力。如果#1是问题，你应该致力于提升搜索算法。如果#2是问题，你应该致力于估计`Score_A(S)`的学习算法。

面对这种情况，一些研究者将随机决定使用搜索算法；另一些研究者将随机以更好的方式去学习`Score_A(S)`的值。但除非你知道哪个是错误的根本原因，否则你的努力可能会被浪费。你怎样更系统的决定要做什么？

让S_out表示输出转录（“我爱机器人”）。让`S*`表示正确的转录（“我爱机器学习”）。为了了解上面的#1和#2哪个是问题，你可以执行**优化验证测试**：首先，计算Score_A(`S*`)和Score_A(S_out)。然后检查是否Score_A(`S*`) > Score_A(S_out)。有两种可能：

Case 1：Score_A(`S*`) > Score_A(S_out)

在这种情况下，你的学习算法已经正确地给出`S*`比S_out更高的分数。尽管如此，我们的近似搜索算法选择了S_out，而不是`S*`。这告诉你近似搜索算法未能选择最大化`Score_A(S)`的值S。在这种情况下，优化验证测试告诉你你的搜索算法有问题，你应该专注在该问题上。例如，你可以尝试增加束算法的束宽。

Case 2：Score_A(`S*`) <= Score_A(S_out)

在这种情况下，你了解到计算`Score_A(.)`的方式不对：相比错误的输出S_out，它并没有给正确的输出`S*`严格更高的分数。优化验证测试告诉你你的目标（打分）函数有问题。因此，你应该专注于提高对不同语句S的学习或近似`Score_A(S)`。

我们的讨论集中在一个例子上。为了在实践中应用优化验证测试，你应该检测开发集中的错误。对每个错误，你可以测试是否Score_A(`S*`) > Score_A(S_out)。该不等式所持有的每个开发样例将标记为由优化算法引起的错误。不等式Score_A(`S*`) <= Score_A(S_out)所持有的每个样例由于计算Score_A(.)的方式不对而被视为错误。

例如，假设你发现95%的错误是由打分函数Score_A(.)引起的，只有5%的错误是因为优化算法。现在你知道不论你改进了多少优化程序，你只会真实地消除约5%的错误。因此，你应该专注于改进评估Score_A(.)的方式。



## 优化验证集的一般形式 General form of Optimization Verification test

当给定输入x，你知道如何计算表示响应y对输入x有多好的Score_x(y)时，你可以应用优化验证测试。而且，你正在使用近似算法来试图找到`arg max_y Score_x(y)`，但怀疑搜索算法有时不能找到最大值。在我们前面的语音识别示例中，x=A是一个音频剪辑，y=S是输出转录。

假设`y*`是“正确的”输出，但算法输出y_out。那么关键测试是测量是否Score_x(`y*`) > Score_x(y_out)。如果该不等式成立，那么我们会将错误归咎于优化算法。参考前一章以确保你理解其背后的逻辑。否则，我们将归咎于Score_x(y)的计算。

让我们再看一个例子。假设你正在构建一个中译英的机器翻译系统。系统通过输入中文句子C，并为每个可能的翻译E计算Score_c(E)。例如，你可能使用Score_c(E)=P(E|C)，给定输入语句C，翻译为E的概率。

你的算法通过尝试如下计算来翻译句子：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/64E86eL966.png?imageslim)

然而，所有可能的英语语句E的集合太大了，所以你依靠启发式搜索算法。

假设你的算法输出错误的翻译 E_out，而不是正确的翻译`E*`。那么优化验证测试将要求你计算是否Score_c(`E*`) > Score_c(E_out)。如果该不等式成立，那么Score_c(.)正确地将`E*`识别为比E_out更好的输出；因此，你会将此错误归咎于近似搜索算法。否则，你将此错误归咎于Score_c(.)的计算。

这是AI中一个非常常见的“设计模式”，首先学习一个近似打分函数Score_x(.)，然后使用一个近似最大化算法。如果你能够发现该模式，那么你将能够使用优化验证测试来了解您的错误来源。

## Chapter 46、Reinforcement learning example

**强化学习样本**

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/GJKaI7a0GD.png?imageslim)

假设你正在使用机器学习去教直升机复杂的演习。 这是计算机控制器直升机在发动机关闭时执行着陆时的延时照片。

这被称为“自动旋转”演习。即使发动机意外失效，它也允许直升机着陆。人类飞行员将该演习作为他们的训练的一部分。你的目标是使用学习算法通过以安全着陆为结尾的轨迹T飞行直升机。

为了应用增强学习，你必须开发一个“奖励函数”R(.)，它给一个分数来衡量每个可能的轨迹T有多好。例如，如果T导致直升机坠毁，那么可能奖励是R(T)=-1000——大的负值奖励。导致安全着陆的轨迹T根据其着陆的平滑程度可能导致一个具体的正的R(T)值。通常通过手动选择奖励函数R(.)来量化不同轨迹T的理想程度。它必须权衡着陆时的颠簸程度，直升机是否准确着陆在期望地点，乘客下来的路有多崎岖，等等。设计一个好的奖励函数不简单。

给定一个奖励函数R(T)，强化学习算法的工作是去控制直升机以便获取 `max_T R(T)`。然而，强化学习算法产生很多近似值，并且可能不会成功实现它的最大化。

假设你选择了某个奖励R(.)并运行了你的学习算法。然而，它的表现比人类飞行员要差的远——比起人类飞行员，其着陆颠簸而且看起来不安全。你怎么知道该缺点与增强学习算法有关（算法试图执行一条达到`max_T R(T)`的轨迹），或者与奖励函数有关（其试图衡量及指定乘坐颠簸和着陆点准确性之间的理想权衡）？

为了应用优化验证测试，让T_human作为人类飞行员实现的轨迹，T_out作为算法实现的轨迹。根据我们之前的描述，T_human是一条比T_out更好的轨迹。因此，关键测试如下：

不等式`R(T_human) > R(T_out)` 是否成立？

Case 1：如果该不等式成立，那么奖励函数R(.)正确地评估出T_human优于T_out。但我们的强化学习算法找到较差的T_out。这表明致力于提升强化学习算法是值得的。

Case 2：该不等式不成立：`R(T_human) <= R(T_out)`。这意味着尽管T_human是更优的轨迹，R(.)仍然为其分配了一个更糟的分数。你应该致力于提升R(.)以更好捕获对应于良好着陆的折衷。

很多机器学习应用都有这种使用近似搜索算法来优化近似打分函数Score_x(.)的“模式”。有时，没有指定的输入x，所以这只会减少Score(.)。在我们上面的例子中，打分函数就是奖励函数Score(T)=R(T)，优化函数是试图执行良好轨迹T的强化学习算法。

在该例子和以前例子的一个区别是，与其和“最优”输出进行比较，不如和人类水平的表现T_human进行比较。我们假设T_human是非常好的，即使不是最优的。一般来说，只要你有某个`y*`（在该例子中是T_human），它是比你当前学习算法表现更好的输出（即使它不是“最优”输出），那么优化验证测试可以指示它是否更有希望提升优化算法或打分函数。



## REF

- [machine-learning-yearning](https://github.com/xiaqunfeng/machine-learning-yearning/)
- 《Machine Learning Yearning》 by Andrew NG
