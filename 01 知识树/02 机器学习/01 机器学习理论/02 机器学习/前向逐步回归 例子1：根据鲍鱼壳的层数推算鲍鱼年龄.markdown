---
author: evo
comments: true
date: 2018-05-12 23:44:10+00:00
layout: post
link: http://106.15.37.116/2018/05/13/stage-sample1/
slug: stage-sample1
title:
wordpress_id: 5625
categories:
- 随想与反思
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL






  1. [第8章 预测数值型数据：回归](http://ml.apachecn.org/mlia/regress/)




# TODO






  * **没有很理解，需要结合理论进行理解**




# MOTIVE






  * aaa





* * *





# 项目要求


我们有一份来自 UCI 的数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。


# 项目数据


[download id="5613"]

数据存储格式：


    1   0.455   0.365   0.095   0.514   0.2245  0.101   0.15    15
    1   0.35    0.265   0.09    0.2255  0.0995  0.0485  0.07    7
    -1  0.53    0.42    0.135   0.677   0.2565  0.1415  0.21    9
    1   0.44    0.365   0.125   0.516   0.2155  0.114   0.155   10
    0   0.33    0.255   0.08    0.205   0.0895  0.0395  0.055   7




# 与其它项目的关系


前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有权重都设置为 1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。


# 完整代码




    import numpy as np
    import matplotlib.pylab as plt


​
    # 加载数据集
    def load_data_set(file_name):
        feature_num = len(open(file_name).readline().split('\t')) - 1
        data_arr = []
        label_arr = []
        fr = open(file_name)
        for line in fr.readlines():
            line_arr = []
            cur_line = line.strip().split('\t')
            # 特征值
            for i in range(feature_num):
                line_arr.append(float(cur_line[i]))
            data_arr.append(line_arr)
            # 标签
            label_arr.append(float(cur_line[-1]))
        data_mat = np.mat(data_arr)
        label_mat = np.mat(label_arr).T
        return data_mat, label_mat


​
    # 前向逐步回归计算权重值
    def stage_wise(data_mat_m, label_mat_m, epsilon=0.01, num_iter=100):
        row_num, column_num = np.shape(data_mat_m)
        weight_mat = np.zeros((num_iter, column_num))
        weights = np.zeros((column_num, 1))
        weights_test = weights.copy()
        weights_max = weights.copy()
        # 这一段的计算的理论支持是什么？
        for iter in range(num_iter):
            min_error = np.inf
            for column in range(column_num):
                for sign in [-1, 1]:
                    weights_test = weights.copy()
                    weights_test[column] += epsilon * sign
                    label_pred = data_mat_m * weights_test
                    rss_error = calc_rss_error(label_mat_m.A, label_pred.A)
                    if rss_error < min_error:
                        min_error = rss_error
                        weights_max = weights_test  # 只有小于，这个test才会更新到max里面
            weights = weights_max.copy()  # 因此如果大于，那么ws还是用的旧的
            weight_mat[iter, :] = weights.T
        return weight_mat


​
    def regular_data_label(data_mat, label_mat):
        data_mean = np.mean(data_mat, 0)  # xMat 平均值
        data_var = np.var(data_mat, 0)  # X的方差
        data_mat_m = (data_mat - data_mean) / data_var

        label_mean = np.mean(label_mat, 0)
        label_mat_m = label_mat - label_mean  # 也可以规则化ys但会得到更小的coef

        return data_mat_m, label_mat_m


​
    # residual sum of squares error
    def calc_rss_error(label_mat, label_pred_mat):
        # 这个地方如果是(label_mat- label_pred_mat)**2 则错误：
        # ValueError: input must be a square array
        # 因为 array可以平方，而matrix只有方阵时才可以平方
        return (np.square(label_mat - label_pred_mat)).sum()


​
    # 标准线性回归
    def standard_lr(data_mat, label_mat):
        data_square = data_mat.T * data_mat
        # 因为要用到xTx的逆矩阵，因此先确定xTx是否可逆
        # 可逆的条件是矩阵的行列式不为0，如果为0，即不可逆，则无法继续进行计算 为什么？
        # linalg.det()可以求得矩阵的行列式
        if np.linalg.det(data_square) == 0.0:
            print("This matrix is singular, cannot do inverse")
            return
        # 根据书中的公式，求得w的最优解
        weights = data_square.I * (data_mat.T * label_mat)
        label_pred_mat = data_mat * weights
        return label_pred_mat, weights

    def plot_weight_mat(weight_mat,weights):
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.scatter([1000]*8, weights)
        ax.plot(weight_mat)
        plt.show()

    # test for stageWise
    if __name__ == "__main__":
        data_mat, label_mat = load_data_set("abalone.txt")
        data_mat_m, label_mat_m=regular_data_label(data_mat,label_mat)
        weight_mat = stage_wise(data_mat_m, label_mat_m, 0.01, 1000)
        print(weight_mat)
        label_pred_mat, weights = standard_lr(data_mat_m, label_mat_m)
        print(weights.T)
        plot_weight_mat(weight_mat,weights.T)


输出：


    [[ 0.    0.    0.   ...,  0.    0.    0.  ]
     [ 0.    0.    0.   ...,  0.    0.    0.  ]
     [ 0.    0.    0.   ...,  0.    0.    0.  ]
     ...,
     [ 0.05  0.    0.09 ..., -0.64  0.    0.36]
     [ 0.04  0.    0.09 ..., -0.64  0.    0.36]
     [ 0.05  0.    0.09 ..., -0.64  0.    0.36]]
    [[ 0.0430442  -0.02274163  0.13214087  0.02075182  2.22403814 -0.99895312
      -0.11725427  0.16622915]]


输出图像：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/d37e58BIGL.png?imageslim)

有几个问题：


## 我这里的 weight_mat 里面的最高的没有超过0.5 ，但是我看书上好像已经快到1.5了，错在哪里了？


**未解决**


## 怎么才能将这几个点的颜色弄得与线的颜色相同？


**未解决**


## 理论还不是很清楚


**需补充**


## 逐步线性回归算法的主要优点


在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。**？没明白？这些重要的特征指的是线性重要的吗？ 要重新理解下**











* * *





# COMMENT
