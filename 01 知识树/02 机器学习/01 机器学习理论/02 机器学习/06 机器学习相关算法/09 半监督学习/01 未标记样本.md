


未标记样本


我们在丰收季节来到瓜田，满地都是西瓜，瓜农抱来三四个瓜说这都是好瓜，然后再指着地里的五六个瓜说这些还不好，还需再生长若干天。基于这些信息，我们能否构建一个模型，用于判别地里的哪些瓜是已该采摘的好瓜？显然, 可将瓜农告诉我们的好瓜、不好的瓜分别作为正例和反例来训练一个分类器. 然而，只用这不到十个瓜做训练样本，有点太少了吧？能不能把地里的那些瓜也用上呢？

形式化地看，我们有训练样本集  $D_l=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_l,y_l)\}$ ，这 l 个样本的类别标记（即是否好瓜）已知，称为“有标记”（labeled）样本；此外，还有 $D=\{x_{l+1},x_{l+2},\cdots ,x_{l+u}\} ，$l\ll u$ ，这 u 个样本的类别标记未知（即不知是 否好瓜），称为“未标记” （unlabeled）样本.若直接使用传统监督学习技术，则仅有 $D_l$ 能用于构建模型， $D_u$ 所包含的信息被浪费了；另一方面，若 $D_l$ 功较小，则由于训练样本不足，学得模型的泛化能力往往不佳.那么，能否在构建模型的过程中将 $D_u$ 利用起来呢？

一个简单的做法，是将 $D_u$ 中的示例全部标记后用于学习。这就相当于请瓜 农把地里的瓜全都检查一遍，告诉我们哪些是好瓜，哪些不是好瓜，然后再用于 模型训练。显然，这样做需耗费瓜农大量时间和精力.有没有 “便宜” 一点的办法呢？

例如基于D!训练一个 SVM,挑选距离分类超平 面最近的未标记样本来进 行查询.

即尽量少向瓜农询问.

我们可以用Dz先训练一个模型,拿这个模型去地里挑一个瓜，询问瓜农好 不好，然后把这个新获得的有标记样本加入功中重新训练一个模型，再去挑 瓜，……这样，若每次都挑出对改善模型性能帮助大的瓜，则只需询问瓜农比较 少的瓜就能构建出比较强的模型，从而大幅降低标记成本.这样的学习方式称 为“主动学习”（active learning）,其目标是使用尽量少的“查询”（query）来获 得尽量好的性能.

显然，主动学习引入了额外的专家知识，通过与外界的交互来将部分未标记样本转变为有标记样本。若不与专家交互，没有获得额外信息，还能利用未标 记样本来提高泛化性能吗？

答案是“Yes!”，有点匪夷所思？

事实上，未标记样本虽未直接包含标记信息，但若它们与有标记样本是从同样的数据源独立同分布采样而来，则它们所包含的关于数据分布的信息对建立模型将大有禆益.图13.1给出了一个直观的例示.若仅基于图中的一个正例 和一个反例，则由于待判别样本恰位于两者正中间，大体上只能随机猜测；若能 观察到图中的未标记样本，则将很有把握地判别为正例.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/D2g8012I9l.png?imageslim)


让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能，就是半监督学习(semi-supervised learning).半监督学习的现实需求非常强烈，因为 在现实应用中往往能容易地收集到大量未标记样本，而获取“标记”却需耗费 人力、物力.例如，在进行计算机辅助医学影像分析时，可以从医院获得大量医 学影像，但若希望医学专家把影像中的病灶全都标识出来则是不现实的.“有标记数据少，未标记数据多” 这个现象在互联网应用中更明显，例如在进行网页推荐时需请用户标记出感兴趣的网页，但很少有用户愿花很多时间来提供标记，因此，有标记网页样本少，但互联网上存在无数网页可作为未标记样本来使用。半监督学习恰是提供了一条利用“廉价”的未标记样本的途径.


要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记相联系的假设.最常见的是“聚类假设” (cluster assumption),即假设数据存在簇结构，同一个簇的样本属于同一个类别。

图13.1就是基于聚类假设来利用未标记样本，由于待预测样本与正例样本通过未标记样本的 “撮合” 聚在一起，与相对分离的反例样本相比，待判别样本更可能属于正类.半监督学习中另一种常见的假设是“流形假设”(manifold assumption),即假设数据分布在一个流形结构上，邻近的样本拥有相似的输出值。 “邻近” 程度常用 “相似” 程度来刻画，因此，流形假设可看作聚类假设的推广，但流形假设对输出值没有限制，因此比聚类假设的适用范围更广,可用于更多类型的学习任务.事实上，无论聚类假设还是流形假设，其本质都是 “相似的样本拥有相似的输出”  这个基本假设.

半监督学习可进一步划分为纯(pure)半监督学习和直推学习（transductive learning）,前者假定训练数据中的未标记样本并非待预测的数据，而后者则假 定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能。换言之，纯半监督学习是基于“开放世界” 假设，希望学得模型能适用于训练过程中未观察到的数据；而直推学习是基于“封闭世界”假设，仅试图对学习过程中观察到的未标记数据进行预测。 图13.2直观地显示出主动学习、纯半监督学习、直推学习的区别。需注意的是，纯半监督学习和直推学习常合称为半监督学习，本书也采取这一态度，在需专门区分时会特别说明.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/G3G3HB0Kjl.png?imageslim)




# REF
1. 《机器学习》周志华
