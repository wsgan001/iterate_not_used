
# 对偶问题

TODO



之前我们已经得到了这个支持向量机 SVM 的基本型了：

$$\begin{align*} &\underset{w,b}{min}\; \frac{1}{2}||w||^2\\ & s.t.\; y_i(w^Tx_i+b)\geqslant 1,i=1,2,\cdots ,m \end{align*} \tag{6.6}$$

OK，那么怎么求解这个式子中的 $w$ 和 $b$ 呢？


## 使用拉格朗日乘子法求解这个基本型的对偶问题

我们注意到，上面这个式子本身是一个凸二次规划 (convex quadratic programming) 问题，是能够直接用现成的优化计算包求解的。<span style="color:red;">到底怎么求解？</span>

但是，我们可以有更高效的办法。

我们可以使用拉格朗日乘子法来求解它的对偶问题(dual problem)。

具体来说，对上面的式子的每个约束条件添加拉格朗日乘子 $\alpha_i\geqslant 0$ 。

拉格朗日函数如下：

$$L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^{m}\alpha_i (1-y_i(w^Tx_i+b))$$

其中，$\alpha =(\alpha_1;\alpha_2;\cdots;\alpha_m)$。

我们令 $L(w,b,\alpha)$ 对 $w$ 和 $b$ 求偏导为0，可得：

$$w=\sum_{i=1}^{m}\alpha_iy_ix_i$$

$$0=\sum_{i=1}^{m}\alpha_iy_i$$

我们将 $w$ 的表达式带入到拉格朗日函数里，可将 $L(w,b,\alpha)$ 中的 $w$ 和 $b$ 消去，<span style="color:red;">代入计算的过程还是要自己算一下的。</span>然后，我们再考虑 $0=\sum_{i=1}^{m}\alpha_iy_i$ 的约束，就得到了最开始的问题的对偶问题：<span style="color:red;">这里没有特别理解，再补充下。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/Ie78fBLDGb.png?imageslim)

<span style="color:red;">第一次看到这种对偶问题的转化，感觉真心厉害。发明对偶问题的人是怎么想到的。</span>

解出 $\alpha$ 之后，我们就可以求出 $w$ 和 $b$ ，然后就可以得到模型：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/aF4FdI68hG.png?imageslim)


对偶问题求解出来的就是拉格朗日函数的乘子，它恰好对应着训练样本 $(x_i,y_i)$ ，<span style="color:red;">什么叫恰好对应着训练样本  $(x_i,y_i)$ ？</span>我们注意到 (6.6) 中有不等式约束，因此上述过程需满足 KKT(Karush-Kuhn-Tucker) 条件，即要求：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/a90dKhb0g1.png?imageslim)

<span style="color:red;">对于 KKT 条件还是不清楚，为什么要满足这个条件？为什么有不等式约束就要满足这个条件？</span>

于是，对任意训练样本 $(x_i,y_i)$ ，总有 $\alpha_i=0$ 或 $y_if(x_i)=1$。<span style="color:red;">没明白，这个于是是为什么？</span>若 $\alpha_i=0$ ,则该样本将不会在式 (6.12) 的求和中出现，也就不会对 $f(x)$ 有任何影响；若 $\alpha_i>0$ , 则必有 $y_if(x_i)=1$ ，所对应的样本点位于最大间隔边界上，是一个支持向量. 这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。<span style="color:red;">虽然我知道训练完后，模型只与支持向量有关，但是这一段还是没看懂。</span>


## SMO 算法

<span style="color:red;">这个 SMO 完全没看懂。要找一些资料来更详细的看下。</span>

那么，如何求解式 (6.11) 呢？不难发现，这是一个二次规划问题，可使用通用的二次规划算法来求解。然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销。为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法，SMO (Sequential Minimal Optimization)是其中一个著名的代表。<span style="color:red;">之前在 sklearn 的案例中有看到 SMO 这个吗？</span>

SMO的基本思路是先固定 $\alpha_i$ 之外的所有参数，然后求 $\alpha_i$ 上的极值。由于存在约束 $\sum_{i=1}^{m}\alpha_iy_i=0$ ，若固定 $\alpha_i$ 之外的其他变量，则 $\alpha_i$ 可由其他变量导出。 于是，SMO 每次选择两个变量 $\alpha_i$ 和 $\alpha_j$ ，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：

- 选取一对需更新的变量 $\alpha_i$ 和 $\alpha_j$ 。
- 固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解式(6.11)获得更新后的 $\alpha_i$ 和 $\alpha_j$。

注意到只需选取的 $\alpha_i$ 和 $\alpha_j$ 中有一个不满足 KKT 条件(6.13)，目标函数就会在迭代后减小。直观来看，KKT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。

于是，SMO 先选取违背 KKT 条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但由于比较各变量所对应的目标函数值减幅的复杂度过高，因此 SMO 采用了一个启发式：使选取的两变量所对应样本之间的间隔最大。一种直观的解释是，这样的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。

SMO算法之所以高效，恰由于在固定其他参数后，仅优化两个参数的过程能做到非常高效。具体来说，仅考虑 $\alpha_i$ 和 $\alpha_j$ 时，式(6.11)中的约束可重写为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/mKdbimmeLI.png?imageslim)

其中:

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/cJlF89ffLl.png?imageslim)

是使 $\sum_{i=1}^{m}\alpha_iy_i=0$ 成立的常数。用：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/hfJgmA39ae.png?imageslim)

消去式(6.11)中的变量  $\alpha_j$ ，则得到一个关于 $\alpha_i$ 的单变量二次规划问题，仅有的 约束是  $\alpha_i\geq 0$ 。不难发现，这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效地计算出更新后的 $\alpha_i$ 和 $\alpha_j$ 。

如何确定偏移项 $b$ 呢？注意到对任意支持向量 $(x_s,y_s)$ 都有 $y_sf(x_s)=1$ ，即：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/EljG8CkfgF.png?imageslim)


其中 $S=\{i|\alpha_i>0,i=1,2,\cdots ,m\}$ 为所有支持向量的下标集。理论上，可选取任意支持向量并通过求解式(6.17)获得 $b$ ,但现实任务中常采用一种更鲁棒的做法：使用所有支持向量求解的平均值

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/424kf2LHmD.png?imageslim)




## REF

1. 《机器学习》周志华
