



流形学习


流形学习(manifold learning)是一类借鉴了拓扑流形概念的降维方法. “流形”是在局部与欧氏空间同胚的空间，换言之，它在局部具有欧氏空间的 性质，能用欧氏距离来进行距离计算。

这给降维方法带来了很大的启发：若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂, 但在局部上仍具有欧氏空间的性质，因此，可以容易地在局部建立降维映射关系，然后再设法将局部映射关系推广到全局.当维数被降至二维或三维时，能对数据进行可视化展示，因此流形学习也可被用于可视化.本节介绍两种著名的流形学习方法.

## 1等度量映射

等度量映射(Isometric Mapping,简称 Isomap)的基本出发点，是认为低维流形嵌入到高维空间之后，直接在高维空间中计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的. 如图10.7(a)所示，低维嵌入流形上两点间的距离是“测地线”(geodesic)距离: 想象一只虫子从一点爬到另一点，如果它不能脱离曲面行走，那么图10.7(a)中 的红色曲线是距离最短的路径，即 S 曲面上的测地线，测地线距离是两点之间 的本真距离.显然，直接在高维空间中计算直线距离是不恰当的.


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/j07fAIfF7C.png?imageslim)

那么，如何计算测地线距离呢？这时我们可利用流形在局部上与欧氏空间同胚这个性质，对每个点基于欧氏距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接，于是，计算两点之间测地线距离的问题，就转变为计算近邻连接图上两点之间的最短路径问题.从图10.7(b)可看出，基于近邻距离逼近能获得低维流形上测地线距离很好 的近似。

在近邻连接图上计算两点间的最短路径，可采用著名的 Dijkstra 算法或 Floyd 算法，在得到任意两点的距离之后，就可通过10.2节介绍的MDS方法来 获得样本点在低维空间中的坐标.图10.8给出了 Isomap 算法描述.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/kGfaAC4DkF.png?imageslim)


需注意的是，Isomap仅是得到了训练样本在低维空间的坐标，对于新样本， 如何将其映射到低维空间呢？这个问题的常用解决方案,是将训练样本的高维 空间坐标作为输入、低维空间坐标作为输出，训练一个回归学习器来对新样本 的低维空间坐标进行预测.这显然仅是一个权宜之计，但目前似乎并没有更好的办法.

对近邻图的构建通常有两种做法，一种是指定近邻点个数，例如欧氏距离 最近的 k 个点为近邻点，这样得到的近邻图称为 k 近邻图；另一种是指定距离阈值 $\epsilon$ ，距离小于 $\epsilon$ 的点被认为是近邻点，这样得到的近邻图称为e近邻图。

两种方式均有不足，例如若近邻范围指定得较大，则距离很远的点可能被误认为近邻，这样就出现“短路”问题;近邻范围指定得较小，则图中有些区域可能与其他区域不存在连接，这样就出现“断路”问题.短路与断路都会给后续的最短路径计算造成误导.

## 2局部线性嵌入

与Isomap试图保持近邻样本之间的距离不同，局部线性嵌入(Locally Linear Embedding,简称LLE) 试图保持邻域内样本之间的线性关系.如图10.9所示,假定样本点 $x_i$ 的坐标能通过它的邻域样本 $x_j$ ，$x_k$,$x_l$ 的坐标通过线性组合而重构出来，即

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/6dFc8aBdC1.png?imageslim)

LLE希望式(10.26)的关系在低维空间中得以保持.

LLE先为每个样本  $x_i$ 找到其近邻下标集合 $Q_i$ ，然后计算出基于 $Q_i$ 中的样本点对 $x_i$ 进行线性重构的系数 $w_i$ :

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/iggGGb8ij6.png?imageslim)

其中 $x_i$ 和 $x_j$ 均为已知，令 $C_{jk}=(x_i-x_j)^T(x_i-x_k)$， $w_{ij}$ 有闭式解：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/add1e8cK76.png?imageslim)

LLE在低维空间中保持 $w_i$ 不变，于是 $x_i$ 对应的低维空间坐标 $z_i$ 可通过下式求解：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/5mcf5a8e9L.png?imageslim)

式(10.27)与(10.29)的优化目标同形，唯一的区别是式(10.27)中需确定的是 $w_i$ ，而式(10.29)中需确定的是 $x_i$ 对应的低维空间坐标 $z_i$ 。


令$Z=(z_1,z_2,\cdots ,z_m)\in \mathbb{R}^{d'\times m}$ ，$(W)_{ij}=w_{ij}$ ,

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/FbE8hD5100.png?imageslim)


则式(10.29)可重写为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/jHi5DlFBAl.png?imageslim)

式(10.31)可通过特征值分解求解：M 最小的 $d'$ 个特征值对应的特征向量组成的矩阵即为 $Z^T$ 。

LLE的算法描述如图 10.10 所示.算法第 4 行显示出：对于不在样本 $x_i$ 邻域区域的样本 $x_j$ ,无论其如何变化都对 $x_i$ 和 $z_i$ 没有任何影响；这种将变动限制在局部的思想在许多地方都有用.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/fJ8EcAGcF9.png?imageslim)








# REF
1. 《机器学习》周志华
