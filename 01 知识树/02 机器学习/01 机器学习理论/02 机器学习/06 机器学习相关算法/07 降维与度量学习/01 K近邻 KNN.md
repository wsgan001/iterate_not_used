
# K近邻 KNN

K近邻 (K-Nearest Neighbor  KNN)

K近邻是一种常用的监督学习方法, 它的工作机制非常简单：

给定测试样本，基于某种距离度量找出训练集中与其最靠近的 K 个训练样本，然后基于这 K 个“邻居”的信息来进行预测。

通常，在分类任务中可使用“投票法”，即选择这$k$个样本中出现最多的类别标记作为预 测结果；在回归任务中可使用“平均法”，即将这$k$个样本的实值输出标记的 平均值作为预测结果；还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大.

与前面介绍的学习方法相比，$k$近邻学习有一个明显的不同之处：它似乎 没有显式的训练过程！<span style="color:red;">是的。</span>事实上，它是“懒惰学习”(lazy learning)的著名代表<span style="color:red;">原来这个就是懒惰学习</span>， 此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到 测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习” (eager learning).<span style="color:red;">嗯。</span>

图10.1给出了$k$近邻分类器的一个示意图.显然，$k$是一个重要参数，当$k$取不同值时，分类结果会有显著不同.另一方面，若采用不同的距离计算方式， 则找出的“近邻”可能有显著差别，从而也会导致分类结果有显著不同.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/bCe7ImF5kb.png?imageslim)

暂且假设距离计算是“恰当”的，即能够恰当地找出$k$个近邻，我们来对 “最近邻分类器” (1NN,即$k$= 1)在二分类问题上的性能做一个简单的讨论.


给定测试样本 x ，若其最近邻样本为 z ，则最近邻分类器出错的概率就是 x 与 z 类别标记不同的概率，即

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/Bk05aE9IbL.png?imageslim)


假设样本独立同分布，且对任意 x 和任意小正数 $\delta$ ，在 x 附近 $\delta$ 距离范围内总能找到一个训练样本；换言之，对任意测试样本，总能在任意近的范围内找到式(10.1)中的训练样本令 $c^*=arg\,max_{c\in \mathcal{Y} }P(c|x)$ 表示贝叶斯最优分类器的结果，有

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/dg7HGB0EK1.png?imageslim)

于是我们得到了有点令人惊讶的结论:最近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍！<span style="color:red;">真的假的，再看看</span>






## REF

1. 《机器学习》周志华
