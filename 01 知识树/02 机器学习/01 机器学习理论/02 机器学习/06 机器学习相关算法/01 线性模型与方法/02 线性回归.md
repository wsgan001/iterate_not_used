# 线性回归

TODO

- 上一节中末尾的问题没有加进来。
- 从回归里面把加入扰动的部分拿进来 <span style="color:red;">有加入扰动的部分吗？</span>
- 从回归里面把整个计算过程拿进来。
- 一直想透彻理解么是线性回归，和 Logistic 回归有什么关系和区别？<span style="color:red;">一直到现在还是不清楚</span>
- 而且，实际中真的会用到线性回归模型吗？
- 实际中真的会用到广义线性模型吗？广义线性模型的联系函数是怎么确定的？


## 知识前提

- 均方误差
- 矩阵求逆，求导


## 什么是线性回归？


给定一个数据集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m),\}$，其中的$(x_1,y_1)$ 就是一个样本， $x_i=(x_{i1};x_{i2};\cdots ;x_{id})$ 就是一个样本的属性，$y_i\in \mathbb{R}$ 就是这个样本的标记 。

那么 "线性回归"(linear regression) 实际上就是试图学得一个线性模型，以尽可能准确地预测实值，来输出标记。<span style="color:red;">嗯，这种拟合的确是回归</span>


## 我们看看只有一个特征的时候

### 只有一个特征的时候

当只有一个特征的时候，即 $w$ 只是一个数值，那么此时的线性回归试图学得 $f(x_i)=wx_i+b$ 使得 $f(x_i)\simeq y_i$。

OK，现在我们想求得这个 $w$ 和 $b$，使得 $f(x_i)\simeq y_i$。

### 现在需要对 $f(x)$ 与 $y_i$ 之间的近似进行衡量

但是呢，在这之前，我们必须首先知道，到底怎么去衡量 $f(x_i)$ 与 $y_i$ 的近似情况。不然的话，我们也不知道我们得到的 $w$ 和 $b$ 是不是足够好，是不是有更好的使 $f(x_i)$ 更近似于 $y_i$。<span style="color:red;">是呀</span>

也就是说：我们要知道对 $f(x_i)$ 与 $y_i$ 的近似性的度量方法

怎么衡量呢？

其实我们可以把 $f(x_i)$ 与 $y_i$ 的近似情况看成是 $f(x_i)$ 与 $y_i$ 的距离，而越近似，就说明距离越小。

那么这二者之间的距离怎么求呢？OK，这里我们想到了欧式距离 (Euclidean distance)。是可以的。<span style="color:red;">差点忘了，这个地方怎么就自然的想到了欧式距离呢？这么多种距离的计算方式，别的都不考虑了吗？</span>

写成如下：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/l5eahc8m4C.png)

<span style="color:red;">上面这个式子左边为什么是 $(w^*,b^*)$ ？</span>

实际呢，这个欧氏距离其实就是对应的均方误差，而这其实是回归任务中最常用的性能度量。<span style="color:red;">是的，我看到这个式子的时候就想到了均方误差</span>

### 现在我们来求一套 $w$ 和 $b$ 使得这个距离最小

实际上，基于均方误差最小化来进行模型求解的方法被称为 "最小二乘法"(least squre method)。<span style="color:red;">是这样吗？为什么叫做最小二乘法？</span>而求解 $w$ 和 $b$ 使![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/mLiK6B4GJI.png)最小化的过程，称为线性回归模型的最小二乘 "参数估计" (prameter estimation)。<span style="color:red;">对于最小二乘还是要仔细总结下的。</span>

OK，要求最小化，我们首先想到求导数：

将 $E_{(w,b)}$ 分别对 $w$ 和 $b$ 求导，得到：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/134G5l55H5.png)

令两个导数为 0 得到：<span style="color:red;">这个式子自己要推一遍的</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/9C9JC8jgc3.png)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/JLJDK17j03.png)


而这两个是 $w$ 和 $b$ 的最优解的闭式 (closed-form) 解 。<span style="color:red;">什么是闭式解？</span>

其中 ![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/K2mkgHaIK0.png)为 $x$ 的均值。

OK，实际上上面，我们已经求出了，只有一个特征的时候，为了使 $f(x_i)$ 最近似于 $y_i$ ，此时的 $w$ 和 $b$ 的值。

下面，我们把它扩展到 $d$ 个属性的时候：


## $d$ 个特征的时候

### 当有多个特征的时候的距离

此时我们试图学得：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/jL0heg6BKG.png)

这称为"多元线性回归" (multivariate linear regression）。

同样的，可以使用最小二乘法来对 $w$ 和 $b$ 进行估计。为便于讨论，我们把 $w$ 和 $b$ 吸收入向量形式 $\hat{w}=(w;b)$ ，相应的，把数据集 $D$ 表示为一个 $m \times (d + 1)$ 大小的矩阵 $X$ ，其中每行对应于一个示例，该行前 $d$ 个元素对应于示例的 $d$ 个属性值，最后一个元素恒置为 1 :

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/Id1FjgdiL5.png)

再把标记也写成向量形式 $y=(y_1;y_2; \cdots ;y_m)$， 则有：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/a59I80hL1B.png)

<span style="color:red;">是的，这个没问题，右边是距离的平方。仍然是求使距离最小的时候的参数。</span>

### 这时候的求导

<span style="color:red;">这里的推导没看懂，包括下面对满秩矩阵和非满秩的分析。</span>

令![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/KmBACIAaK9.png)，对 $\hat{w}$ 求导得到：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/a8883KCF13.png)

<span style="color:red;">怎么对矩阵求导的？一直不知道</span>

令上式为零可得 $\hat{w}$ 最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些.下面我们做一个简单的讨论.


#### $X^TX$ 为满秩矩阵的时候


当 $X^TX$ 为满秩矩阵 (full-rank matrix)或正走矩阵 (positive definite matrix) 时，令上式为0可得：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/jilFKjg5ha.png)

其中：$(X^TX)^{-1}$ 是矩阵 $(X^TX)$ 的逆矩阵，令 $\hat{x_i}=(x_i,1)$ ,则最终学得的多元线性回归模型为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/K2Am7CcJF5.png)

#### $X^TX$ 为不满秩怎么办？

然而，现实任务中 $X^TX$ 往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致 $X$ 的列数多于行数 ，$X^TX$ 显然不满秩。此时可解出多个 $\hat{w}$，它们都能使均方误差最小化。

选择哪一个解作为输出 ，将由学习算法的归纳偏好决定，常见的做法是引入正则化 (regularization) 项。<span style="color:red;">什么叫学习算法的归纳偏好？为什么正则化可以调整？正则化不是为了降低模型复杂度的过拟合问题的吗？</span>

## 从线性回归到广义线性模型

嗯，其实，到上面这里，线性回归的主要东西就已经都讲了。

那么，我们现在继续想一下：

线性模型虽简单，其实却有可以有丰富的变化的，之前我们想让预测值逼近真实标记 $y$ 的时候， 就得到了线性回归模型：

$$y=w^Tx+b$$

OK，那么可否令模型预测值逼近 $y$ 的衍生物呢？比如 $lny$ ？OK，我们先假设样本的目标 $y$ 实际上是在指数尺度上变化的，那么，我们就可以将输出标记的对数作为线性模型逼近的目标 ，即：<span style="color:red;">为什么可以这么假定？即使从领域知识也不能得出这样的假定吧？</span>

$$lny=w^Tx+b$$

而这，就是 "对数线性回归" (log-linear regression) ，它实际上是在试图让 $e^{w^Tx+b}$ 逼近 y。实际上，上面这个式子虽然在形式上仍是线性回归，但实质上已经是在求取输入空间到输出空间的非线性函数映射了，如图所示：<span style="color:red;">是的，的确是输入空间到输出空间的非线性映射。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/42bCB3hLib.png)


而这里面的对数函数则起到了将线性回归模型的预测值与真实标记联系起来的作用。<span style="color:red;">厉害呀，但是，怎么能从一开始就知道要加上这个对数函数？</span>

更一般地，考虑单调可微函数 $g(\cdot )$， $g(\cdot )$ 连续且充分光滑
令：<span style="color:red;">为什么一定是一个单调可微函数呢？</span>

$$y=g^{-1}(w^Tx+b)$$

这样得到的模型称为 "广义线性模型" (generalized linear model)，其中 函数 $g(\cdot )$ 称为"联系函数" (link funtin)。显然，对数线性回归是广义线性模型在 $g(\cdot )=ln(\cdot )$ 时的特例。<span style="color:red;">嗯，是的，这样的广义线性模型威力还是很大的感觉，但是这种联系函数我们在一开始的时候怎么知道呢？</span>

注：广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。<span style="color:red;">厉害了，要总结下，而且，什么是加权最小二乘法？到底要怎么计算广义线性模型的参数估计？</span>


# REF

1. 《机器学习》周志华
