
包裹式选择

与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集.

一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此 从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但另一方面， 由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多.


LVW (Las Vegas Wrapper) 是一个典型的包裹式特征选择方法.它在拉斯维加斯方法(Las Vegas method)框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则。算法描述如图11.1所示.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/9K4mHIe7lE.png?imageslim)


图11.1算法第 8 行是通过在数据集 D 上，使用交叉验证法来估计学习器 $\pounds$ 的误差，注意这个误差是在仅考虑特征子集 $A'$ 时得到的，即特征子集 $A'$ 上的误差，若它比当前特征子集 $A$ 上的误差更小，或误差相当但 $A'$ 中包含的特征 数更少，则将 $A'$ 保留下来.

需注意的是，由于 LVW 算法中特征子集搜索采用了随机策略，而每次特征子集评价都需训练学习器，计算开销很大，因此算法设置了停止条件控制参数 T.然而，整个 LVW 算法是基于拉斯维加斯方法框架，若初始特征数很多(即 $|A|$ 很大)、 $T$ 设置较大，则算法可能运行很长时间都达不到停止条件。换言之，若有运行时间限制，则有可能给不出解.






# REF
1. 《机器学习》周志华
