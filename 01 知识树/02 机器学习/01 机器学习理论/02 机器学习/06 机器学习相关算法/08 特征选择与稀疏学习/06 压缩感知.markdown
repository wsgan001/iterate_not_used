








压缩感知


在现实任务中，我们常希望根据部分信息来恢复全部信息.例如在数据通讯中要将模拟信号转换为数字信号，根据奈奎斯特(Nyquist)采样定理，令采样频率达到模拟信号最高频率的两倍，则采样后的数字信号就保留了模拟信号的全部信息；换言之，由此获得的数字信号能精确重构原模拟信号.然而，为了便于传输、存储，在实践中人们通常对采样的数字信号进行压缩，这有可能损失一些信息，而在信号传输过程中，由于信道出现丢包等问题，又可能损失部分信息.那么，接收方基于收到的信号，能否精确地重构出原信号呢？压缩感知(compressed sensing)为解决此类问题提 供了新的思路。




假定有长度为 m 的离散信号 x ,不妨假定我们以远小于奈奎斯特采样定理 要求的采样率进行采样，得到长度为 n 的采样后信号 y ,$n\ll m$ ，即

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/2AAHkF4gEb.png?imageslim)

其中 $\Phi\in \mathbb{R}^{m\times m}$ 是对信号 x 的测量矩阵，它确定了以什么频率进行采样以及如何将采样样本组成采样后的信号.

在已知离散信号 x 和测量矩阵 $\Phi$ 时要得到测量值 y 很容易，然而，若将测量值和测量矩阵传输出去，接收方能还原出原始信号 x 吗？

一般来说，答案是 “No”，这是由于 $n\ll m$ ，因此 $y$,$x$,$\Phi$ 组成的式(11.19)是一个欠定方程，无法轻易求出数值解.


现在不妨假设存在某个线性变换 $\Psi \in \mathbb{R}^{m\times m}$ 。使得 x 可表示为 $\Psi s$ 于是 y 可表示为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/fgdKhDJmk4.png?imageslim)

其中 $A=\Phi \Psi\in\mathbb{R}^{n\times m}$ 。于是，若能根据 y 恢复出 s ，则可通过 $x=\Psi s$ 来恢复出信号 x.

粗看起来式(11.20)没有解决任何问题，因为式(11.20)中恢复信号 s 这个逆问题仍是欠定的。然而有趣的是，若 s 具有稀疏性，则这个问题竟能很好地得以解决！这是因为稀疏性使得未知因素的影响大为减少。此时式(11.20)中的 $\Psi$  称为稀疏基，而 A 的作用则类似于字典，能将信号转换为稀疏表示.

事实上，在很多应用中均可获得具有稀疏性的 s ，例如图像或声音的数字信号通常在时域上不具有稀疏性，但经过傅里叶变换、余弦变换、小波变换等处理后却会转化为频域上的稀疏信号.

显然，与特征选择、稀疏表示不同，压缩感知关注的是如何利用信号本身所具有的稀疏性,从部分观测样本中恢复原信号.通常认为，压缩感知分为“感知测量”和“重构恢复”这两个阶段.“感知测量”关注如何对原始信号进行 处理以获得稀疏样本表示，这方面的内容涉及傅里叶变换、小波变换以及 11.5 节介绍的字典学习、稀疏编码等，不少技术在压缩感知提出之前就已在信号处理等领域有很多研究；“重构恢复” 关注的是如何基于稀疏性从少量观测中恢复原信号，这是压缩感知的精髓，当我们谈到压缩感知时，通常是指该部分。

压缩感知的相关理论比较复杂，下面仅简要介绍一下 “限定等距性” (Restricted Isometry Property,简称 RIP)

对大小为 $n\times m$ $(n\ll m)$ 的矩阵 A ,若存在常数 $\delta_k\in (0,1)$ 使得对于任意向量 s 和 A 的所有子矩阵 $A_k\in \mathbb{R}^{n\times k}$ 有

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/l61bh9de2h.png?imageslim)

则称 A 满足 k 限定等距性(k-RIP).此时可通过下面的优化问题近乎完美地从 y 中恢复出稀疏信号 s ,进而恢复出 x :

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/A52Gl9jFJC.png?imageslim)


然而,式(11.22)涉及 $L_0$ 范数最小化,这是个 NP 难问题.值得庆幸的是， $L_1$  范数最小化在一定条件下与 $L_0$ 范数最小化问题共解,于是实际上只需关注

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/e7jkAD89Bm.png?imageslim)

这样，压缩感知问题就可通过 $L_1$ 范数最小化问题求解，例如式(11.23)可转化为 LASSO 的等价形式再通过近端梯度下降法求解，即使用 “基寻踪去噪” (Basis Pursuit De-Noising)。


基于部分信息来恢复全部信息的技术在许多现实任务中有重要应用.例如网上书店通过收集读者在网上对书的评价，可根据读者的读书偏好来进行新书推荐，从而达到定向广告投放的效果.显然，没有哪位读者读过所有的书，也没有哪本书被所有读者读过，因此，网上书店所搜集到的仅有部分信息.

例如 表11.1给出了四位读者的网上评价信息，这里评价信息经过处理，形成了“喜好程度”评分(5分最高).由于读者仅对读过的书给出评价，因此表中出现了很多未知项 “?”.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/d5806L76al.png?imageslim)


那么，能否将表 11.1 中通过读者评价得到的数据当作部分信号，基于压缩 感知的思想恢复出完整信号呢？

我们知道，能通过压缩感知技术恢复欠采样信号的前提条件之一是信号有稀疏表示。读书喜好数据是否存在稀疏表示哌？答案是肯定的.一般情形 下，读者对书籍的评价取决于题材、作者、装帧等多种因素，为简化讨论，假定表11.1中的读者喜好评分仅与题材有关.《笑傲江湖》和《云海玉弓缘》是武侠小说，《万历十五年》和《人¥的故事》是历史读物，《人间词话》属于诗词文学.一般来说，相似题材的书籍会有相似的读者，若能将书籍按题材归类, 则题材总数必然远远少于书籍总数，因此从题材的角度来看，表11.1中反映出的信号应该是稀疏的.于是，应能通过类似压缩感知的思想加以处理.


矩阵补全(matrix completion)技术可用于解决 这个问题，其形式为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/1F3f41lmGA.png?imageslim)


其中，$X$ 表示需恢复的稀疏信号; $rank(X)$ 表示矩阵$X$ 的秩;$A$是如表11.1的读者评分矩阵这样的已观测信号； $\Omega$ 是 A 中非 “?” 元素 $(A)_{ij}$ 的下标 $(i,j)$ 的集合。式(11.24)的约束项明确指出，恢复出的矩阵中 $(X)_{ij}$ 应当与已观测到的对应元素相同.


与式(11.22)相似，式(11.24)也是二个 NP 难问题.注意到 $rank(X)$ 在集合  $\{X\in\mathbb{R}^{m\times n}:||X||_F^2\leq 1\}$ 上的凸包是 $X$ 的“核范数”(nuclear norm):

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/hGmm10BdIb.png?imageslim)


其中 $\rho_j(X)$ 表示 $X$ 的奇异值，即矩阵的核范数为矩阵的奇异值之和，于是可通过最小化矩阵核范数来近似求解式(11.24)，即

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/679i4c2mmk.png?imageslim)


式(11.26)是一个凸优化问题，可通过半正定规划(Semi-Definite Programming, 简称SDP)求解。理论研究表明，在满足一定条件时，若 $A$ 的秩为 $r$，$n\ll m$ ,则只需观察到 $O(mr\,log^2m)$ 个元素就能完美恢复出 $A$ 。






# COMMENT

下面是辅助阅读


特征选择是机器学习中研究最早的分支领域之一，早期研究主要是按特 征子集“生成与搜索-评价”过程进行.在子集生成与搜索方面引入了很多 人工智能搜索技术，如分支限界法［Narendra and Pukunaga, 1977］ ＞浮动搜索 法［Pudil et al, 1994］等；在子集评价方面则采用了很多源于信息论的准则， 如信息熵、AIC (Akaike Information Criterion) ［Akaike, 1974］等.［Blum and Langley, 1997］对子集评价准则进行了讨论，［Forman, 2003］则进行了很多实验 比较..

早期特征选择方法主要是过滤式的，包裹式方法出现稍晚［Kohavi and John, 1997］,嵌入式方法事实上更晚［Weston et al., 2003］,但由于决策树算法 在构建树的同时也可看作进行了特征选择，因此嵌入式方法也可追溯到ID3 ［Quinlan, 1986］.有很多文献对特征选择方法的性能进行了实验比较［Yang and Pederson, 1997; Jain and Zongker, 1997］.更多关于特征选择的内容可参 阅［Guyon and Elisseeff，2003; Liu et al., 2010］，以及专门关于特征选择的书籍

[Liu and Motoda, 1998, 2007].

直译为“最小角面归”， 通常直接称LARS.


LARS (Least Angle Regression) [Efron et al., 2004]是一种嵌入式特征 选择方法，它基于线性回归平方误差最小化，每次选择一个与残差相关性最 大的特征.LASSO [Tibshirani, 1996]可通过对LARS稍加修改而实现.在 LASSO基础上进一步发展出考虑特征分组结构的Group LASSO [Yuan and Lin, 2006]、考虑特征序结构的 Fused LASSO [Tibshirani et al., 2005]等变体. 由于凸性不严格，LASSO类方法可能产生多个解，该问题通过弹性网(Elastic Net)得以解决[Zou and Hastie，2005].

仍以汉语文档为例，一 个概念可能由多个字词来 表达，这些字词就构成了 一个分组；若这个概念在 文档中没有出现，则这整 个分组所对应的变量都将 为零.


对字典学习与稀疏编碍[Aharon et al, 2006],除了通过控制字典规模从 而影响稀疏性，有时还希望控制字典的“结构”，例如假设字典具有“分组 结构”，即同一个分组内的变量或同为非零，或同为零.这样的性质称为“分 组稀疏性”(group sparsity),相应的稀疏编码方法则称为分组稀疏编码(group sparse coding) [Bengio et al., 2009].稀疏编码和分组稀疏编码在图像特征抽取 方面有很多应用，可参阅[Mairal et al.，2008; Wang et al.5 2010].

压缩感知[Donoho, 2006; Candes et al., 2006]直接催生了人脸识别的 鲁棒主成分分析[Candes et al.5 2011]和基于矩阵补全的协同过滤[Recht et al., 2010]. [Baraniuk, 2007]是关于压缩感知的一个简短介绍.将Lq范 数最小化转化为范数最小化后，常用求解方法除了转化为LASSO的 基寻踪去噪，还可使用基寻踪(Basis Pursuit) [Chen et al., 1998]＞匹配寻 踪(Matching Pursuit )[Mallat and Zhang, 1993]等.[Liu and Ye, 2009]使 用投影法快速求解稀疏学习问题，并提供了一个稀疏学习程序包SLEP (http: / / www.yelab.net/software/SLEP/).







# REF
1. 《机器学习》周志华
