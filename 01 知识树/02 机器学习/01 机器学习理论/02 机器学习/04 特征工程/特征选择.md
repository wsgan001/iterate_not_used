# 特征选择


TODO

* 以前一直以为降维就是对于特征的缩减，没想到还有特征选择这个过程。
* **这个特征选择是在数据集拆分之前还是之后？**
* **里面的一些方法需要仔细看一下，只是看概念不知道在实际中是怎么使用的。**



# 特征选择的概念？


特征选择是一个从给定的特征集合中选择与当前学习任务相关的特征的过程。特征选择中所谓的 “无关特征” 是指与当前学习任务无关，比如有一类特征称为 “冗余特征” 可以从其他特征中推演出来，它在很多时候是不起作用的，并且会增加学习过程的负担。


# 常用的特征选择方法：


大致可分为三类：过滤式、包裹式和嵌入式。




* 过滤式选择：先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。典型算法为 Relief 算法。方差选择法


* 相关系数法


* 卡方检验


* 互信息法


* 包裹式选择：选择直接把最终将要使用的学习器的性能作为特征子集的评价标准。典型算法为 LVM（Las Vegas Wrapper）。递归特征消除法


* 嵌入式选择：将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。典型算法为岭回归（ridge regression）、LASSO 回归（Least Absolute Shrinkage and Selection Operator）等。基于惩罚项的特征选择法


* 基于树模型的特征选择法


良好特征的特点包括

* 避免很少使用的离散特征值：良好的特征值应该在数据集中出现大约 5 次以上
* 最好具有清晰明确的含义
* 良好的浮点特征不包含超出范围的异常断点或 “神奇” 的值
* 特征的定义不应随时间发生变化














## REF

1. [特征工程](https://feisky.xyz/machine-learning/basic/feature-engineering.html)
