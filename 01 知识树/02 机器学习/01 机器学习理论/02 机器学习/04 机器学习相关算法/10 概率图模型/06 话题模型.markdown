
话题模型




话题模型(topic model)是一族生成式有向图模型，主要用于处理离散型的 数据(如文本集合)，在信息检索、自然语言处理等领域有广泛应用。隐狄利克雷分配模型(Latent Dirichlet Allocation,简称LDA)是话题模型的典型代表.



我们先来了解一下话题模型中的几个概念：词(word)、文档(document)和 话题(topic).具体来说：

- “词”是待处理数据的基本离散单元，例如在文本处理 任务中，一个词就是一个英文单词或有独立意义的中文词.
- “文档”是待处理 的数据对象，它由一组词组成，这些词在文档中是不计顺序的，例如一篇论文、 一个网页都可看作一个文档；这样的表示方式称为“词袋”(bag-of-words).数 据对象只要能用词袋描述，就可使用话题模型.
- “话题”表示一个概念，具体表 示为一系列相关的词，以及它们在该概念下出现的概率.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/ikfEl45aL5.png?imageslim)

形象地说，如图14.11所示，一个话题就像是一个箱子，里面装着在这 个概念下出现概率较高的那些词.不妨假定数据集中一共包含 K 个话题和 T 篇文档，文档中的词来自一个包含 $N$ 个词的词典.我们用 T 个 $N$ 维向量 $W=\{w_1,w_2,\cdots,w_T\}$ 表示数据集(即文档集合)，K个 $N$ 维向量典 $\beta_k=(k=1,2,\cdots,K)$ 表示话题，其中 $w_t\in\mathbb{R}^N$ 的第 n 个分量 $w_{t,n}$ 表示文档 t 中词 n 的词频，$\beta_k\in \mathbb{R}^N$ 的第 n 个分量 $\beta_{k,n}$ 表示话题 k 中词 n 的词频.



在现实任务中可通过统计文档中出现的词来获得词频向量 $w_i(i=1,2,\cdots ,T)$ ，但通常并不知道这组文档谈论了哪些话题，也不知道每篇文档与哪些话题有关。LDA从生成式模型的角度来看待文档和话题.具体来说，LDA认为每篇文档包含多个话题，不妨用向量 $\Theta_t\in\mathbb{R}^K$  表示文档 $t$ 中所包含的每个话题的比例，$\Theta_{t,k}$ 即表示文档 t 中包含话题 k 的比例，进而通过下面的步骤由话题“生成”文档 t:

1. 根据参数为 $\alpha$ 的狄利克雷分布随机采样一个话题分布 $\Theta_t$ ;
2. 按如下步骤生成文档中的 $N$ 个词：
    - 根据 $\Theta_t$ 进行话题指派，得到文档 t 中词 n 的话题 $z_{t,n}$
    - 根据指派的话题所对应的词频分布 $\beta_k$ 随机采样生成词.

图14.11演示出根据以上步骤生成文档的过程.显然，这样生成的文档自 然地以不同比例包含多个话题（步骤1）,文档中的每个词来自一个话题（步骤 2b）,而这个话题是依据话题比例产生的（步骤2a）.

图14.12描述了 LDA 的变量关系，其中文档中的词频 $w_{t,n}$ 是唯一的已观测变量，它依赖于对这个词进行的话题指派 $z_{t,n}$ ,以及话题所对应的词频 $\beta_k$ ;同时，话题指派 $z_{t,n}$ 依赖于话题分布$\Theta_t$,$\Theta_t$ 依赖于狄利克雷分布的参数 $\alpha$ ,而话题词频则依赖于参数 $\eta$ .


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/49mBjh6KDL.png?imageslim)

于是,LDA模型对应的概率分布为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/97HmmlflE7.png?imageslim)


其中 $p(\Theta_t|\alpha)$ 和 $p(\beta_k|\eta)$ 通常分别设置为以 $\alpha$ 和$\eta$ 为参数的 $K$ 维和 $N$ 维狄利克雷分布，例如

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/A37j105cEK.png?imageslim)


其中 $\Gamma(\cdot)$ 是 Gamma 函数.显然，$\alpha$ 和 $\eta$ 是模型式(14.41)中待确定的参数.

给定训练数据 $W=\{w_1,w_2,\cdots,w_T\}$ , LDA 的模型参数可通过极大似然法估计，即寻找 $\alpha$ 和 $\eta$ 以最大化对数似然

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/6bebC0lggH.png?imageslim)

但由于 $p(w_t|\alpha,\eta)$ 不易计算，式(14.43)难以直接求解，因此实践中常采用变分法来求取近似解.

若模型已知，即参数 $\alpha$ 和 $\eta$ 已确定，则根据词频 $w_{t,n}$ 来推断文档集所对应的话题结构(即推断 $\Theta_t$ ,$\beta_k$ 和 $z_{t,n}$ )可通过求解

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/h5IA1F7kjh.png?imageslim)


然而由于分母上的 $p(W|\alpha,\eta)$ 难以获取，式(14.44)难以直接求解，因此在实践中常采用吉布斯采样或变分法进行近似推断.








# COMMENT

下面是补充阅读。




概率图模型方面已经有专门的书籍如［Koller and Friedman, 2009］.

［Pearl, 1982］倡导了贝叶斯网的研究，［Pearl, 1988］对这方面的早期研究工 作进行了总结.马尔可夫随机场由［Geman and Geman, 1984］提出.现实应用 中使用的模型经常是贝叶斯网与马尔可夫随机场的结合.隐马尔可夫模型及 其在语音识别中的应用可参阅［Rabiner, 1989］.条件随机场由［Lafferty et al., 2001］提出，更多的内容可参阅［Sutton and McCallum, 2012］.

信念传播算法最早由［Pearl, 1986］作为精确推断技术提出，后来衍生出 多种近似推断算法.对一般的带环图，信念传播算法需在初始化、消息传递 等环节进行调整，由此形成了迭代信念传播算法(Loopy Belief Propagation) ［Murphy et al., 1999］,但其理论性质尚不清楚，这方面的进展可参阅［Mooij and Kappen, 2007; Weiss, 2000］.有些带环图可先用“因子图”(factor graph) ［Kschischang et al., 2001］描述，再转化为因子树(factor tree)进行信念传播.对 任意图结构的信念传播已有一些研究［Lauritzen and Spiegelhalter, 1988］.近来 随着并行计算技术的发展，信念传播的并行加速实现受到关注，例如［Gonzalez et al, 2009］提出Te近似推断的概念并设计出多核并行信念传播算法,其时间 开销随内核数的增加而线性降低.

概率图模型的建模和推断，尤其是变分推断在20世纪90年代中期逐步发 展成熟，［Jordan, 1998］对这个阶段的主要成果进行了总结.关于变分推断的更 多内容可参阅［Wainwright and Jordan, 2008］.

“非参数化”指参数的 数目无领事先指定，是贝 叶斯学习方法的重要发展.

贝叶斯学习参见p.164.


图模型带来的一大好处是使得人们能直观、快速地针对具体任务定义模 型.LDA ［Blei et al., 2003］是这方面的重要代表，由它产生了很多变体,关于这 方面的内容可参阅［Blei, 2012］.概率图模型的一个发展方向是使得模型的结构 能对数据有一定的自适应能力，即“非参数化” (non-parametric)方法，例如层 次化狄利克雷过程模型［Teh et al., 2006］＞无限隐特征模型［Ghahramani and Griffiths, 2006］等.

LSA是SVD在文本数据 上的变体.

参见p.266.


话题模型包含了多种模型,其中有些并不采用贝叶斯学习方法,例如PLSA (概率隐语义分析)［Hofmann, 2001］,它是LSA (隐语义分析)的概率扩展.

蒙特卡罗方法是二十世纪四十年代产生的一类基于概率统计理论、使用 随机数来解决问题的数值计算方法，MCMC是马尔可夫链与蒙特卡罗方法的 结合，最早由［Pearl, 1987］引入贝叶斯网推断.关于MCMC在概率推断中的应 用可参阅［Neal, 1993］,更多关于MCMC的内容可参阅［Andrieu et al., 2003; Gilks et al., 1996］.


# REF
1. 《机器学习》周志华
