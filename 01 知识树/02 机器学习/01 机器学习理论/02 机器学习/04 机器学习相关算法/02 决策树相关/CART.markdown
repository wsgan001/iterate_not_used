# CART



TODO

* 要系统的总结下 CART 树


之前在决策树文中，只是提了一下CART用的度量方法是基尼指数，再也没有更多信息了。。
难道CART与 ID3 和 C4.5 只有这点不同吗？


# CART介绍




## 回到对于决策树的介绍


构建决策树算法，最常用到的是三个方法: ID3, C4.5, CART

这三种方法的区别只是划分树的分支的方式不同：




  * ID3 是用信息增益来划分分支


  * C4.5 是用信息增益率来划分分支


  * CART 是用 GINI 系数来划分分支


其中 CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；


## 什么是 分类回归树 CART？


CART(Classification And Regression Trees， 分类回归树) 是一种决策树算法，该算法既可以用于分类还可以用于回归。**怎么用于回归的？**


## CART 与 ID3 和 C4.5的比较




以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。

首先，我们看一下ID3，ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说：




  * 一种方法是：如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，后面的切分过程中只会对应别的特征。因此有的观点认为这种切分方式过于迅速。**嗯，好像是过于迅速了。这句是不是应该放在下面？二分法好像切分也很迅速。**


  * 另外一种方法：二分切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。**是等于进入左子树吗？是大于的时候吧？确认下。**


除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但是，这种连续型转换成离散的过程会破坏连续型变量的内在性质。**为什么会破坏内在本质？什么内在本质？**

而使用二分切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二分切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。**什么情况？怎么突然讲起二分切分法了？**

CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第 3 章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。

回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。


## 为什么要引入分类回归树？


之前我们学习了线性回归的一些强大的方法，比如岭回归什么的，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。因此，它实际上是有些局限性的，比如：




  * 当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。


  * 实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。


那么怎么办呢？

一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。**还是没明白什么是树回归？**


## 要怎么计算连续型数值的混乱度呢？


为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。之前我们做决策树的时候，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？

在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。

上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。




# 树回归 算法特点

优点：
  * 可以对复杂和非线性的数据建模。
缺点：
  * 结果不易理解。
适用数据类型：
  * 数值型和标称型数据。





## REF

1.[第9章 树回归](http://ml.apachecn.org/mlia/tree-regress/)
