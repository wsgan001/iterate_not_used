# 剪枝处理


TODO

* **实际上，我一直决定剪枝很神秘的，看了这一节之后才觉得，嗯，原来是这样。**
* **为什么我之前有看到说，剪枝的时候要涉及到树的重新排列呢？看错了吗？**







## 什么是剪枝？


剪枝 (priming) 是决策树学习算法对付 “过拟合” 的主要手段。

在决策树学习的时候中，不断的分支，可能会造成过拟合，这个时候就可以通过主动去掉一些分支来降低过拟合的风险。这就是剪枝。<span style="color:red;">是的，好像是有道理的。</span>


## 两种剪枝策略

决策树剪枝的基本策略有两种：

- 预剪枝 (prepruning)
- 后剪枝 (post-pruning)


- 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。<span style="color:red;">怎么知道这个划分不能带来决策树泛化性能提升？</span>
- 后剪枝则是先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能够带来决策树泛化性能提升，那么将该子树替换为叶结点。<span style="color:red;">嗯，这个倒是好理解，但是怎么做呢？</span>


<span style="color:red;">这两种剪枝的方法听着还是很清楚的。</span>

## 剪枝前的准备

## 怎么知道对于这个结点的划分能不能带来决策树泛化性能的提升呢

OK，我们看了上面这两种方法，马上就有一个问题：

我们怎么知道对于这个结点的划分能不能带来决策树泛化性能的提升呢？

实际上，能不能带来泛化性能的提升，我们只能通过在验证集上进行验证得到，至于如何划分数据集，参考 [划分好测试集、验证集，为评估做准备] 。<span style="color:red;">好吧，我竟然没想到。</span>


## 划分出验证集


OK，我们这里使用的是留出法，即预留一部分数据用作 “验证集” 以进行性能评估。

我们现在的数据集如下：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/cib75L00df.png?imageslim)

现在，我们把它随机划分为两部分：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/6hm6IBbcim.png?imageslim)


嗯，如上图所示：

* 训练集为 ｛1,2,3,6,7,10,14,15,16,17｝
* 验证集为 ｛4,5,8,9,11,12,13｝


OK，现在假定我们采用信息增益准则来做属性划分，那么我们从上表中的训练集中会产生这样一棵决策树：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/mLGFgC43Ca.png?imageslim)


为便于讨论，我们对图中的部分结点做了编号。

OK，准备已经做好了，下面重点讲一下两种剪枝方法：




# 预剪枝策略

## 仔细进行预剪枝

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/43eE2Ig7G0.png?imageslim)


OK，我们来讨论一下预剪枝，基于信息増益准则，我们会选取属性 “脐部” 来对训练集进行划分，并产生 3 个分支，如图 4.6 所示。

OK，这个时候，我们就要来判断了，要不要进行这个划分呢？

这个时候，我们就要看下划分前后的泛化性能了。

* 如果不进行划分，也就是说这个结点将被标记为叶结点，那么这个结点的类别将被标记为训练样本数最多的类别，即 “好瓜” 类， OK，然后，我们用验证集对这个单结点的决策树进行评估，样本 {4,5,8} 分类正确，另外 4 个样本分类错误，那么，这个时候的验证集的精度为 $\frac{3}{7}\times 100\% = 42.9\%$。<span style="color:red;">是对这个决策树进行评估吗？不是单单对节点进行评估是吧？</span>
* 如果用属性 “脐部” 进行划分，那么划分之后，分成了 ②、③、④ 三类，分别包含编号为{1,2,3,14}、{6,7,15,17}、{10,16} 的训练样本，每个结点的类别就是这个结点的样本最多的类别，即： “好瓜”、“好瓜”、“坏瓜”。OK，然后，我们用验证集对这个决策树进行评估，验证集中编号为 {4,5,8,11,12} 的样例被分类正确，验证集精度为 $\frac{5}{7}\times  100\% =71.4\%$ 。


$71.4\% > 42.9\%$ ，因此，可以用 “脐部” 进行划分。

<span style="color:red;">注意上面这个过程与决策树的生成时候的区别。</span>

OK，决策树算法继续对结点 ② 进行划分，基于信息增益准则，我们挑选出属性 “色泽” 。然而，在使用 “色泽” 划分后，计算出的验证集的精度为 57.1% ，相对于 71.4% 下降了，因此这个结点 ② 不能被。<span style="color:red;">解释一下****：首先，这个色泽的特征是通过信息增益准则挑选出来的，也就是说，这个对于结点 ② 来说已经是最好的选择了，但是，随后的验证却发现这个特征虽然是最好的，但是造成的效果却比原来差。因此这个时候就没有必要再尝试别的特征了。是这样吗？为什么？</span>

类似的，对于结点 ③ ，最优划分属性为 "根蒂"，划分后验证集精度还是 71.4%。也就是说这个划分不能提升验证集精度，因此，这个结点 ③ 也被禁止划分。

对于结点 ④，它所含的训练样例已属于同一类，没法找到一个特征进行划分了，因此自然也是不用划分的。

OK，到这里，我们就从表4.2 的数据中生成了这个已经被剪枝好的树 图4.6 。它的验证集精度为  71.4%。

这是一棵仅有一层划分的决策树，也被称为决策树桩 (decision stump)。<span style="color:red;">注：决策树桩在随机森林中经常会用到。</span>


## 预剪枝策略的优缺点

我们可以对比一下 图4.6 和图 4.5，可以看到，预剪枝使得决策树的很多分支都没有 “展开”。我们很容易就能列出这样的剪枝的优点：

* 降低了过拟合的风险。
* 显著减少了决策树的训练时间开销和测试时间开销。


OK，那么这样的预剪枝是十全十美的吗？

实际上，还是有点问题的：


  * 有些分支的当前划分虽然这次不能提升泛化性能、甚至可能导致泛化性能的暂时下降，但是在它基础上进行的后续划分却是有可能导致性能显著提高的。而预剪枝策略基于 “贪心” 的本质禁止了这些分支展开，这就给预剪枝决策树带来了欠拟合的风险。<span style="color:red;">这个还是没有很理解？能不能举个例子？而且为什么会出现这种情况？这种方法的本质上有问题了吗？</span>


<span style="color:red;">上面这个缺点现在有什么说法吗？比如有什么降低欠拟合的风险的方法？</span>




# 后剪枝策略


OK，现在我们介绍一下后剪枝。

后剪枝它与预剪枝还是不一样的，它先从训练集生成一棵完整的决策树，如图4.5。这个时候，这个决策树的验证集精度为 42.9%
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/GE4bCh919E.png?imageslim)


OK，现在开始后剪枝：

我们先考察图4.5 中的结点 ⑥，如果我们将以它为父节点的分支剪除，也就是说把 ⑥ 替换为一个叶结点，由于替换后的叶结点包含着编号为 {7,15} 的训练样本，于是，这个叶结点的类别被标记为 “好瓜”，OK，这时我们看一下此时的决策树在验证集上的精度，为 57.1%。57.1% > 42.9%，因此我们决定剪枝。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/h44A5GEmDg.png?imageslim)


接着我们考察图4.5 中的结点 ⑤，如果把以它为父节点的子树替换为一个叶结点，替换后的叶结点包 含编号为 {6,7,15} 的训练样例，这个叶结点类别被标记为 “好瓜” ，此时的决策树验证集精度仍为57.1%。于是，我们可以不进行剪枝。<span style="color:red;">注意：此种情形下检证集精度虽无提高，但是根据奥卡姆剃刀准则，剪枝后的模型更好。因此，实际的决策树算法在此种情形下通常要进行剪枝，本书为绘图的方便，采取了不剪枝的保守策略。 实际上一般都是要剪的。这里图方便没减而已。</span>

类似的，我们对应结点 ②、结点 ③、结点 ①。

最终，我们就得到了图4.7 所示的决策树，其验证集精度为 71.4%。

<span style="color:red;">整体过程还是很清晰的。</span>




# 后剪枝与预剪枝的对比

OK，对比一下图4.7 和图4.6 ，我们可以看出：后剪枝决策树通常比预剪枝决策树保留了更多的分支。

一般情形下：
  * 欠拟合方面：后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。
  * 时间开销方面：后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。 <span style="color:red;">注意：是大得多。</span>


<span style="color:red;">那么一般的类库里面都是用的后剪枝还是预剪枝？默认使用什么？</span>



# REF
1. 《机器学习》周志华
