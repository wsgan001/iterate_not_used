# 决策树基本流程

TODO

- **还是没怎么明白 尤其是后面的先验概率和后验概率。**


## 决策树介绍

### 我们一般的决策过程是什么样子的？

决策树 (decision tree) 是一类常见的机器学习方法。顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

比如，我们要对 "这是好瓜吗?" 这样的问题进行决策时，通常会进行一系列的判断或 "子决策" ，我们先看 "它是什么颜色?"，如果是"青绿色"，则我们再看"它的根蒂是什么形态?"，等等，最后，得出一个结论，整个决策过程如图：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/CIFbe9L92k.png?imageslim)

<span style="color:red;">是的，还是很形象的。</span>

### 决策树可以用来解决什么问题？

决策树可以用来解决分类问题，以二分类任务为例，比如我们希望从样本中学得一个模型来对新示例进行分类，那么这个分类的任务，就可以看作是对 "当前样本属于 XX 类吗?" 这个问题一个决策过程。<span style="color:red;">决策树可以用于回归问题吗？</span>

显然 ，上面这个决策过程的最终结论对应了我们所希望的判定结果，例如 "是" 或 "不是" 好瓜。

而整个决策过程中提出的每个判定问题都是对某个属性的测试，例如 "色泽=?" ，每个测试的结果或是导出最终结论，或是导出进一步的判定问题，其考虑的范围是在上次决策结果的限定范围之内的。<span style="color:red;">是的额，那么落实下来是什么样的？</span>


### 一般的决策树是什么样子的？

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点。叶结点对应于决策结果，其他每个结点则对应于一个属性测试，每个结点包含的样本集合根据属性测试的结果被划分到子结点中，根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。

而决策树学习的目的就是想产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的 "分而治之" (divide-and-conquer) 策略。<span style="color:red;">什么是分而治之策略？是一种数学算法吗？</span>




## 生成一棵决策树

### 决策树的生成流程

决策树学习基本算法如下图所示：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/6aBKeL3IiF.png?imageslim)

<span style="color:red;">这种算法那流程图是怎么写出来的？我看很多算法都有这种流程图。写的时候有什么规范吗？</span>

<span style="color:red;">怎么选择划分属性的？</span>

显然，决策树的生成是一个递归过程。

在递归的过程中，有三种情形会导致递归返回:

1. 当前结点包含的样本全属于同一类别，说明这几个样本都属于同一类别，就不需要划分了。
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。应该是没有可以划分的属性值了，这几个样本虽然不知道 label 是不是相同的，但是他们所有的属性值都是相同的。
3. 当前结点包含的样本集合为空，不能划分。就是说，没有符合当前这个叶节点对应的属性限制的样本。

在递归返回时对应的处理如下：

- 对于第 1 种情况，说明这个已经正确分类了，哪个节点就是哪个类别。
- 对于第 2 种情况，我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别。<span style="color:red;">嗯，没问题。</span>
- 对于第 3 种情况，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。<span style="color:red;">为什么这么设置？</span>


注意，第2、3 这两种情形的处理实质是不同的：

- 情形 2 是在利用当前结点的后验分布。
- 情形 3 则是把父结点的样本分布作为当前结点的先验分布。

<span style="color:red;">没看懂，还是要看下什么是先验分布，什么是后验分布，然后再确认这个地方到底是什么。</span>


## REF

1. 《机器学习》周志华
