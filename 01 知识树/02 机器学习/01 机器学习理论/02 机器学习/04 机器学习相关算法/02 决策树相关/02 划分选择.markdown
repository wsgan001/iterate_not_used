



# TODO
* **还是理解的不够，公式的推导，和缘由不知道。**




由之前决策树的生成过程可以知道，决策树生成的关键就是如何选择最优划分属性。

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的 "纯度" (purity)越来越高。


# 怎么来衡量划分的优劣呢？


我们主要有三种方式来衡量划分的优劣：

  * 信息增益       对应 ID3 决策树学习算法
  * 信息增益率     对应 C4.5 决策树算法
  * 基尼系数       对应CART决策树




# 信息增益

## 对于样本集合来说，信息熵是什么？


"信息熵" (information entropy) 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$  中第 $k$  类样本所占的比例为  $p_k(k=1,2,\cdots ,|\mathcal{Y}|)$ ，则 $D$ 的信息熵定义为


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/175GfAEKK3.png?imageslim)


Ent(D)的最小值为0，最大值为 $log_2|\mathcal{Y}|$。Ent(D) 的值越小，则 D 的纯度越高。<span style="color:red;">嗯，这个式子是怎么来的呢？</span>

注意：在计算信息熵的时候，我们约定：若p=0，则 $plog_2p=0$。


## OK，我们看看某个属性的信息熵


假定离散属性 $\alpha$ 有 V 个可能的取值 $\{\alpha^1,\alpha^2,\cdots ,\alpha^V\}$，若使用 α 来对样本集 D 进行划分，则会产生 V 个分支结点，其中第 v 个分支结点包含了 D 中所有在属性 α 上取值为 $a^v$ 的样本，记为 $D^V$ 。


## 那么如果使用这个属性进行分类的话，信息增益是？


我们可根据上面这个信息熵的式子计算出 $D^V$ 的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 $|D^V|/|D|$ ，即样本数越多的分支结点的影响越大，于是可计算出用属性 α 对样本集 D 进行划分所获得的"信息增益" (information gain)：<span style="color:red;">嗯，看起来很公平。</span>


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/IH7c46ai0B.png?imageslim)


一般而言，信息增益越大，则意味着使用属性 α 来进行划分所获得的"纯度提升"越大。

因此，我们可用信息增益来进行决策树的划分属性选择，即我们选择的属性为：

$$a_*= \underset{a\in A}{arg\,max}\,Gain(D,α)$$

著名的 ID3 决策树学习算就是以信息增益为准则来选择划分属性的。


## OK，举个例子

OK，我们来根据上面的样本举个例子：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/Liia4JEC7j.png?imageslim)

以上表中的西瓜数据集为例：该数据集包含了 17 个训练样本，用来学习一棵西瓜决策树。显然，$|\mathcal{Y}|=2$。

在决策树学习开始时，根结点包含 D 中的所有样例，其中正例占 $p_1 =\frac{8}{17}$，反例占的 $p_2 =\frac{9}{17}$。

于是，计算出根结点的信息熵为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/mcjageJJBJ.png?imageslim)


然后，我们要计算出当前属性集合  {色泽，根蒂，敲声，纹理，脐部，触感}  中每个属性的信息增益。

以属性"色泽"为例：它有 3 个可能的取值:  {青绿，乌黑，浅自}，如果使用该属性对 D 进行划分，则可得到 3个子集，分别记为:

* $D^1$ (色泽=青绿)，
* $D^2$ (色泽=乌黑)，
* $D^3$ (色泽=浅白)。

分析每个子集：


* 子集 $D^1$ 包含编号为 { 1, 4, 6, 10, 13, 17 } 的 6 个样例，其中正例占 $p_1 =\frac{3}{6}$ ，反例占 $p_2 =\frac{3}{6}$;
* 子集 $D^2$ 包含编号为 { 2, 3, 7, 8, 9, 15 } 的 6 个样例，其中正、反例分别占 $p_1 =\frac{4}{6}$， $p_1 =\frac{2}{6}$
* 子集 $D^3$ 包含编号为 { 5, 11, 12, 14, 16 } 的 5 个样例，其中正、反例分别占 $p_1 =\frac{1}{5}$，$p_1 =\frac{4}{5}$


可以计算出用 "色泽" 划分之后所获得的 3 个分支结点的信息熵为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/4K7Bfe11gl.png?imageslim)


因此，可以计算出属性 "色泽" 的信息增益为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/738ehgmgLe.png?imageslim)


类似的，我们可计算出其他属性的信息增益:

  * Gain(D，根蒂) = 0.143
  * Gain(D，敲声) = 0.141
  * Gain(D，纹理) = 0.381
  * Gain(D，脐部) = 0.289
  * Gain(D，触感) = 0.006

显然，属性 "纹理" 的信息增益最大，于是它被选为划分属性。下图给出了基于 "纹理" 对根结点进行划分的结果，各分支结点所包含的样例子集显示在结点中.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/BeF6FHbm3G.png?imageslim)


OK，现在，第一步的划分已经结束了，我们使用的特是纹理。接下来，我们对每个分支结点做进一步划分。

以上图第一个分支结点 ( "纹理=清晰" ) 为例，该结点包含的样例集合 $D^1$ 中有编号为 {1 ,2, 3, 4, 5, 6, 8, 10, 15} 的 9 个样例，可用属性集合为{色泽，根蒂，敲声，脐部，触感}。那么基于 $D^1$ 我们可以计算出各属性的信息增益：

  * Gain($D^1$ ，色泽) = 0.043;
  * Gain($D^1$ ，根蒂) = 0.458;
  * Gain($D^1$ ，敲声) = 0.331;
  * Gain($D^1$ ，脐部) = 0.458;
  * Gain($D^1$ ，触感) = 0.458.

可见，"根蒂"、 "脐部"、 "触感" 3 个属性均取得了最大的信息增益，这里我们可以任选其中之一作为划分属性。

类似的，我们对每个分支结点都进行上述操作，最终得到的决策树如图所示：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/k0cg9dm8cm.png?imageslim)

<span style="color:red;">嗯，还是很清晰的。</span>


# 增益率


在上面的介绍中，我们有意忽略了数据集中的"编号"这一列，那么如果我们把"编号"也作为一个候选划分属性，会是什么样子的？

其实，我们可以计算出它的信息增益为 0.998，远大于其他候选划分属性。那么为什么呢？这其实很容易理解："编号" 会产生 17 个分支，而每个分支结点仅包含一个样本，而这些分支结点的纯度己经达到最大了。<span style="color:red;">没想到</span>，然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。<span style="color:red;">嗯，是的，分到最细的时候，信息增益是提高了，但是泛化能力几乎没有了。</span>

实际上，信息增益准则对可取值数目较多的属性是有所偏好的，<span style="color:red;">的确</span>，为了减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使用信息增益，而是使用 "增益率" (gain ratio) 来选择最优划分属性。

我们定义增益率为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/J5gbLGf3Fa.png?imageslim)

其中：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/943Bk7e7K9.png?imageslim)

称为属性 α 的 "固有值" (intrinsic value) 。属性 α 的可能取值数目越多(即 V 越大)，则 IV(α) 的值通常会越大。<span style="color:red;">这个固有值是怎么定的？为什么这么计算？</span>

比如，对于西瓜数据集 2.0，有

  * IV(触感) = 0.874 (V = 2),
  * IV(色泽) = 1.580 (V = 3),
  * IV(编号) = 4.088 (V = 17).


需注意的是，增益率准则对可取值数目较少的属性是有所偏好的。因此 ， C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<span style="color:red;">启发式是什么意思？</span>


# 基尼指数


<span style="color:red;">还是没怎么理解。</span>

CART 决策树使用 "基尼指数" (Gini index) 来选择划分属性。采用与信息熵相同的一些符号，数据集 D 的纯度可用基尼值来度量：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/KiGDAH12Bd.png?imageslim)


直观来说， Gini(D) 反映了从数据集中随机抽取两个样本，其类别标记不一致的概率.因此， Gini(D)越小，则数据集的纯度越高。<span style="color:red;">嗯，再理解下</span>

属性 α 的基尼指数定义为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/k14lLl82h0.png?imageslim)


于是，我们在候选属性集合 A 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/24BcI5dlbE.png?imageslim)





# REF
  1. 《机器学习》周志华
