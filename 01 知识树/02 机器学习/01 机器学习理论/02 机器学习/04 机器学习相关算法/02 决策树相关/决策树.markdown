# TODO
- 又添加了邹博的东西进去，要整理下。
- 实际上 关于决策树的生成，剪枝，新增样本时候怎么调整还是有一些问题不清楚的。要弄清楚。
- 而且没有实例，需要补充











  * a




# MOTIVE






  * 对决策树进行总结


  * 可见，学习一个技术知识，最好是找一个贴合的简单例子，从头到尾贯通一遍，再把理论和公式 重新梳理明确一遍，现在他只讲理论，配合简单的解释，有点云里雾里。





* * *





# 知识前提






  * 熵


  * 信息熵


  * 信息增益





# 主要内容：






  * 决策树的生成与裁剪算法  嗯


  * 然后基于决策树，提出了集成学习的思路，一个是Bagging，一个是Boosting 其中Bagging 的代表就是随机森林


  * 然后是RF的代码实现与数据实验  **这个没有，要拆分开来。**





# 基础知识






  * 熵计算


  * 条件熵




#




# 决策树的定义






  * 每个非叶节点标识一种对样本的分割，通常是选用样本的某个特征，将样本分散到不同的子节点中


  * 子节点继续对分散开的样本进行分割操作


  * 叶子节点标识输出，每个分散该叶节点中样本都属于同一类（或近似的回归值）




#




# 决策树的架构：


分为两个阶段：

决策树学习：




  * 一种根据样本为基础的归纳学习


  * 采用的是自顶向下的递归方法：开始数据都在根节点，递归的进行数据分片


  * 采用剪枝方法，防止过拟合


决策树的使用：


  * 对未知数据进行分类


  * 按照决策树上生成时所采用的分割属性逐层往下，直到一个叶子节点


** 怎么进行剪枝的？为什么用剪枝的方法可以防止过拟合？因为假如说我把所有的训练样本很好的分类，因为我的样本存在噪声，而且样本数可能不够。但是还是没有说为什么能防止过拟合？ **


# 决策树的示意




![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4c7b45643.png)


上图可以看出决策树的一个很重要的特点：它不挑特征的种类，比如可以是年龄，也可以是标记型数据，其他的比如SVM，logistic，一般都要求是标量，或者数字。

注意：决策树是可以多插树的。


# 那么我们怎么生成这样一个树呢？




## 首先，我们怎样实现分割：


可以选择一个特征，设定一个阈值（threshold），Decision Stump。当然，也可以使用两个特征，构造一个平面来划分。**这个是什么样的？**


## 什么样的分割是好的分割？


什么样的分割的分类效果最好（或分类最纯的，或能使树的路径最短的） 度量方法如下：**分类最纯的这个是什么意思？ **




  * 信息增益（ID3）


  * 信息增益率（C4.5）


  * 基尼指数（CART）


现在基本用的就是CART，ID3和C4.5现在已经难得用到了。

OK，我们挨个介绍一下：


## 信息增益 信息增益率


经验熵：某个节点分割前的熵值（empirical entropy）


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4d1d07b9f.png)


经验条件熵：分割后的熵值


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae065f1b63c7.png)


里面的求和是针对Di的。然后乘以前面的权重，再求和，就得到了经验条件熵。

那么信息增益呢？


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4d436ff51.png)


信息增益率呢？


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4d46818b1.png)


**这个地方写错了把？H(A)?**

而我们就旋转信息增益最大的那个分割，我们会在这个节点把所有的分割全部考察一遍。

实际上，最常用的并不是信息增益和增益率，而是基尼指数。


## 基尼指数


分割之前的计算：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4d6233242.png)


分割之后的计算：对每个子节点加权求和


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4d7ccb6bc.png)


感觉与上面的信息增益还是有些类似的。


## OK，总结一下生成方法


执行一个分割的信息增益越大，表明这个分割对样本的熵减少的能力越强，这个分割所在的特征使得数据由不确定性变成确定性的能力越强。

这三种方法的效果是很类似的，随机森林里面用的是CART这个算法就是基尼指数。**为什么另外两个不用？这个CART和RF是同一个发明人。**


# 怎么处理这样的完全树的过拟合？




## 过拟合的出现


由于：样本噪声，或者样本数量不足，会使得这样的对样本完全分类的树（完整树），容易发生过拟合，泛化能力较弱。


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4dec90a19.png)


样本量多少叫少？可以使用各种的验证方法来验证我的这个分类算法的泛化能力，比如交叉验证。**到底什么样的样本叫多还是少？ 什么叫做交叉验证？**

怎么解决这个过拟合的问题呢？




  * 可以对树进行剪枝。


  * 可以使用随机森林。 这个在集成学习里面会讲到，是基于决策树的。




## 怎么进行剪枝？


为了进行剪枝，我们需要对决策树进行评价，可以设计一个树的分类误差评价函数：


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae14edf2715d.png)


即对所有叶节点的熵加权求和：


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc4e2a30c27.png)


t是指所有叶子节点，Nt指的是在这个叶子节点下面的样本的数目，H(t)值得是这个叶子节点下面的熵值。如果说，t这个叶子下面都是一个样本，那么熵就是0，也就是说完整树的C(T)就是0，但是，假如样本里面有两个一模一样的树，然后分配个两个不同的属性，有这种可能。

实际上，决策树在决定那个特征作为分支的时候是比较贪心的，而且梯度下降也是一个比较贪心的算法。 **什么是贪心？**

可以使用叶节点数目复杂度评估函数，剪枝的目标就是在这两个函数之间做出平衡，设计出最后的评价函数


![](http://106.15.37.116/wp-content/uploads/2018/03/img_5abc7a5db8408.png)


因此树的剪枝，就是挑选出完整树的子树，从而使评价函数值最小


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae14f9c6fd52.png)





## 怎样进行剪枝？






  * 第一种方法，固定某个经验值α，生成唯一的使评价函数最小的树


  * 第二种方法：通过迭代操作，构造一系列不同的α值，但是评价函数近似的备选树，然后通过交叉验证的方法选择最好的树。




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae14faa65c46.png)


即固定权重，迭代生成备选树。

**怎么通过交叉验证来选择最好的树的？**


## 那么如何构造备选树呢？


**这个地方没有很明白，要弄明白。**




  1. 从完整树开始，当α为0的时候，明显完整树使最有数，即第一个备选树，评价函数值就是\(C(T_0)\)


  2. 现在裁剪一个r节点，其他部分不动，因此我们可以独立考察r节点构成的单节点树和根节点树的评价函数的变化     \(C_α(t_r)=C(t_r)+α\)        \(C_α(T_r)=C(T_r)+α|T_r|\)


  3. 当α比较小的时候，\(C_α(T_r)<C_α(t_r)\)，可以满满增大α，使得不等式变成等式，为了使得α满满增加，可以选择改动最小的节点 \(\frac{C_α(T_r)-C_α(t_r)}{|T_r|-1}\)  即相等的时候就可以算出这个\(\alpha\)值，**什么是改动最小的那个节点？**


  4. 当选择改动最小那个节点之后，我们可以增大α，使得不等式为等式


  5. 重复迭代这个过程，知道根节点为止，生成一系列备选树。




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae14fce325f1.png)


为什么要令二者相等？哦，我先算一个作为临界点，而相等的时候，就是剪枝前后的决策树的损失是相等的，把这个作为临界点，稍微调大一点，那么剪枝后的就是优的，如果比这个临界点小，那么剪枝前的就是优的。


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae14fe57d3e6.png) 这个是什么？叶子的个数吗？


那么我们怎么到底剪枝那些呢？


![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae15000e7189.png)


从T1到T2的过程中，你需要把所有的内部节点再算一遍的。

**为什么是查找最小剪枝系数的节点？为什么这个点是稳妥的？因为 \(\alpha=0\) 就是原始状态，等于∞就是树根的状态，因此稍微减一点。但是为什么平稳的是好的？**

这部分在李航的书里面还是有介绍的

一般样本分为三类，一个是做训练用的，一个是用来调节超参数用的，最后一个是用来做测试用的。也可能是分为两类，没有调节超参数的。


# 决策树的效果：




![](http://106.15.37.116/wp-content/uploads/2018/04/img_5ae11215e36cf.png)


就是这样一刀一刀切出来的。


# 决策树的优缺点：


优点：




  * 构造简单，判别计算快速


  * 对数据不需要任何加工，数据不需要任何加工，让本既可以是离散性也可以是连续型，而且数据也是允许缺失的。比如说20%的数据是没有的，但是这个对决策树，这个没有问题，如果是用线性的方法，缺少就没有办法对应，只能人工填上一个值。


  * 对unblance的数据效果很好。


缺点：


  * 泛化能力差，容易出现过拟合


  * 对新增加的样本，需要重新调整树的结构。如果新增加的一个样本，如果分错了，那么这个树就不是一个最优树了，那么就需要重新调整。**怎么进行调整？**





# 使用决策树算法


需要注意的：树构造算法只适用于标称型数据，因此数值型数据必须离散化。

决策树算法在进行测试的时候使用经验树计算错误率。**什么是经验树？**

构造完树之后，最好把树画出来看看是否符合你的预期。


# 决策树算法的特点


优点：




  * 计算复杂度不高


  * 输出结果易于理解


  * 对中间值的缺失不敏感


  * 可以处理不相关特征数据  **什么叫可以处理不相关特征数据？**


缺点：


  * 可能会产生过度匹配问题。


适用数据类型：


  * 数值型和标称型。





# REF
  * 七月在线 机器学习
  *  [机器学习方法(四)：决策树Decision Tree原理与实现技巧](https://blog.csdn.net/xbinworld/article/details/44660339)
