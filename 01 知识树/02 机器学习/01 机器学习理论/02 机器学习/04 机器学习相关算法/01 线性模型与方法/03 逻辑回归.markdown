# TODO
  * **没有很清楚 公式要自己推一边**
  * **推导仍然不清楚。**
  * **从回归里把逻辑回归拿过来**




# 知识前提
- Logistic 函数，又叫 **对数几率函数 为什么叫这个，本篇有讲**







# 线性回归怎么用在分类任务中？
在线性回归中，我们讨论了如何使用线性模型进行回归学习，但如果要做的是分类任务该怎么办？答案蕴涵在广义线性模型中，我们只需找一个单调可微函数将分类任务的真实标记 y 与线性回归模型的预测值联系起来即可。


# 考虑简单的二分类任务


OK，我们考虑二分类任务， 其输出标记 $y\in \{0,1\}$ ，而线性回归模型产生的预测值 $z=w^Tx+b$ 是实值，那么我们只要将实值 z 转换为 0/1 值就可以了，而最理想的就是"单位阶跃函数" (unit-step function) ：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/37I9K1k5ea.png?imageslim)


即若预测值 z 大于零就判为正例，小于零则判为反例，预测值为临界值零则可任意判别。

如图：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/CgCam8lLEc.png?imageslim)


但是呢，在线性回归中，我们要求了，g 必须是一个连续且充分平滑的函数， 从上图可看出，单位阶跃函数不连续，因此不能直接用作 \(g^-(\cdot )\) 。 于是我们希望找到能在一定程度上近似单位阶跃函数的 "替代函数" (surrogate function) ，并希望它单调可微。

那么哪个函数可以近似单位阶跃呢？


# Logistic 函数


而Logistic 函数 (logistic function) 正是这样一个常用的替代函数。**厉害了，没想到logistic函数是这么来的。但是，我们只能够知道它的输出是0~1呀，怎么能知道它一定能够描述 f 于 y 之间的关系呢？就比如线性回归那篇的ln，我怎么就能知道ln一定可以满足呢？有点疑问。**

$$y=\frac{1}{1+e^{-z}}$$


从上面的图可以看出，Logistic 函数是一种 "Sigmoid 函数"（ Sigmoid 函数即形似 s 的函数），它将 z 值转化为一个接近 0 或 1 的 y 值，并且其输出值在 z= 0 附近变化很陡。嗯，我们将Logistic 函数作为 \(g^-(\cdot )\)  代入广义线性模型，就得到：

$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$

可以变化为：

$$ln\frac{y}{1-y}=w^Tx+b$$





# 叫做对数几率函数的由来


OK，看到这个式子，我们要注意，如果我们将 y 视为样本 作为正例的可能性，那么 1-y 就是其反例可能性，那么两者的比 $\frac{y}{1-y}$ 称为"几率" (odds) ，几率反映了作为正例的相对可能性。而我们对几率取对数则得到 "对数几率" (log odds，亦称 logit )，即：<span style="color:red;">厉害了，没想到对数几率是在这里出现的，对于Logistic回归函数进行变换就可以得到。</span>

$$ln\frac{y}{1-y}$$


由此可看出
$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$
实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。<span style="color:red;">嗯是的。**因此，其对应的模型称为 "Logistic 回归"  (logistic regression，亦称 logit regrssion) 。注意：虽然它的名字是"回归"，但实际却是一种分类学习方法。</span>

比如说：我们可以将范围在 0~1 之间的数值中大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。这样就可以用来解决分类问题。


# Logistic 回归的优点


这种方法有很多优点：
- 它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题；
- 它不是仅预测出"类别"，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；
- 此外，对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解.




# 这时候怎么确定 w 和 b 呢？

$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$



下面我们来看看如何确定中的 w 和 b 的. 若将上式中的 y 视为类后验概率估计 $p(y=1|x)$ ， 则

$ln\frac{y}{1-y}=w^Tx+b$ 可重写为： <span style="color:red;">为什么可以将y视为类后验概率？</span>

$$ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b$$


显然有：<span style="color:red;">怎么得到的？</span>


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/JEK84KCLdC.png?imageslim)





# 使用极大似然法


于是，我们可通过"极大似然法" (maximum likelihood method)来估计w 和 b 。给定数据集 $\{(x_i,y_i)\}_{i=1}^{m}$ ，"对数似然" (log likelihood) 为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/1II9diB21D.png?imageslim)


最大化"对数似然" ，即令每个样本属于其真实标记的概率越大越好.为便于讨论，令 $\beta= (w;b)$，$\hat{x}=(x;1)$， 则 $w^Tx +b$ 可简写为 $\beta^T\hat{x}$，再令 $p1(\hat{x};\beta) = p(y= 1|\hat{x};\beta)$ ，$p_0(\hat{x};\beta)=p(y=0\mid \hat{x};\beta)=1-p_1(\hat{x};\beta)$，则上式中的似然项可重写为 ：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/C1cLFhL6dl.png?imageslim)


带入对数似然函数，并根据 $p(y=1\mid x)$ 和 $p(y=0\mid x)$，得到：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/3ea87achdA.png?imageslim)




# 求解


<span style="color:red;">自己求解一下</span>

这个式子是关于 $\beta$ 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法 (gradient descent method) 、牛顿法 (Newton method) 等都可求得其最优解，于是就得到 ：<span style="color:red;">使用梯度下降和牛顿法求解的过程这里要说一下。</span>

$$\beta ^*=\underset{\beta}{arg\, min}\,\ell (\beta)$$

以牛顿法为例，其第 t+1 轮迭代解的更新公式为


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/6EmKkJkmCb.png?imageslim)


其中关于 $\beta$  的一阶、 二阶导数分别为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/b6l9gLCkHA.png?imageslim)




# Logistic 回归算法流程
* 开始时候，先将每个回归系数初始化为 1。<span style="color:red;">确定吗？</span>
* 重复 R 次：计算整个数据集的梯度，使用 步长 x 梯度 更新回归系数的向量
* 返回回归系数


# Logistic 回归 开发流程

* 收集数据: 采用任意方法收集数据
* 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。
* 分析数据: 采用任意方法对数据进行分析。
* 训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。
* 测试算法: 一旦训练步骤完成，分类将会很快。
* 使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。




# Logistic 回归 算法特点


优点:
  * 计算代价不高，
  * 易于理解和实现。

缺点:
  * 容易欠拟合，
  * 分类精度可能不高。


适用数据类型:
  * 数值型和标称型数据。
