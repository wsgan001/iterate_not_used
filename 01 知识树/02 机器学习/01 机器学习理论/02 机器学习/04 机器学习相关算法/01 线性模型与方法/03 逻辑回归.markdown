# 逻辑回归


TODO

* **没有很清楚 公式要自己推一边**
* **推导仍然不清楚。**
* **从回归里把逻辑回归拿过来**





## 知识前提

- Logistic 函数，又叫 **对数几率函数 为什么叫这个，本篇有讲**



## 逻辑回归的由来

### 起因，想把线性回归用在分类任务中

在线性回归中，我们讨论了如何使用线性模型进行回归学习，但如果要做的是分类任务该怎么办？<span style="color:red;">是呀？之前将的线性模型对应的是回归，相当于拟合一条曲线，那么分类任务要怎么对应？</span>

答案蕴涵在广义线性模型中，我们只需找一个单调可微函数将分类任务的真实标记 $y$ 与线性回归模型的预测值联系起来即可。<span style="color:red;">really ？ 怎么找到的？到底怎么把分类任务的真实标记与回归模型的预测值联系起来的？嗯，说实话，我之前一直没怎么明白逻辑回归为什么被创造出来，看到这里，我才终于明白一点点。但是为什么这种致力于解决分类问题的方案叫做逻辑回归呢？为什么不是逻辑分类？</span>

### 看一看二分类任务

我们先考虑简单的二分类任务。

OK，我们考虑二分类任务，其输出标记 $y\in \{0,1\}$ ，而线性回归模型产生的预测值 $z=w^Tx+b$ 是实值，那么我们只要将实值 $z$ 转换为 $0/1$ 值就可以了，而要完成这种转换，最理想的就是"单位阶跃函数" (unit-step function) ：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/37I9K1k5ea.png?imageslim)

即若预测值 $z$ 大于零就判为正例，小于零则判为反例，预测值为临界值零则可任意判别。

如图：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/CgCam8lLEc.png?imageslim)

但是呢，在线性回归那一章中，我们要求了广义线性模型的联系函数 $g$ 必须是一个连续且充分平滑的函数。<span style="color:red;">是的。</span> 而从上图可看出，单位阶跃函数它并不连续，因此，我们不能直接把它用作 $g^-(\cdot )$ 。于是我们希望找到能在一定程度上近似单位阶跃函数的 "替代函数" (surrogate function) ，并希望它单调可微。<span style="color:red;">这个 $g^-(\cdot )$ 的负号哪里来的？我看到这里的时候，倒吸了一口气，对这个逻辑回归的由来又了解了一步，他是作为单位阶跃函数的替代品出现的。。好吧</span>

那么哪个函数可以近似单位阶跃函数呢？嗯，答案已经呼之欲出了。

### Logistic 函数

Logistic 函数 (logistic function) 正是这样的一个常用来近似单位阶跃函数的函数。

$$y=\frac{1}{1+e^{-z}}$$

从之前的图可以看出，Logistic 函数是一种 "Sigmoid 函数"，Sigmoid 函数即形似字母 s 的函数，它将 $z$ 值转化为一个接近 $0$ 或 $1$ 的 $y$ 值，并且其输出值在 $z= 0$ 附近变化很陡。嗯，我们将 Logistic 函数作为 $g^-(\cdot )$  代入广义线性模型，就得到了：

$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$

这个式子可以变化为：

$$ln\frac{y}{1-y}=w^Tx+b$$


### 为什么逻辑回归又叫做对数几率函数

OK，看到这个式子，我们要注意，如果我们将 $y$ 视为样本作为正例的可能性，那么 $1-y$ 就是其反例可能性，那么两者的比 $\frac{y}{1-y}$ 称为"几率" (odds) ，几率反映了作为正例的相对可能性。而我们对几率取对数则得到 "对数几率" (log odds，亦称 logit )，即：$ln\frac{y}{1-y}$ 。<span style="color:red;">厉害了，没想到对数几率是在这里出现的，对于 Logistic 回归函数进行变换就可以得到。</span>

由此我们可以把这个式子 $ln\frac{y}{1-y}=w^Tx+b$ 解读成：我们是在用线性回归模型的预测结果去逼近真实标记的对数几率。<span style="color:red;">嗯是的。很精彩！</span>

因此，其对应的模型 $y=\frac{1}{1+e^{-(w^Tx+b)}}$ 称为 "Logistic 回归" (logistic regression，亦称 logit regrssion) 。

### 它的名字虽然是回归，但实际上是一种分类学习方法

**注意：虽然它的名字是"回归"，但实际却是一种分类学习方法。**<span style="color:red;">是的，这个是我以前感觉非常奇怪的地方。。原来是这样，这次才稍微清晰了些。</span>

比如说：我们可以将范围在 0~1 之间的数值中大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。这样就可以用来解决分类问题。<span style="color:red;">是的，很好。</span>


## 求解 $w$ 和 $b$

<span style="color:red;">这次看比上次又清楚了一些。</span>

### 怎么确定式子中的 $w$ 和 $b$ 呢？

$$y=\frac{1}{1+e^{-(w^Tx+b)}}$$

下面我们来看看如何确定式子中的 $w$ 和 $b$ 的。 若将上式中的 $y$ 视为类后验概率估计 $p(y=1|x)$ ，则 $ln\frac{y}{1-y}=w^Tx+b$ 可重写为： <span style="color:red;">为什么可以将 $y$ 视为类后验概率估计 $p(y=1|x)$ ，没明白？</span>

$$ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b$$


显然有：<span style="color:red;">是的，把 $p(y=1|x)$ 代入 $ln\frac{y}{1-y}=w^Tx+b$ 中的 $y$ 就可以算出。</span>


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/JEK84KCLdC.png?imageslim)

<span style="color:blue;">这个式子写出来之后后续有用到吗？有用到，代入对数似然中，把概率用 $\beta$ 的表达式替换了，这样式子中就只有 $\beta$ 这个参数了。</span>

### 使用极大似然法来估计 $w$ 和 $b$

于是，我们可通过"极大似然法" (maximum likelihood method) 来估计 $w$ 和 $b$ 。给定数据集 $\{(x_i,y_i)\}_{i=1}^{m}$ ，"对数似然" (log likelihood) 为：<span style="color:red;">为什么可以用极大似然法来估计？原因还是要补充在这里的。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/1II9diB21D.png?imageslim)


最大化"对数似然" ，即令每个样本属于其真实标记的概率越大越好。为便于讨论，令 $\beta= (w;b)$，$\hat{x}=(x;1)$， 则 $w^Tx +b$ 可简写为 $\beta^T\hat{x}$，再令 $p_1(\hat{x};\beta) = p(y= 1|\hat{x};\beta)$ ，$p_0(\hat{x};\beta)=p(y=0\mid \hat{x};\beta)=1-p_1(\hat{x};\beta)$，则上式中的似然项可重写为 ：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/C1cLFhL6dl.png?imageslim)

<span style="color:red;">是的，这里还是看得懂的，可以这么写。</span>

带入对数似然函数，并根据上面的 $p(y=1\mid x)$ 和 $p(y=0\mid x)$，得到：<span style="color:red;">这一步略了计算过程，要补上，每一步都要清楚的算过。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/3ea87achdA.png?imageslim)

现在，这个对数似然函数已经变成这样了，我们要求的就是这个对数似然函数最大的时候的 $\beta$ 值。

### 使用牛顿法求解 $\beta$

<span style="color:red;">自己求解一下</span>

这个式子是关于 $\beta$ 的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法 (gradient descent method) 、牛顿法 (Newton method) 等都可求得其最优解，于是就得到 ：<span style="color:red;">使用梯度下降和牛顿法求解的过程这里要说一下。</span>

$$\beta ^*=\underset{\beta}{arg\, min}\,\ell (\beta)$$

<span style="color:red;">不是要最大化对数似然吗？这里是不是抄错了？核对一下 pdf 。</span>

以牛顿法为例，其第 $t+1$ 轮迭代解的更新公式为


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/6EmKkJkmCb.png?imageslim)


其中关于 $\beta$  的一阶、 二阶导数分别为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/b6l9gLCkHA.png?imageslim)

<span style="color:red;">好吧，关键的步骤都写出来了，要自己把它补全。</span>






## Logistic 回归的使用方式

OK，到这里，整个 Logistic 回归算法的前因，求解的过程，我们都已经知道了。

那么在实际的应用中，我们是怎么使用的呢？怎么计算的呢？

<span style="color:red;">好像在哪里看到过说 LR 要先对数据进行 standardnize ，忘记是不是了，确认下，使用 LR 的数据前提。</span>

### 完整的 Logistic 回归模型的开发流程如下

* 收集数据: 采用任意方法收集数据
* 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。
* 分析数据: 采用任意方法对数据进行分析。
* 训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。
* 测试算法: 一旦训练步骤完成，分类将会很快。
* 使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。

### 实际应用中，怎么使用开源模块实现 Logistic 回归？

实际中，一般是适应的 sklearn ，它内部已经完全把计算的过程封装进去了，外在的接口只有几个超参数。<span style="color:red;">把 sklearn 关于 Logistic 回归的暴露在外面的超参数总结一下，可以拆分成一个文章并带一个例子</span>

<span style="color:red;">把别的开源模块中关于逻辑回归的使用也结合例子总结下。</span>

### 如果不是用开源模块，怎么自己实现对于 Logistic 回归的求解？

* 开始时候，先将每个回归系数初始化为 1。<span style="color:red;">确定吗？</span>
* 重复 R 次：计算整个数据集的梯度，使用 步长 x 梯度 更新回归系数的向量
* 返回回归系数

<span style="color:red;">还是要补充下，并且自己实现下。看看整个完整的实现过程。或者把开源库的实现代码总结到这里。</span>



## Logistic 回归的优缺点

基本上，对于 Logistic 回归已经介绍完了，现在，我们来看看这个方法的一些优缺点：

这种方法有很多的优点：

- Logistic 回归的计算代码还是不是很高的，不需要太多的计算资源。
- Logistic 还是比较容易理解的，这样使用的时候得到的结果就有很好的可解释性。
- 它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题。<span style="color:red;">什么叫假设数据分布？别的机器学习方法有假设数据分布吗？</span>
- 它不是仅预测出"类别"，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用。<span style="color:red;">是的，不过能结合例子谈一下吗？</span>
- 此外，对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。<span style="color:red;">有哪些数值优化算法？怎么直接求取最优解的？</span>


当然，它也是有一些缺点的：

* Logistic 回归方法容易欠拟合。<span style="color:red;">好吧，能说一下为什么呢？还是说因为它是从线性回归过来的，所以有这种欠拟合的缺点？</span>
* 分类精度可能不高。<span style="color:red;">什么叫分类精度不高？还是说因为容易欠拟合所以才分类精度不高？确认下。</span>


## REF

- 《机器学习》 周志华
