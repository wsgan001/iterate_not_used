# 间隔与支持向量

TODO

-


## 出发点，想用一个超平面来划分样本集

给定的训练样本集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\},y_i\in \{-1,+1\}$ ，分类学习最基本的想法就是基于训练集 $D$ 在样本空间中找到一个划分超平面，将不同类别的样本分开。

但是，实际上能够将训练样本分开的划分超平面可能有很多，如下图所示，我们选择哪一个最好呢？

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/JaDCGAjhfh.png?imageslim)


直观上看，我们应该去找位于两类训练样本的所谓 “正中间” 的划分超平面，因为这样的超平面看起来对训练样本局部扰动的 “容忍” 性最好。也就是说，这样的分类结果应该是最鲁棒的，泛化能力最强的。

OK，到这里，我们就想问了，什么是所谓的 “正中间” ？怎么衡量？怎么求出这个 “正中间”？


## 划分超平面的表示

我们先看下对于这个划分超平面的描述。

实际上，在样本空间中，划分超平面可通过如下线性方程来描述：

$$w^Tx+b=0$$

其中:

- $\mathbb{w}=(w_1;w_2;\cdots w_d)$ 为法向量，决定了超平面的方向。
- $b$ 为位移项，决定了超平面与原点之间的距离。

显然，一个划分超平面是可以被法向量 $w$ 和位移 $b$ 确定的，<span style="color:red;">是这么回事。</span>我们可以记为 $(w,b)$ 。

我们可以把样本空间中任意点 $x$ 到超平面 $(w,b)$ 的距离表示为：

$$r=\frac{|w^Tx+b|}{||w||}$$

<span style="color:red;">上面这个式子是不是漏了什么步骤，要添加一下。</span>

## 支持向量与间隔

假设超平面 $(w,b)$ 能将训练样本正确分类，即对于 $(x_i,y_i)\in D$ ，若 $y_i=+1$，则有 $w^Tx_i+b>0$ ；若 $y_i=-1$ ，则有 $w^Tx_i+b<0$ 。令：<span style="color:red;">这一段中的 若 则有 不是很明白？</span>

$$\left\{\begin{matrix} w^Tx_i+b\geqslant +1&y_i=+1 \\ w^Tx_i+b\leqslant -1&y_i=-1 \end{matrix}\right.$$

<span style="color:red;">上面这个式子是不是抄错了？$w^Tx_i+b$ 到底是与 0 比还是与 1 比较？</span>

可见，对于上面这个式子来说，距离超平面最近的这几个训练样本点使得等号成立。

由于每个样本点对应一个特征向量，因此，这几个使得取等号的样本点就称为支持向量 (support vector)。从下图可以看出就是这三个被画了圆圈的点。 <span style="color:red;">嗯，这就是支持向量。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/h9jKJD4J15.png?imageslim)


然后，我们就把两个异类支持向量到超平面的距离的和 $\gamma=\frac{2}{||w||}$ 定义为间隔(margin)。


## 最中间的超平面就是具有最大间隔的超平面


OK，这时候，我们想找到一个最中间的超平面，也就是想要找到具有最大间隔 (maximum margin) 的划分超平面，也就是想找到这样：

$$\begin{align*} &\underset{w,b}{max}\; \frac{2}{||w||}\\ & s.t.\; y_i(w^Tx_i+b)\geqslant 1,i=1,2,\cdots ,m \end{align*}$$

<span style="color:red;">这个 $\frac{2}{||w||}$ 是哪里来的？</span>

即在满足 $y_i(w^Tx_i+b)\geqslant 1$ 的条件下，使得 $\gamma$ 最大的 $w$ 和 $b$ 。<span style="color:red;">拿来的 $\gamma$ ？是不是漏掉了什么？</span>

也就是说：

$$\begin{align*} &\underset{w,b}{min}\; \frac{1}{2}||w||^2\\ & s.t.\; y_i(w^Tx_i+b)\geqslant 1,i=1,2,\cdots ,m \end{align*}$$

这个，就是支持向量机 (Support Vector Machine，简称 SVM ) 的基本型。<span style="color:red;">嗯，从前面走下来，看起来还是很有道理的。</span>

注：上面这个式子看起来这个最小仅与 $w$ 有关, 但是事实上 $b$ 通过约束隐式地影响着 $w$ 的取值，进而对间隔产生影响。<span style="color:red;">嗯，是的</span>


<span style="color:red;">感觉在这里中断的有点突兀，看看能不能把后面一节接到这里来。</span>


## REF
1. 《机器学习》周志华
