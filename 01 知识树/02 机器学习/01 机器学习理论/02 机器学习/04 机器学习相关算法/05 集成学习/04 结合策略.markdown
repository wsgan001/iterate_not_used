


结合策略

学习器结合可能会从三个方面带来好处:

- 首先，从统计 的方面来看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险；
- 第二，从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后 进行结合，可降低陷入糟糕局部极小点的风险；
- 第三，从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大， 有可能学得更好的近似

图8.8给出了一个直观示意图.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/6EeCLb3B2F.png?imageslim)

假定集成包含 T 个基学习器 $\{h_1,h_2,\cdots ,h_T\}$ 。其中 $h_i$ 在示例 x 上的输出为 $h_i(x)$ .本节介绍几种对$h_i$进行结合的常见策略.


## 1平均法

对数值型输出 $h_i(x)\in \mathbb{R}$ ,最常见的结合策略是使用平均法(averaging).

- 简单平均法(simple averaging)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/jjjkFcjILb.png?imageslim)

- 加权平均法(weighted averaging)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/J9flkfmiA1.png?imageslim)

其中 $w_i$ 是个体学习器 $h_i$ 的权重，通常要求 $w_i\geq 0$ , $\sum_{i=1}^{T}w_i=1$ 。


显然，简单平均法是加权平均法令 $w_i= 1/T$ 的特例。加权平均法在二十世 紀五十年代已被广泛使用[Markowitz, 1952], [Perrone and Cooper, 1993]正式 将其用于集成学习.它在集成学习中具有特别的意义，集成学习中的各种结合 方法都可视为其特例或变体.事实上加权平均法可认为是集成学习研究的基本出发点，对给定的基学习器，不同的集成学习方法可视为通过不同的方式来确定加权平均法中的基学习器权重.


加权平均法的权重一般是从训练数据中学习而得，现实任务中的训练样本通常不充分或存在噪声，这将使得学出的权重不完全可靠.尤其是对规模比较大的集成来说，要学习的权重比较多，较容易导致过拟合.因此，实验和应用均显示出，加权平均法未必一定优于简单平均法。一般而言，在个体学习器性能相差较大时宜使用加权平均法,而在个体学习器性能相近时宜使用简单平均法.

## 2投票法

对分类任务来说，学习器 $h_i$ 将从类别标记集合 $\{c_1,c_2,\cdots ,c_N\}$ 中预测出一个标记，最常见的结合策略是使用投票法(voting).为便于讨论，我们将 $h_i$ 在样本 x 上的预测输出表示为一个 N 维向量 $(h_i^1(x);h_i^2(x);\cdots ;h_i^N(x);)$，其中 $h_i^j(x)$ 是 $h_i$ 在类别标记 $c_j$ 上的输出.

- 绝对多数投票法(majority voting)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/3K44C3Hg12.png?imageslim)
即若某标记得票过半数，则预测为该标记;否则拒绝预测.

- 相对多数投票法(plurality voting)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/F9ifeELmDg.png?imageslim)
即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个.

- 加权投票法(weighted voting)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/gl1DGjEa4k.png?imageslim)
与加权平均法类似,$w_i$ 是 $h_i$ 的权重,通常 $w_i\geq 0$ , $\sum_{i=1}^{T}w_i=1$ 。


标准的绝对多数投票法(8.24)提供了 “拒绝预测”选项，这在可靠性要求 较高的学习任务中是一个很好的机制。但若学习任务要求必须提供预测结果， 则绝对多数投票法将退化为相对多数投票法.因此，在不允许拒绝预测的任务 中，绝对多数、相对多数投票法统称为“多数投票法”。

式(8.24)~(8.26)没有限制个体学习器输出值的类型.在现实任务中，不同 类型个体学习器可能产生不同类型的 $h_i^j(x)$ 值，常见的有：

- 类标记: $h_i^j(x)\in \{0,1\}$ ,若 $h_i$ 将样本 x 预测为类别 $c_j$ 则取值为1，否则为 0.使用类标记的投票亦称“硬投票” (hard voting).
- 类概率：$h_i^j(x)\in [0,1]$，相当于对后验概率 $P(c_j|x)$ 的一个估计.使用类概率的投票亦称 “软投票” (soft voting).

不同类型的 $h_i^j(x)$ 值不能混用.对一些能在预测出类别标记的同时产生分类置信度的学习器，其分类置信度可转化为类概率使用.若此类值未进 行规范化，例如支持向量机的分类间隔值，则必须使用一些技术如 Platt 缩 放(Platt scaling) 、等分回归(isotonic regression) 等进行“校准”(calibration)后才能作为类概率使用。

有趣的是，虽然分类器估计出的类概率值一般都不太准确，但基于类概率进行结合却往往比直接基于类标记进行结合性能更好.需注意的是，若基学习器的类型不同，则其类概率值不能直接进行比较；在此种情形下，通常可将类概率输出转化为类 标记输出(例如将类概率输出最大的 $h_i^j(x)$ 设为1，其他设为0)然后再投票.

## 3学习法

Stacking 本身是一种著 名的集成学习方法，且有 不少集成学习算法可视为 其变体或特例.它也可看 作一种特殊的结合策略， 因此本书在此介绍.


当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过 另一个学习器来进行结合。stacking是学习法 的典型代表.这里我们把个体学习器称为初级学习器，用于结合的学习器称为 次级学习器或元学习器(meta-learner).


Stacking先从初始数据集训练出初级学习器，然后“生成” 一个新数据集 用于训练次级学习器.在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。

Stacking的算法描述如图8.9所 示，这里我们假定初级学习器使用不同学习算法产生，即初级集成是异质的.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/JkghECbEbb.png?imageslim)


在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器 的训练集来产生次级训练集，则过拟合风险会比较大;因此,一般是通过使用交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来生次级学习器的训练样本.以 $k$ 折交叉验证为例，初始训练集 D 被隨机划分为 $k$ 个大小相 似的集合 $D_1,D_2,\cdots ,D_k$ 。令 $D_j$ 和 $\overline{D}_j=D\D_j$ 分别表示第 j 折的测试集和训练集.给定 T 个初级学习算法，初级学习器 $h_t^{(j)}$ 通过在 $\overline{D}_j$ 上使用第 t 个学习算法而得.对于 $D_j$ 中每个样本 $x_i$ ，令 $z_{it}=h_t^{(j)}(x_i)$ ，则由 $x_i$ 所产生的次级训练样例的示例部分为 $z_i=(z_{i1};z_{i2};\cdots ;z_{iT}) ，标记部分为 $y_i$ 。于是，在整个交叉验证过程结束后，从这 T 个初级学习器产生的次级训练集是 $D'=\{(z_i,y_i)\}_{i=1}^m$ ,然后 $D'$ 将用于训练次级学习器.


次级学习器的输入属性表示和次级学习算法对 Stacking 集成的泛化性能 有很大影响.有研究表明，将初级学习器的输出类概率作为次级学习器的输入 属性，用多响应线性回归(Multi-response Linear Regression,简称MLR)作为 次级学习算法效果较好［Ting and Witten, 1999］，在MLR中使用不同的属性集更佳［Seewald，2002］.

贝叶斯模型平均(Bayes Model Averaging,简称BMA)基于后验概率来为 不同模型赋予权重，可视为加权平均法的一种特殊实现。[Clarke，2003]对 Stacking和BMA进行了比较.理论上来说，若数据生成模型恰在当前考虑的 模型中，且数据噪声很少，则BMA不差于Stacking;然而，在现实应用中无法确保数据生成模型一定在当前考虑的模型中，甚至可能难以用当前考虑的模型来进行近似，因此，Stacking通常优于BMA，因为其鲁棒性比BMA更好，而且 BMA 对模型近似误差非常敏感。







# REF
1. 《机器学习》周志华
