

EM算法


在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到, 即训练样本是“完整”的.但在现实应用中往往会遇到“不完整”的训练样本，例如由于西瓜的根蒂已脱落，无法看出是“蜷缩”还是“硬挺”，则训练样本的 “根蒂” 属性变量值未知.在这种存在 “未观测” 变量的情形下，是否仍能对模型参数进行估计呢？

未观测变量的学名是 “隐变量”(latent variable).令 $X$ 表示已观测变量集，$Z$ 表示隐变量集，$\Theta$ 表示模型参数.若欲对 $\Theta$ 做极大似然估计，则应最大化 对数似然

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/Hfch1EI6JD.png?imageslim)

然而由于 Z 是隐变量，上式无法直接求解.此时我们可通过对 Z 计算期望，来最大化已观测数据的对数“边际似然”(marginal likelihood)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/F6i8iEI7B7.png?imageslim)

EM (Expectation-Maximization)算法是常用的估计参数隐变量的利器，它是一种迭代式的方法，其基本想法是：若参数 $\Theta$ 已知, 则可根据训练数据推断出最优隐变量 Z 的值(E步);反之，若 Z 的值已知，则可方便地对参数 $\Theta$ 做极大似然估计(M步).

于是，以初始值为起点，对式(7.35)，可迭代执行以下步骤直至收敛：

- 基于 $\Theta^t$ 推断隐变量 Z 的期望，记为 $Z^t$ ；

- 基于已观测变量 X 和 $Z^t$ 对参数 $\Theta$ 做极大似然估计，记为 $\Theta^{t+1}$

这就是 EM 算法的原型.

进一步，若我们不是取 Z 的期望，而是基于 $\Theta^t$ 计算隐变量 Z 的概率分布 $P(Z|X,\Theta^t)$ ，则EM算法的两个步骤是：

- E步(Expectation):以当前参数 $\Theta^t$ 推断隐变量分布 $P(Z|X,\Theta^t)$ ,并计 算对数似然 $LL(\Theta |X,Z)$ 关于Z的期望:
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/BJfbl075LJ.png?imageslim)

- M步(Maximization):寻找参数最大化期望似然，即
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180628/ba2m2c5L4J.png?imageslim)


简要来说，EM算法使用两个步骤交替计算：第一步是期望(E)步，利用当 前估计的参数值来计算对数似然的期望值;第二步是最大化(M)步，寻找能使 E步产生的似然期望最大化的参数值.然后，新得到的参数值重新被用于E步，……直至收敛到局部最优解.

事实上，隐变量估计问题也可通过梯度下降等优化算法求解，但由于求和的项数将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而EM算 法则可看作一种非梯度优化方法.









# COMMENT

下面是补充阅读

贝叶斯决策论在机器学习、模式识别等诸多关注数据分析的领域都极 为重要的地位.对贝叶斯定理进行近似求解，为机器学习算法的设计提供了 一种有效途径.为避兔贝叶斯定理求解时面临的组合爆炸、样本稀疏问题，朴 素贝叶斯分类器引入了属性条件独立性假设.这个假设在现实应用中往往很 难成立，但有趣的是，朴素贝叶斯分类器在很多情形下都能获得相当好的性能 [Domingos and Pazzani, 1997; Ng and Jordan, 2002]. 一种解释是对分类任务 来说，只需各类别的条件概率排序正确、无须精准概率值即可导致正确分类结 果[Domingos and Pazzani, 1997];另一种解释是,若属性间依赖对所有类别影 响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开 销的同时不会对性能产生负面影响[Zhang, 2004].朴素贝叶斯分类器在信息检 索领域尤为常用[Lewis, 1998], [McCallum and Nigam, 1998]对其在文本分类 中的两种常见用法进行了比较.

根据对属性间依赖的涉及程度，贝叶斯分类器形成了一个“谱”：朴素贝 叶斯分类器不考虑属性间依赖性，贝叶斯网能表示任意属性间的依赖性，二者 分别位于“谱”的两端；介于两者之间的则是一系列半朴素贝叶斯分类器，它 们基于各种假设和约束来对属性间的部分依赖性进行建模.一般认为，半朴素 贝叶斯分类器的研究始于[Kononenko, 1991]. ODE仅考虑依赖一个父属性， 由此形成了独依赖分类器如 TAN [Friedman et al., 1997]> AODE [Webb et al.5 2005]、LBR (lazy Bayesian Rule) [Zheng and Webb, 2000]等；feDE 则考虑最 多依赖个父属性，由此形成了 依赖分类器如KDB [Sahami, 1996]> NBtree [Kohavi, 1996]等.

贝叶斯分类器(Bayes Classifier)与一般意义上的“贝叶斯学习” (Bayesian Learning)有显著区别，前者是通过最大后验概率进行单点估计，后者则是进行 分布估计.关于贝叶斯学习的内容可参阅[Bishop, 2006].

J. Pearl教授因这方面的 卓越贡献而获得2011年 图灵奖,参见第14章.

贝叶斯网是经典的概率 图模型，参见第14章.

贝叶斯网为不确定学习和推断提供了基本框架，因其强大的表示能力、 良好的可解释性而广受关注[Pearl，1988].贝叶斯网学习可分为结构学习和 参数学习两部分.参数学习通常较为简单，而结构学习则被证明是NP难问 题[Cooper, 1990; Chickering et al., 2004],人们为此提出了多种评分搜索方法 [Friedman and Goldszmidt, 1996].贝叶斯网通常被看作生成式模型，但近年来 也有不少关于贝叶斯网判别式学习的研究[Grossman and Domingos, 2004].关 于贝叶斯网的更多介绍可参阅[Jensen, 1997; Heckerman, 1998].

EM算法是最常见的隐变量估计方法，在机器学习中有极为广泛的用途，例如常被用来学习高斯混合模型(Gaussian mixture model,简称GMM)的参数; 9.4节将介绍的均值聚类算法就是一个典型的EM算法.更多关于EM算法 的分析、拓展和应用可参阅[McLachlan and Krishnan, 2008].
“数据挖掘十大算法” 还包括前几章介绍的 C4.5、CART决策树、支 持向量机，以及后几章将 要介绍的AdaBoost、k均 值聚类、fc近邻算法等.

如常被用来学习高斯混合模型(Gaussian mixture model,简称GMM)的参数; 9.4节将介绍的均值聚类算法就是一个典型的EM算法.更多关于EM算法 的分析、拓展和应用可参阅[McLachlan and Krishnan, 2008].
本章介绍的朴素贝叶斯算法和EM算法均曾入选“数据挖掘十大算法” [Wu et al.3 2007].

# REF

1. 《机器学习》周志华
