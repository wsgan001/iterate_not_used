

过滤式选择


过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关.这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型.

Relief (Relevant Features) 是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性.该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定.于是，最终只需指定一个阈值 $\tau$ ，然后选择比 $\tau$  大的相关统计量分量所对应的特征即可;也可指定欲选取的特征个数 k ，然后选择相关统计量分量最大的 k 个特征.

显然，Relief的关键是如何确定相关统计量.给定训练集 $\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m),\}$ 对每个示例 $x_i$ ，Relief先在 $x_i$ 的同类样本中寻找 其最近邻 $x_{i,nh}$，称为“猜中近邻” (near-hit)，再从 $x_i$ 的异类样本中寻找其最近邻 $x_{i,nm}$ ，称为“猜错近邻” (near-miss),然后，相关统计量对应于属性 j 的分量为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/DlAiaf1iJ2.png?imageslim)


其中 $x_a^j$ 表示样本 $x_a$ 在属性 j 上的取值，$diff(x_a^j,x_b^j)$  取决于属性 j 的类型：若属性 j 为离散型，则 $x_a^j=x_b^j$ 时 $diff(x_a^j,x_b^j)=0$ ,否则为 1;若属性 j 为连续型， 则 $diff(x_a^j,x_b^j)=|x_a^j-x_b^j|$ ,注意 $x_a^j$,$x_b^j$ 已规范化到[0,1]区间.

从式(11.3)可看出，若 $x_i$ 与其猜中近邻 $x_{i,nh}$ 在属性 j 上的距离小于  $x_i$ 与其猜错近邻 $x_{i,nm}$ 的距离，则说明属性 j 对区分同类与异类样本是有益的，于是增大属性 j 所对应的统计量分量;

反之，若 $x_i$ 与其猜中近邻 $x_{i,nh}$ 在属性 j 上的距离大于 $x_i$ 与其猜错近邻 $x_{i,nm}$ 的距离，则说明属性 j 起负面作用，于是减小属性j所对应的统计量分量。

最后，对基于不同样本得到的估计结果进行平均， 就得到各属性的相关统计量分量,分量值越大，则对应属性的分类能力就越强.

式(11.3)中的 i 出了指出了用于平均的样本下标.实际上 Relief 只需在数据集的 采样上而不必在整个数据集上估计相关统计量.显然，Relief的时间开销随采样次数以及原始特征数线性増长，因此是一个运行效率很高的过滤式特征选择算法.

Relief 是为二分类问题设计的，其扩展变体 Relief-F 能处理多分类问题.假定数据集 D 中的样本来自 $|\mathcal{Y}|$ 个类别.对示例 $x_i$ ，若它属于第 k 类 $(k\in \{1,2,\cdots ,|\mathcal{Y}|\}$ ,则 Relief-F 先在第 k 类的样本中寻找 $x_i$ 的最近邻示例 $x_{i,nh}$ 并将其作为猜中近邻，然后在第 k 类之外的每个类中找到一个 $x_i$ 的最近邻示例作为猜错近邻，记为 $x_{i,l,nm}(l=1,2,\ldots,|\mathcal{Y}|;l\neq k)$ 。于是，相关统计量对应于属性 j 的分量为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/1lLgG3g9Kd.png?imageslim)

其中 $p_l$ 为第 l 类样本在数据集 D 中所占的比例.




# REF
1. 《机器学习》周志华
