# 过拟合与欠拟合


## 过拟合与欠拟合

我们实际希望的，是在新样本上能表现得很好的学习器。

那么为了达到这个目的，我们想要从训练样本中尽可能学出适用于所有潜在样本的 “普遍规律”，这样才能在遇到新样本时做出正确的判别。<span style="color:red;">是的。</span>

然而，当学习器把训练样本学得 “太好” 了的时候，很可能会把训练样本自身的一些特点作为所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。<span style="color:red;">嗯。</span>

这种现象在机器学习中称为 “过拟合”(overfitting)。与 “过拟合” 相对的是 “欠拟合”(underfitting)，这是指对训练样本的一般性质尚未学好。

下图是一个过拟合和欠拟合的一个直观的类比：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fe3jiB3fFi.png?imageslim)

## 过拟合出现的原因

实际上，有多种因素可能导致过拟合，其中最常见的情况就是：由于学习能力过于强大, 以至于把训练样本所包含的不太一般的特性都学到了。（学习能力是否“过于强大”，是由学习算法和数据内涵共同决定的。） <span style="color:red;">这个只是一个原因吧。 还是要好好整理下的，关于过拟合还是有很多可以整理的，当然这个地方只是说了个大概，真正的对应于某个算法的过拟合还是要放到某个算法里面介绍。</span>

## 过拟合与欠拟合的解决

对于欠拟合问题来说，由于欠拟合通常是由于学习能力低下而造成的，因此比较容易克服。例如在决策树学习中扩展分支、在神经网络学习中增加训练轮数等。

但是对于过拟合问题就很麻烦了，实际上，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施。而且，我们要知道，过拟合是无法彻底避免的，一些针对过拟合的措施只是 “缓解”，或者说减小过拟合的风险。<span style="color:red;">为什么不能彻底避免呢？下面的原因还是有些没明白？</span>

关于过拟合为什么不能彻底避免，原因如下：机器学习面临的问题通常是 NP 难甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性地证明了 “P=NP” ，因此，只要相信 “P ≠ NP” ，过拟合就不可避免。<span style="color:red;">什么是 NP 难问题？现在对于这种问题的研究有新的进展吗？要总结下。</span>



# REF

- 《机器学习》西瓜书
