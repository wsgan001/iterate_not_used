# 训练集、测试集、验证集


TODO

1. <span style="color:red;">这几种采样的方法都要自己用 Python 和C++进行实现，同时在 sklearn、tensorflow上进行实现，要找个简单的例子。要补充到这里或者新开一个总结的地方</span>
2. <span style="color:red;">这个关于调参的还是要单独拿出来吧？</span>
3. <span style="color:red;">关于验证集 还是要仔细看看，这个只是在最后一段提了一下，实际上还是有很多问题的。验证集一般什么时候使用？一般什么情况下使用？</span>


OK，我们从降低泛化误差出发，看看有什么可以做的


## 测试集的划分

### 测试集的划分是有必要的

我们知道，泛化误差是无法获得的，但是，我们可以用一种方法来计算出一个近似的泛化误差：我们可以建立一个 “测试集” (testing set) ，然后用这个测试集来测试模型对新样本的判别能力，然后，我们就可以把测试集上得出的 “测试误差” (testing error) 作为我们这个模型的泛化误差的近似。**嗯，是可以的。**

注意：测试集中的测试样本，我们假设它是从样本的真实分布中独立同分布采样得到的。而且，我们在建立测试集的时候要尽可能与训练集互斥，也就是说测试样本尽量不被用于训练。<span style="color:red;">怎么能保证他是独立同分布采样得到的？</span>

嗯，关于测试样本为什么要尽量不出现在训练集中这一点，我们这样考虑，比如说：老师出了 10 道习题供同学们练习，考试时老师又用同样的这 10 道题作为试题，那么这个考试成绩肯定不能够反应出学生的学习情况的。而我们希望得到泛化性能强的模型，就好比是希望学生对课程学得尽可能的好，这个训练的样本就是相当于给学生练习的习题，测试的样本就相当于考试的题目，很明显，如果测试样本被用作训练了，那么则得到的将是过于 “乐观” 的估计结果。<span style="color:red;">是的。</span>

OK，但是，我们现在只有一个包含 m 个样例的数据集 $D=\{(x_1,y_1),(x_2,y_2),\cdots (x_m,y_m)\}$ ，现在既要训练，又要测试，怎么做呢？

OK，我们可以对 $D$ 进行适当的处理，从中产生出训练集 $S$ 和测试集 $T$ 。

### 几种常见的训练集/测试集划分方法

下面我们介绍几种常见的做法：

#### 留出法

**从这一段，感觉是不是作者梳理的不是很透彻？因为分层采样实际上还是有几种同等的东西的，没讲，而且分层采样讲的并不丝丝入扣，与留出法本身割离了。而且，这一段的举例子的层级也不对吧，怎么一层套一层，里面还讲概念？**

**重新整理下，而且，确认下留出法这个名字是不是通用的？**

最普通的方法就是“留出法” (hold-mit) ：

直接将数据集 $D$ 划分为两个互斥的集合，其中一个集合作为训练集 $S$，另一个作为测试集 $T$ ，即 $D=S\cup T$ ，$S\cap T=\Phi$ 。在 $S$ 上训练出模型后，用 $T$ 来评估其测试误差，作为对泛化误差的估计。

OK，这种留出法实际上是最常用的，但是，这中间有个问题，我们想让测试集是从样本的真实分布中独立同分布采样得到的。现在这个数据集放在这里，怎么才能保证我的划分过程是真的 OK 呢？

下面，我们就介绍几种常用的划分方式：

##### 分层采样

分层采样 (stratified sampling)。

我们在划分数据集的时候，是需要尽可能的保持数据分布的一致性的，不然的话会引入额外的偏差，这个偏差会对最终结果产生影响。<span style="color:red;">什么额外的偏差？若 $S$、$T$中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。什么偏差？需要确认下。</span>

OK，那么怎么在划分的时候保持数据分布的一致性呢？

首先，什么是保持数据分布的一致性？其实就是在分类任务中至少要保持样本的类别比例相似。<span style="color:red;">这里只提了分类任务，如果是回归任务，怎么分层采样？如果是聚类任务呢？</span>

OK，这就是我们使用分层采样的原因，因为它正是一种保留类别比例的采样方式。

比如说：我们想对 $D$ 进行 7/3 分 ，若 $D$ 包含 500 个正例、500 个反例，那么分层采样得到的 $S$ 应包含 350 个正例、350 个反例，而 $T$ 则包含 150 个正例和 150 个反例。

嗯，看起来是 OK 了。

###### 多次随机划分取平均

但是，实际上，即便在给定 7/3 分的比例之后，我们仍然是有很多种划分方式来对 $D$ 进行分割的。

比如上面的例子中，可以把 $D$ 中的样本排序，然后把前 350 个正例放到 $S$ 中，也可以把后 350 个正例放到 $S$ 中，等等，这些不同的划分还是会将导致不同的训练集/测试集，相应的，模型评估的结果也还是会有差别。<span style="color:red;">是的。</span>

可见，单次使用留出法得到的估计结果往往还是不够稳定可靠的。<span style="color:red;">是的。</span>

因此，我们在使用留出法时，一般要采用若干次随机划分、然后重复进行实验评估后取平均值作为留出法的评估结果。<span style="color:red;">嗯，这就是留出法一定要多次验证取平均值的原因。</span>

例如我们可以进行 100 次随机划分，每次产生一个训练集/测试集用于实验评估，那么 100 次后就得到 100 个结果，而留出法返回的则是这 100 个结果的平均。（同时可得估计结果的标准差，<span style="color:red;">这个标准差有用吗？能衡量什么吗？</span>）

OK，到这里，应该没有什么问题了吧？实际上还是有一个问题的：

### 划分数据集导致的一个问题

在开始的时候，我们希望评估的是用 $D$ 训练出的模型的性能，但是现在留出法需要划分训练集/测试集，这就会导致一个问题：<span style="color:red;">我们有说要评估的是 $D$ 训练处的模型的性能吗？</span>

* 如果我令 $S$ 包含绝大多数样本，那么训练出的模型可以更接近用全部的 $D$ 训练出的模型，但是，这时候，由于 $T$ 比较小，因此评估结果可能不够稳定准确。

* 而如果我们令 $T$ 多包含一些样本，那么 $S$ 与 $D$ 的差别就更大了，也就是说，根据 $S$ 生成的模型与用 $D$ 训练出的模型是有可能有较大差别的，这就而降低了评估结果的保真性 (fidelity)。<span style="color:red;">是的，但是什么是保真性？</span>

事实上这个问题现在还没有完美的解决方案，常见做法是将大约  `2/3 ~ 4/5` 的样本用于训练，剩余样本用于测试。一般而言，测试集至少应含 `30` 个样例 。

OK，我们对留出法有了基本的认识了，感觉上，的确应该这么做。

下面我们看看别的一些方法：




# 交叉验证法

## 交叉验证的概念


交叉验证法 (cross validation) ：

先将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集，即 $D=D_1\cup D_2\cup \cdots \cup D_k$，$D_i\cap D_j=\Phi\; (i\neq j)$。每个子集 $D_i$ 都尽可能保持数据分布的一致性，即从 $D$ 中通过分层采样得到这些子集。<span style="color:red;">这个有这个分层采样的要求吗？之前我看的代码里面好像没有特别注意这个。确认下</span>

然后，每次用 $k-1$ 个子集的并集作为训练集，余下的那个子集作为测试集。这样就可获得 $k$  组训练/测试集，从而可进行 $k$ 次训练和测试，最终返回的是这 $k$ 个测试结果 的均值。

显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 $k$ 的取值，为强调这一点，通常把交叉验证法称为 $k$ 折交叉验证 (k fold cross validation)， $k$ 最常用的取值是 $10$ ，此时称为 $10$ 折交叉验证。其他常用的 $k$ 值有 $5$、$20$ 等。<span style="color:red;">为什么稳定性和保真性很大程度取决于 $k$ ？如果 $k$ 太大或太小会怎么样？太大应该没问题，因为最多就是变成留一法。</span>

下图是 $10$ 折交叉验证的示意图：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/hEB55JD99C.png?imageslim)




## 多次 $k$ 折交叉验证取平均


与留出法相似，将数据集 $D$ 划分为 $k$ 个子集同样存在多种划分方式。为了减小因样本划分不同而引入的差别，$k$ 折交叉验证通常要随机使用不同的划分重复 $p$ 次，最终的评估结果是这 $p$ 次 k 折交叉验证结果的均值，例如常见的有：10 次 10 折交叉验证。<span style="color:red;">是这样吗？之前没有注意过。以前的sklearn 的 kaggle 的代码里面有写要多次进行 $k$ 折交叉验证码？确认下。</span>


## 特殊的 $k=1$ 折交叉验证：留一法

假定数据集 $D$ 中包含 $m$ 个样本，如果令 $k = m$ ，我们就得到了交叉验证法的一个特例：留一法(Leave-One-Out，简称LOO)。

显然，留一法是不受随机样本划分方式的影响的，因为 $m$ 个样本只有唯一的方式划分为 $m$ 个子集，每个子集包含一个样本。<span style="color:red;">是的。</span>

留一法使用的 $S$ 与 $D$ 相比只少了一个样本，这就使得在绝大多数情况下，留一法的 $S$ 得到的模型与期望评估的用 $D$ 训练出的模型很相似。<span style="color:red;">嗯，是的，是个好方法。</span>

因此，留一法的评估结果往往被认为比较准确。

然而，留一法也有其缺陷：

在数据集比较大时，训练 $m$ 个模型的计算开销可能是难以忍受的 (例如数据集包含 1 百万个样本，则需训练 1 百万个模型)，而且这还是在未考虑算法调参的情况下的。<span style="color:red;">是的，如果考虑 grid_search 那会更多。</span>另外，留一法的估计结果也未必永远比其他评估方法准确。没有免费的午餐定理对实验评估方法同样适用。<span style="color:red;">为什么？ 那么到底用不用留一法呢？什么情况下使用呢？还是说更倾向于使用 10 折交叉法？ 为什么说未必永远比其他评估方法更准确？</span>




# 有放回采样法，又称 自助法 bootstrapping

## 有放回采样法的概念


（Bootstrap 本意是解鞋带，**是因为像自助餐一样吗？**才命名为 “自助法” 、“可重复采样”、“有放回采样” 的吗？）

我们希望评估的是用 D 训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比 D 小，这必然会引入一些因训练样本规模不同而导致的估计偏差。**是的。**留一法虽然受训练样本规模变化的影响较小，但是计算复杂度又太高了。（关于样本复杂度与泛化性能之间的关系，参见第 12章，**样本复杂度是什么？**）

那么，有没有什么办法既可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？ 

**厉害了。**

自助法 (bootstrapping) 就是一个比较好的解决方案，它直接以自助采样法 (bootstrap sampling) 为基础：




  * 给定包含 m 个样本的数据集 D ，我们对它进行采样产生数据集 D' ，每次随机从 D 中挑选一个 样本，将其拷贝后放入 D' ,然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到。

  * 这个过程重复执行 m 次后，我们就得到了包含 m 个样本的数据集 D‘，这就是自助采样的结果。**是的。这个还是用过的。**


显然， D 中有一部分样本会在 D’ 中多次出现，而另一部分样本不出现。


## OK，看一下这时候的训练集和测试集


可以做一个简单的估计，某个样本在 m 次采样中始终不被采到的概率是\(1-\frac{1}{m})^m\) ，取极限得到：

\[\underset{m\mapsto \infty}{lim}(1-\frac{1}{m})^m\mapsto \frac{1}{e}\approx 0.368\]

即通过自助采样，初始数据集 D 中约有 36.8% 的样本未出现在采样数据集 D' 中。OK，那么我们就可将 D' 用作训练集， D\D' 用作测试集。

这样，实际评估的模型与期望评估的模型都使用 m 个训练样本，而我们仍有数据总量约 1/3 的、没在训练集中出现的样本可以用于测试。

这样的测试结果，亦称 包外估计  (out-of-bag estimate)。**厉害呀，不过包外估计还是第一次看到。**


## 有放回采样法的使用场景，顺便说了下留出法和交叉验证


自助法在数据集较小、难以有效划分训练集 / 测试集的时候还是很有用的。而且，自助法由于能够从初始数据集中产生多个不同的训练集，这就对集成学习等方法有很大的好处。

然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差，因此，在初始数据量足够多的时候，还是留出法和交叉验证法更常用一些。

**嗯，这些方法的选取在实践中再确认下，然后将经验补充到这里。**




# 验证集




## 基于验证集的性能来进行模型选择和调参，测试集只是为了估计泛化能力


另外，需注意的是，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为 验证集 (validation set)。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，我们基于验证集上的性能来进行模型选择和调参。**基于验证集的性能来进行模型选择和调参，测试集只是为了估计泛化能力，是这样吗？那么这个验证集到底怎么划分？对应上面的留出法、k折、有放回采样的时候，验证集怎么划出来？**





# 最终交给用户的模型


给定包含 m 个样本的数据集 D ，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集 D 重新训练模型，这个模型在训练过程中使用了所有 m 个样本，这才是我们最终提交给用户的模型。 **确定****是这样吗？ 以前我看到这个地方的时候就有疑问。注意一下，确认一下。**













* * *





# COMMENT


自助采样法在机器学习中有重要用途，［Efron and Tibshirani，1993］对此 进行了详细的讨论.

[Dietterich, 1998]指出了常规fc折交叉验证法存在的风险，并提出了 5 x 2 交叉验证法.[Demsar, 2006]讨论了对多个算法进行比较检验的方法.



# REF

* 《机器学习》
