---
author: evo
comments: true
date: 2018-05-20 12:51:52+00:00
layout: post
link: http://106.15.37.116/2018/05/20/tl-%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86/
slug: tl-%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86
title: TL 迁移学习的概念与定义
wordpress_id: 6095
categories:
- 人工智能学习
tags:
- Transfer Learning
---

<!-- more -->

[mathjax]

**注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。**


# ORIGINAL





 	
  1. [迁移学习简明手册](https://github.com/jindongwang/transferlearning-tutorial)  [王晋东](https://zhuanlan.zhihu.com/p/35352154)




# TODO





 	
  * aaa





* * *





# INTRODUCTION





 	
  * aaa







# 两个基本的概念：领域 和 任务


在迁移学习中，有两个基本的概念：领域 (Domain) 和任务 (Task) 。它们是最基础的概念。

他们定义如下：


## 领域  (Domain)


领域是进行学习的主体，主要由两部分构成：** 什么叫进行学习的主体？**



 	
  * 数据

 	
  * 生成这些数据的概率分布


注意：概率分布 \(P\) 通常只是一个逻辑上的概念，即我们认为不同领域有不同的概率分布，却一般不给出（也难以给出） \(P\) 的具体形式。**领域包括生成这些数据的概率分布吗？为什么？领域与特征空间是什么关系？**

在迁移学习中，我们会对应于两个基本的领域：



 	
  * 源领域 (Source Domain) ：就是有知识、有大量数据标注的领域，是我们要迁移的对象；

 	
  * 目标领域 (Target Domain) ：就是我们最终要赋予知识、赋予标注的对象。


把知识从源领域传递到目标领域，就完成了迁移。

我们通常使用的符号如下：

 	
  * 我们用花体 \(\mathcal{D}\) 来表示一个 domain，用小写下标 \(s\) 和 \(t\) 来分别指代两个领域，如 \(\mathcal{D}_s\) 表示源领域， \(\mathcal{D}_t\) 表示目标领域。

 	
  * 我们用大写斜体 \(P\) 来表示一个概率分布。概率分布只是一个逻辑上的概念，即我们认为不同领域有不同的概率分布，却一般不给出（也难以给出） \(P\) 的具体形式。

 	
  * 我们用大写的黑体 \(\mathbf{X}\) 表示一整个领域的数据，这是一种矩阵形式。

 	
  * 对于领域上的单个数据，我们通常用小写粗体 \(\mathbf{x}\) 来表示，它也是向量的表示形式。例如， \(\mathbf{x}_i\) 就表示第 \(i\) 个样本或特征。**？是样本还是特征？这个有很大不一样吧？要确认下。**

 	
  * 我们用大写花体 \(\mathcal{X}\) 来表示数据的特征空间。**特征空间与领域有什么区别和联系？**




## 任务 (Task)


任务是学习的目标，它主要由两部分组成：



 	
  * 标签 **标签也是任务的一部分吗？ 为什么？**

 	
  * 标签对应的函数


我们通常使用的符号如下：

 	
  * 我们用花体 \(\mathcal{Y}\) 来表示一个标签空间，领域和目标领域的类别空间就可以分别表示为 \(\mathcal{Y}_s\) 和 \(\mathcal{Y}_t\)

 	
  * 我们用小写的 \(y_s\) 和 \(y_t\) 分别表示源领域和目标领域的实际类别。

 	
  * 我们用 \(f(\cdot)\) 来表示一个学习函数。




## 再对这些符号总结一下


\[\begin{array}{|c|c|}
\hline
符号 & 含义 \\ \hline
下标 s / t & 指示源域 / 目标域 \\ \hline
\mathcal{ D }_s / \mathcal{ D }_t & 源域数据 / 目标域数据 \\ \hline
\mathbf{ x } / \mathbf{ X } / \mathcal{ X } &向量 / 矩阵 / 特征空间 \\ \hline
\mathbf{ y } / \mathcal{ Y } &类别向量 / 类别空间 \\ \hline
(n, m)[或(n_1, n_2) 或(n_s, n_t)] &(源域样本数, 目标域样本数)\\ \hline
P(\mathbf{ x }_s) / P(\mathbf{ x }_t) & 源域数据 / 目标域数据的边缘分布 \\ \hline
Q(\mathbf{ y }_s | \mathbf{ x }_s) / Q(\mathbf{ y }_t | \mathbf{ x }_t) & 源域数据 / 目标域数据的条件分布 \\ \hline
f(\cdot) & 要学习的目标函数 \\ \hline
\end{array}\]



OK，有上面两个概念之后，我们就可以对迁移学习进行形式化：






# 迁移学习的形式化




## 定义


把一个问题进行形式化，是进行一切研究的前提。

迁移学习 (Transfer Learning) 定义如下：

我们给定一个有标记的源域 \(\mathcal{D}_s=\{\mathbf{x}_{i},y_{i}\}^n_{i=1}\) 和一个无标记的目标域 \(\mathcal{D}_t=\{\mathbf{x}_{j}\}^{n+m}_{j=n+1}\) ，这两个领域的数据分布 \(P(\mathbf{x}_s)\) 和P( \(\mathbf{x}_t)\) 是不同的，即 \(P(\mathbf{x}_s) \ne P(\mathbf{x}_t)\) ，那么迁移学习的目的，就是要借助 \(\mathcal{D}_s\) 的知识，来学习目标域 \(\mathcal{D}_t\) 的知识(标签)。**为什么 \(\mathcal{D}_t\)  的下标是从 n+1 开始的？而把n+m作为上线？为什么要把 n 放进来？**

更进一步，结合我们前面说过的迁移学习研究领域，迁移学习的定义需要进行如下的考虑：



 	
  1. 特征空间的异同，即 \(\mathcal{X}_s\) 和 \(\mathcal{X}_t\) 是否相等。

 	
  2. 类别空间的异同：即 \(\mathcal{Y}_s\) 和 \(\mathcal{Y}_t\) 是否相等。

 	
  3. 条件概率分布的异同：即 \(Q_s(y_s|\mathbf{x}_s)\) 和 \(Q_t(y_t|\mathbf{x}_t)\) 是否相等。


**之前的 [TL 迁移学习的研究领域](http://106.15.37.116/2018/05/20/tl-%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0%e7%9a%84%e7%a0%94%e7%a9%b6%e9%a2%86%e5%9f%9f/) 中没有提到这些考虑吧？确认下这些异同是怎么来的？**

在实际的研究和应用中，读者可以针对自己的不同任务，结合上述表述，灵活地给出相关的形式化定义，比如 领域自适应的定义如下：

领域自适应(Domain Adaptation)：**领域自适应不是在计算机视觉中用到的吗？是吗？确认下。**

给定一个有标记的源域 \(\mathcal{D}_s=\{\mathbf{x}_{i},y_{i}\}^n_{i=1}\) 和一个无标记的目标域 \(\mathcal{D}_t=\{\mathbf{x}_{j}\}^{n+m}_{j=n+1}\) ，假定它们的：



 	
  * 特征空间相同，即 \(\mathcal{X}_s = \mathcal{X}_t\) ，

 	
  * 并且它们的类别空间也相同，即 \(\mathcal{Y}_s = \mathcal{Y}_t\)

 	
  * 以及条件概率分布也相同，即 \(Q_s(y_s|\mathbf{x}_s) = Q_t(y_t|\mathbf{x}_t)\) 。


但是，这两个域的边缘分布不同，即 \(P_s(\mathbf{x}_s) \ne P_t(\mathbf{x}_t)\) ，那么迁移学习的目标就是，利用有标记的数据 \(\mathcal{D}_s\) 去学习一个分类器 \(f:\mathbf{x}_t \mapsto \mathbf{y}_t\) 来预测目标域 \(\mathcal{D}_t\) 的标签 \(\mathbf{y}_t \in \mathcal{Y}_t\)  。

**为什么这个地方放了一个领域自适应的定义呢？要把它放到领域自适应里面。**


## 迁移学习研究的总体思路


迁移学习的总体思路可以概括为：



 	
  * 开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。





# 迁移学习的核心：相似性




## 找出相似性，也即不变量


之前，我们已经知道，迁移学习的核心：



 	
  * 找到源领域和目标领域之间的相似性，并加以合理利用。


相似性其实是非常普遍的。

比如：

 	
  * 不同人的身体构造是相似的；

 	
  * 自行车和摩托车的骑行方式是相似的；

 	
  * 国际象棋和中国象棋是相似的；

 	
  * 羽毛球和网球的打球方式是相似的。


这种相似性也可以理解为不变量。

举一个例子：

我们都知道在中国大陆开车时，驾驶员坐在左边，靠马路右侧行驶。这是基本的规则。然而，如果在英国、香港等地区开车，驾驶员是坐在右边，需要靠马路左侧行驶。那么，如果我们从中国大陆到了香港，应该如何快速地适应他们的开车方式呢？

诀窍就是找到这里的不变量：不论在哪个地区，驾驶员都是紧靠马路中间。这就是我们这个开车问题中的不变量。找到相似性 (不变量)，是进行迁移学习的核心。


## 度量相似性


有了这种相似性后，下一步工作就是，如何要度量这种相似性。

度量工作的目标有两点：



 	
  1. 要很好地度量两个领域的相似性，不仅能够定性地告诉我们它们是否相似，更能够定量地给出相似程度。

 	
  2. 要以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。**增大两个领域之间的相似性 是什么意思？怎么增大？为什么要增大？增大什么？**


一句话总结： 相似性是核心，度量准则是重要手段。


## 度量的准则


度量不仅是机器学习和统计学等学科中使用的基础手段，也是迁移学习中的重要工具。它的核心就是衡量两个数据域的差异。

实际上，计算两个向量（点、矩阵）的距离和相似度是许多机器学习算法的基础，有时候一个好的距离度量就能决定算法最后的结果好坏。比如 KNN 分类算法就对距离非常敏感。

本质上就是找一个变换使得源域和目标域的距离最小（相似度最大）。所以，相似度和距离度量在机器学习中非常重要。**是吗？使得最小 这个是本质吗？本质不应该是一种衡量吗？**

这里给出常用的度量手段，它们都是迁移学习研究中非常常见的度量准则。对这些准则有很好的理解，可以帮助我们设计出更加好用的算法。用一个简单的式子来表示，度量就是描述源域和目标域这两个领域的距离：

\begin{equation}
\label{eq-distance}
DISTANCE(\mathcal{D}_s,\mathcal{D}_t) = \mathrm{DistanceMeasure}(\cdot,\cdot)
\end{equation}

下面我们从距离和相似度度量准则几个方面进行简要介绍。

关于距离和相似度准则等，已经总结到：[ML 距离](http://106.15.37.116/2018/05/20/ml-distance/)  这里就不再重复。















* * *





# COMMENT


OK，本部分介绍迁移学习领域的一些基本知识。我们将对迁移学习的问题进行简单的形式化，给出迁移学习的总体思路，并且介绍目前常用的一些度量准则。

本部分中出现的所有符号和表示形式，是以后章节的基础。
