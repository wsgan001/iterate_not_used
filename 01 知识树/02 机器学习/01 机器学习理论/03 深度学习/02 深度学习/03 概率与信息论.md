
# REF
1. 《深度学习》Ian Goodfellow




# TODO






  * aaa





* * *





# INTRODUCTION






  * aaa











第三章 概率与信息论
本章我们讨论概率论和信息论。

概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方

法，也提供了用于导出新的不确定性声明(statement)的公理。在人工智能领域，

概率论主要有两种用途。首先，概率法则告诉我们 AI 系统如何推理，据此我们设计

一些算法来计算或者估算由概率论导出的表达式。其次，我们可以用概率和统计从 理论上分析我们提出的 AI 系统的行为。

概率论是众多科学学科和工程学科的基本工具。我们提供这一章，是为了确保

那些背景偏软件工程而较少接触概率论的读者也可以理解本书的内容。

概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行推理

而信息论使我们能够量化概率分布中的不确定性总量。

如果你已经对概率论和信息论很熟悉了，那么除了第3.14节以外的整章内容，你

都可以跳过。而在第3.14节中，我们会介绍用来描述机器学习中结构化概率模型的

图。即使你对这些主题没有任何的先验知识，本章对于完成深度学习的研究项目来 说也已经足够，尽管如此我们还是建议你能够参考一些额外的资料，例如 Jaynes

(2003)。

3.1 为什么要使用概率？
计算机科学的许多分支处理的实体大部分都是完全确定且必然的。程序员通常

可以安全地假定CPU将完美地执行每条机器指令。虽然硬件错误确实会发生，但它

们足够罕见，以致于大部分软件应用在设计时并不需要考虑这些因素的影响。鉴于

许多计算机科学家和软件工程师在一个相对干净和确定的环境中工作，机器学习对

47

于概率论的大量使用是很令人吃惊的。

这是因为机器学习通常必须处理不确定量，有时也可能需要处理随机 (非确定性 的) 量。不确定性和随机性可能来自多个方面。至少从 20 世纪 80 年代开始，研究 人员就对使用概率论来量化不确定性提出了令人信服的论据。这里给出的许多论据 都是根据 Pearl (1988) 的工作总结或启发得到的。

几乎所有的活动都需要一些在不确定性存在的情况下进行推理的能力。事实上

除了那些被定义为真的数学声明，我们很难认定某个命题是千真万确的或者确保某

件事一定会发生。

不确定性有三种可能的来源：

1.    被建模系统内在的随机性。例如，大多数量子力学的解释，都将亚原子粒子的 动力学描述为概率的。我们还可以创建一些我们假设具有随机动态的理论情境 例如一个假想的纸牌游戏，在这个游戏中我们假设纸牌被真正混洗成了随机顺 序。

2.    不完全观测。即使是确定的系统，当我们不能观测到所有驱动系统行为的变量 时，该系统也会呈现随机性。例如，在Monty Hall问题中，一个游戏节目的参

与者被要求在三个门之间选择，并且会赢得放置在选中门后的奖品。其中两扇

门通向山羊，第三扇门通向一辆汽车。选手的每个选择所导致的结果是确定的

但是站在选手的角度，结果是不确定的。

3.    不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会

导致模型的预测出现不确定性。例如，假设我们制作了一个机器人，它可以准

确地观察周围每一个对象的位置。在对这些对象将来的位置进行预测时，如果

机器人采用的是离散化的空间，那么离散化的方法将使得机器人无法确定对象

们的精确位置：因为每个对象都可能处于它被观测到的离散单元的任何一个角

落。

在很多情况下，使用一些简单而不确定的规则要比复杂而确定的规则更为实用， 即使真正的规则是确定的并且我们建模的系统可以足够精确地容纳复杂的规则。例 如， ‘‘多数鸟儿都会飞'' 这个简单的规则描述起来很简单很并且使用广泛，而正式的 规则——‘‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的

鸟，包括食火鸟(cassowary)、蛇鸟(ostrich)、几维(kiwi，一种新西兰产的无翼鸟) 等不会飞的鸟类……以外，鸟儿会飞”，很难应用、维护和沟通，即使经过这么多的

努力，这个规则还是很脆弱而且容易失效。

尽管我们的确需要一种用以对不确定性进行表示和推理的方法，但是概率论并

不能明显地提供我们在人工智能领域需要的所有工具。概率论最初的发展是为了分 析事件发生的频率。我们可以很容易地看出概率论，对于像在扑克牌游戏中抽出一 手特定的牌这种事件的研究中，是如何使用的。这类事件往往是可以重复的。当我 们说一个结果发生的概率为p，这意味着如果我们反复实验(例如，抽取一手牌)无 限次，有p的比例可能会导致这样的结果。这种推理似乎并不立即适用于那些不可 重复的命题。如果一个医生诊断了病人，并说该病人患流感的几率为 40%，这意味 着非常不同的事情——我们既不能让病人有无穷多的副本，也没有任何理由去相信 病人的不同副本在具有不同的潜在条件下表现出相同的症状。在医生诊断病人的例 子中，我们用概率来表示一种信任度(degree of belief)，其中1表示非常肯定病人 患有流感，而 0 表示非常肯定病人没有流感。前面那种概率，直接与事件发生的频 率相联系，被称为频率派概率(frequentist probability);而后者，涉及到确定性水 平，被称为 贝叶斯概率( Bayesian probability)。

关于不确定性的常识推理，如果我们已经列出了若干条我们期望它具有的性质

那么满足这些性质的唯一一种方法就是将贝叶斯概率和频率派概率视为等同的。例

如，如果我们要在扑克牌游戏中根据玩家手上的牌计算她能够获胜的概率，我们使

用和医生情境完全相同的公式，就是我们依据病人的某些症状计算她是否患病的概 率。为什么一小组常识性假设蕴含了必须是相同的公理控制两种概率？更多的细节 参见 Ramsey (1926)。

概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规

则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。概

率论提供了一套形式化的规则，可以在给定一些命题的似然后，计算其他命题为真

的似然。

3.2 随机变量
随机变量(random variable )是可以随机地取不同值的变量。我们通常用无路 式字体 (plain typeface) 中的小写字母来表示随机变量本身，而用手写体中的小写字 母来表示随机变量能够取到的值。例如， x1 和 x2 都是随机变量 x 可能的取值。对 于向量值变量，我们会将随机变量写成X，它的一个可能取值为就其本身而言， 一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状 态的可能性。

随机变量可以是离散的或者连续的。离散随机变量拥有有限或者可数无限多的

状态。注意这些状态不一定非要是整数；它们也可能只是一些被命名的状态而没有

数值。连续随机变量伴随着实数值。

3.3 概率分布
概率分布(probability distribution )用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散

的还是连续的。

3.3.1 离散型变量和概率质量函数
离散型变量的概率分布可以用概率质量函数( probability mass function, PMF ) 1来描述。我们通常用大写字母P来表示概率质量函数。通常每一个随机变量都会有 一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的PMF，而不 是根据函数的名称来推断；例如， P(x) 通常和 P(y) 不一样。

概率质量函数将随机变量能够取得的每个状态映射到随机变量取得该状态的概 率。x = x的概率用P(x)来表示，概率为1表示x = x是确定的，概率为0表示 x = x是不可能发生的。有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x= x)。有时我们会先定义一个随机变量，然后用~符号来说明 它遵循的分布：x〜P(x)。

概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称 为 联合概率分布( joint probability distribution)。 P(x = x, y = y) 表示 x = x 和 y = y同时发生的概率。我们也可以简写为P(x,y)。

如果一个函数P是随机变量x的PMF，必须满足下面这几个条件：

• P 的定义域必须是 x 所有可能状态的集合。

译者注：国内有些教材也将它翻译成概率分布律

•    Vx e x, 0 < P(x) < 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。类似的，能够确保一定发生的事件概率为 1，而且不存在比这概率更 高的状态。

•    ExexP(x) = 1.我们把这条性质称之为归一化的(normalized)。如果没有这 条性质，当我们计算很多事件其中之一发生的概率时可能会得到大于 1 的概 率。

例如，考虑一个离散型随机变量x有k个不同的状态。我们可以假设x是均匀 分布(uniform distribution )的(也就是将它的每个状态视为等可能的)，通过将它 的PMF设为

P (x = Xi) = k    (3.1)

对于所有的 i 都成立。我们可以看出这满足上述成为概率质量函数的条件。因为 k 是一个正整数，所以k是正的。我们也可以看出





3.3.2 连续型变量和概率密度函数
当我们研究的对象是连续型随机变量时，我们用概率密度函数(probability density function, PDF )而不是概率质量函数来描述它的概率分布。如果一个函数p

是概率密度函数，必须满足下面这几个条件：

•    p的定义域必须是x所有可能状态的集合。

•    Vx e x,p(x) > 0.注意，我们并不要求p(x) < 1。

•    p(x) dx = 1.

概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为6x的无限小的区域内的概率为p(x)dx。

我们可以对概率密度函数求积分来获得点集的真实概率质量。特别地， x 落在 集合S中的概率可以通过p(x)对这个集合求积分来得到。在单变量的例子中，x落 在区间[a,b]的概率是J[a,b] p(x)dx。

为了给出一个连续型随机变量的 PDF 的例子，我们可以考虑实数区间上的均匀 分布。我们可以使用函数u(x; a,b)，其中a和b是区间的端点且满足b > a。符号 “;”表示“以什么为参数”；我们把x作为函数的自变量，a和b作为定义函数的参 数。为了确保区间外没有概率，我们对所有的x e ［a,b］，令u(x;a,b) = 0。在［a,b］ 内，有u(x; “,~ = &。我们可以看出任何一点都非负。另外，它的积分为1。我们 通常用x〜U(a, b)表示x在［a, b］上是均匀分布的。

3.4 边缘概率

有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为 边缘概率分布( marginal probability distribution)。

例如，假设有离散型随机变量x和y，并且我们知道P(x,y)。我们可以依据下 面的求和法则(sum rule )来计算P(x):

Vx e x, P(x = x)=乏2 P(x = x, y = y).    (3.3)

y

“边缘概率”的名称来源于手算边缘概率的计算过程。当P(x,y)的每个值被写 在由每行表示不同的 x 值，每列表示不同的 y 值形成的网路中时，对网路中的每行 求和是很自然的事情，然后将求和的结果 P(x) 写在每行右边的纸的边缘处。

对于连续型变量，我们需要用积分替代求和：

p(x) = p(x, y)dy.    (3.4)

3.5 条件概率

在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。我们将给定 x = x， y = y 发生的条件概率记为 P(y = y | x = x)。这个条件概率可以通过下面的公式计算：

y x    P(y=y,x=x)    (3.5)

P(y = y|x =x) = P (x = x) -    (3.5)

条件概率只在 P(x=x) >0时有定义。我们不能计算给定在永远不会发生的事件上 的条件概率。

这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什么相混 淆。假定某个人说德语，那么他是德国人的条件概率是非常高的，但是如果随机选 择的一个人会说德语，他的国籍不会因此而改变。计算一个行动的后果被称为干预 查询(intervention query )。干预查询属于因果模型(causal modeling)的范畴，我 们不会在本书中讨论。

3.6 条件概率的链式法则
任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相

乘的形式：

P(x(1),...,x(n)) = P(x⑴)n=2P(x⑴ I x(1),...,x(i-1)).    (3.6)

这个规则被称为概率的链式法则(chain rule)或者乘法法则(product rule)。 它可以直接从式(3.5)条件概率的定义中得到。例如，使用两次定义可以得到

P(a, b, c) = P(a I b, c)P(b, c)

P(b,c) = P(b I c)P(c)

P(a, b, c) = P(a I b, c)P(b I c)P(c).

3.7 独立性和条件独立性
两个随机变量x和y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含x另一个因子只包含y，我们就称这两个随机变量是相互独立的

independent )：

Vx G x,y G y,p(x = x,y = y) = p(x = x)p(y = y).    (3.7)

如果关于 x 和 y 的条件概率分布对于 z 的每一个值都可以写成乘积的形式 那么这两个随机变量 x 和 y 在给定随机变量 z 时是条件独立的( conditionally

independent)：

Vx e x, y e y, z e z, p(x = x, y = y | z = z) = p(x = x | z = z) p(y = y | z = z) .

(3.8)

我们可以采用一种简化形式来表示独立性和条件独立性：x丄y表示x和y相互 独立，x丄y | z表示x和y在给定z时条件独立。

3.8 期望、方差和协方差
函数f(x)关于某分布P(x)的期望(expectation )或者期望值(expected value)是指，当x由P产生，f作用于x时，f(x)的平均值。对于离散型随 机变量，这可以通过求和得到：

Ex^p [f (x)] =    P (x)f (x),    (3.9)

对于连续型随机变量可以通过求积分得到：

Ex-p[f (x)] = y p(x)f (x)dx.    (3.10)

当概率分布在上下文中指明时，我们可以只写出期望作用的随机变量的名称来进行 简化，例如Ex[f(x)]。如果期望作用的随机变量也很明确，我们可以完全不写脚标， 就像E[f(x)]。默认地，我们假设E卜]表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。

期望是线性的，例如，

Ex[af(x)+ 卢g(x)] = aEx[f (x)]+ 卢 Ex[g(x)],    (3.11)

其中a和冷不依赖于x。

方差( variance )衡量的是当我们对x依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：

Var(f(x)) =E[(f(x)-E[f(x)])2].    (3.12)

当方差很小时， f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为标 准差( standard deviation)。

协方差( covariance )在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度：

Cov(f(x),g(y)) =E[(f(x)-E[f(x)])(g(y)-E[g(y)])].    (3.13)

协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很 远。如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。如果协方 差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于 取得相对较小的值，反之亦然。其他的衡量指标如相关系数( correlation )将每个变 量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。

协方差和相关性是有联系的，但实际上是不同的概念。它们是有联系的，因为 两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么 它们一定是相关的。然而，独立性又是和协方差完全不同的性质。两个变量如果协 方差为零，它们之间一定没有线性关系。独立性比零协方差的要求更强，因为独立 性还排除了非线性的关系。两个变量相互依赖但具有零协方差是可能的。例如，假 设我们首先从区间[-1,1]上的均匀分布中采样出一个实数x。然后我们对一个随机 变量s进行采样。s以1的概率值为1,否则为-1。我们可以通过令y = sx来生成 一个随机变量y。显然，x和y不是相互独立的，因为x完全决定了 y的尺度。然 而， Cov(x, y) = 0。

随机向量z e Rn的协方差矩阵(covariance matrix )是一个n x n的矩阵，并 且满足

Cov(x)i,j = Cov(xi, xj).


(3.14)


协方差矩阵的对角元是方差：

Cov(xi, xi) = Var(xi).

(3.15)


3.9 常用概率分布
许多简单的概率分布在机器学习的众多领域中都是有用的

3.9.1 Bernoulli 分布
Bernoulli分布(Bernoulli distribution )是单个二值随机变量的分布。它由单 个参数4 G [0,1]控制，4给出了随机变量等于1的概率。它具有如下的一些性质：

P(x= 1) = 4 P(x = 0) = 1 - 4 P(x=x)=4x(1-4)1-x Ex [x] = 4

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)


Varx(x) = 4(1 - 4)

3.9.2 Multinoulli 分布
Multinoulli 分布( multinoulli distribution)或者范畴分布(categorical distribution ) 是指在具有k个不同状态的单个离散型随机变量上的分布，其中k是一 个有限值。 1 Multinoulli 分布由向量 pG [0,1]k-1 参数化，其中每一个分量 pi 表示 第i个状态的概率。最后的第k个状态的概率可以通过1 - 1Tp给出。注意我们必 须限制1Tpg1。Multinoulli分布经常用来表示对象分类的分布，所以我们很少假 设状态1具有数值1之类的。因此，我们通常不需要去计算Multinoulli分布的随机 变量的期望和方差。

Bernoulli 分布和 Multinoulli 分布足够用来描述在它们领域内的任意分布。它们 能够描述这些分布，不是因为它们特别强大，而是因为它们的领域很简单；它们可 以对那些，能够将所有的状态进行枚举的离散型随机变量进行建模。当处理的是连 续型随机变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分 布都必须在分布上加以严格的限制。

3.9.3 高斯分布
实数上最常用的分布就是正恋分布(normal distribution )，也称为高斯分布 Gaussian distribution)：

N(x; "，CT2) = \/2i2 exp(-2b(x- ")2).    (3.21)

图3.1画出了正态分布的概率密度函数。


图3.1:正态分布。正态分布N(x; M,a2)呈现经典的“钟形曲线”的形状，其中中心峰的x坐标 由"给出，峰的宽度受a控制。在这个示例中，我们展示的是标准正态分布(standard normal distribution)，其中"= 0,a = 1。

正态分布由两个参数控制，P e R和a e (0,⑻)。参数p给出了中心峰值的坐 标，这也是分布的均值：E[x] = 。分布的标准差用a表示，方差用a2表示。

当我们要对概率密度函数求值时，我们需要对 a 平方并且取倒数。当我们需要 经常对不同参数下的概率密度函数求值时，一种更高效的参数化分布的方式是使用 参数冷e (0, to)，来控制分布的精度(precision )(或方差的倒数)：

N(x;"，卢-1) =    exp (—2 卢(x — ")2).    (3.22)

采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实

数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选

择，其中有两个原因。

第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。 中心极限

定理(central limit theorem )说明很多独立随机变量的和近似服从正态分布。这意

味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可

以被分解成一些更结构化的部分。

第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大

的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

充分利用和证明这个想法需要更多的数学工具，我们推迟到第 19.4.2节进行讲解。

正态分布可以推广到 Rn 空间，这种情况下被称为 多维正态分布( multivariate normal distribution )。它的参数是一个正定对称矩阵S:

N(a：;M,S) = ^(2n)n det(S) exp (-j(卜")TS-1(卜")).    (3.23)

参数M仍然表示分布的均值，只不过现在是向量值。参数S给出了分布的协 方差矩阵。和单变量的情况类似，当我们希望对很多不同参数下的概率密度函数多 次求值时，协方差矩阵并不是一个很高效的参数化分布的方式，因为对概率密度函 数求值时需要对S求逆。我们可以使用一个精度矩阵(precision matrix)卢进行替 代：    _

n (x； 卢 _i)=」d(in(^)㊀邓(一2(z_ ")t 卢(z_")) •    (3-24)

我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性 (isotropic)高斯分布，它的协方差矩阵是一个标量乘以单位阵。

3.9.4 指数分布和 Laplace 分布
在深度学习中，我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布(exponential distribution ):

p(x; A) = A1x>o exp(-Ax).    (3.25)

指数分布使用指示函数(indicator function)1x>o来使得当x取负值时的概率为零。

一个联系紧密的概率分布是Laplace分布(Laplace distribution )，它允许我们 在任意一点处设置概率质量的峰值

1    /    | x _    | \

Laplace(x;    y) = — exp (--) .    (3.26)

2^    \ Y /

3.9.5    Dirac 分布和经验分布
在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过Dirac delta函数(Dirac delta function ) J(x)定义概率密度函数来实现：

p(x) = J(x — ^).    (3.27)

Dirac delta函数被定义成在除了 0以外的所有点的值都为0,但是积分为1。Dirac delta 函数不像普通函数一样对 x 的每一个值都有一个实数值的输出，它是一种不同 类型的数学对象，被称为广义函数(generalized function )，广义函数是依据积分性 质定义的数学对象。我们可以把Dirac delta函数想成一系列函数的极限点，这一系 列函数把除 0 以外的所有点的概率密度越变越小。

通过把p(x)定义成d函数左移-^个单位，我们得到了一个在x = ^处具有 无限窄也无限高的峰值的概率质量。

Dirac分布经常作为经验分布(empirical distribution )的一个组成部分出现：

m

p(x) = — 5"^d(x_ x(i))    (3.28)

m

i=1

经验分布将概率密度m赋给m个点x(1),..., x(m)中的每一个，这些点是给定的 数据集或者采样的集合。只有在定义连续型随机变量的经验分布时， Dirac delta 函 数才是必要的。对于离散型随机变量，情况更加简单：经验分布可以被定义成一 个 Multinoulli 分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那 个输入值的 经验频率( empirical frequency)。

当我们在训练集上训练模型时，我们可以认为从这个训练集上得到的经验分

布指明了我们采样来源的分布。关于经验分布另外一种重要的观点是，它是训练数

据的似然最大的那个概率密度函数(见第5.5节)。

3.9.6    分布的混合
通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组 合方法是构造混合分布(mixture distribution )。混合分布由一些组件(component) 分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个 Multinoulli 分 布中采样的结果：

P(x) =    P(c = i)P(x I c = i),    (3.29)

这里 P(c) 是对各组件的一个 Multinoulli 分布。

我们已经看过一个混合分布的例子了：实值变量的经验分布对于每一个训练实 例来说，就是以 Dirac 分布为组件的混合分布。

混合模型是组合简单概率分布来生成更丰富的分布的一种简单策略。在第十

六章中，我们更加详细地探讨从简单概率分布构建复杂模型的技术。

混合模型使我们能够一瞥以后会用到的一个非常重要的概念——潜变量

(latent variable )。潜变量是我们不能直接观测到的随机变量。混合模型的组件标 识变量c就是其中一个例子。潜变量在联合分布中可能和x有关，在这种情况下， P(x, c) = P(x | c)P(c)。潜变量的分布P(c)以及关联潜变量和观测变量的条件分布 P(x | c)，共同决定了分布P(x)的形状，尽管描述P(x)时可能并不需要潜变量。潜

变量将在第16.5节中深入讨论。

一个非常强大且常见的混合模型是高斯混合模型( Gaussian Mixture Model) 它的组件p(x I c = i)是高斯分布。每个组件都有各自的参数，均值M(i)和协方差矩 阵s(i)。有一些混合可以有更多的限制。例如，协方差矩阵可以通过S(i) = S,Vi的 形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制每个组 件的协方差矩阵为对角的或者各向同性的 (标量乘以单位矩阵)。

除了均值和协方差以外，高斯混合模型的参数指明了给每个组件i的先验概率 (prior probability)ai = P(c = i)。 ‘‘先验'' 一词表明了在观测到 x 之前传递给模 型关于c的信念。作为对比，P(c I a:)是后验概率(posterior probability )，因为它 是在观测到 x 之后进行计算的。高斯混合模型是概率密度的 万能近似器( universal approximator)，在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高 斯混合模型以任意精度来逼近。

图3.2演示了某个高斯混合模型生成的样本。


x1

图 3.2: 来自高斯混合模型的样本。在这个示例中，有三个组件。从左到右，第一个组件具有各向 同性的协方差矩阵，这意味着它在每个方向上具有相同的方差。第二个组件具有对角的协方差矩

阵，这意味着它可以沿着每个轴的对齐方向单独控制方差。该示例中，沿着X2轴的方差要比沿着

x1 轴的方差大。第三个组件具有满秩的协方差矩阵，使它能够沿着任意基的方向单独地控制方差。

3.10 常用函数的有用性质
某些函数在处理概率分布时经常会出现，尤其是深度学习的模型中用到的概率

分布。

其中一个函数是 logistic sigmoid 函数：

咖=1 + exp(_x) -    (3.30)

logistic sigmoid函数通常用来产生Bernoulli分布中的参数々，因为它的范围是 (0,1)，处在々的有效取值范围内。图3.3给出了 sigmoid函数的图示。sigmoid函数 在变量取绝对值非常大的正值或负值时会出现饱和(saturate)现象，意味着函数会 变得很平，并且对输入的微小改变会变得不敏感。

另外一个经常遇到的函数是softplus函数(softplus function) (Dugas et al., 2001)：

Z (x) = log(1 + exp(x)).    (3.31)

softplus函数可以用来产生正态分布的冷和a参数，因为它的范围是(0,⑻)。当处 理包含 sigmoid 函数的表达式时它也经常出现。 softplus 函数名来源于它是另外一个


图 3.3: logistic sigmoid 函数。


函数的平滑(或 ‘‘软化'')形式，这个函数是

x+ = max(0, x).

(3.32)


图3.4给出了 softplus 函数的图示。


图 3.4: softplus 函数


下面一些性质非常有用，你可能要记下来：

exp(x)

.(x)


exp(x) + exp(0)

=a(x)(1 - CT(x)) 1 — a(x) = a(—x)

log CT(X) = -Z(_x)

dxZ (x) = "(x)

Vx e (0, 1),CT-I(x) = log (

Vx > 0, Z-1(x) = log(exp(x) — 1)

Z(x)二    a(y)dy

J—⑺

Z(x) - Z(-x) = x

(3.33)

(3.34)

(3.35)

(3.36)

(3.37)

(3.38)

(3.39)

(3.40)

(3.41)


函数a-1(x)在统计学中被称为分对数(logit),但这个函数在机器学习中很少用到。

式(3.41)为函数名“softplus''提供了其他的正当理由。softplus函数被设计成正 部函数(positive part function)的平滑版本，这个正部函数是指x+ = max{0,x}。 与正部函数相对的是负部函数(negative part function)x- = max{0, _x}。为了获 得类似负部函数的一个平滑函数，我们可以使用Z(_x)。就像x可以用它的正部和 负部通过等式 x+_x— = x 恢复一样，我们也可以用同样的方式对 Z(x) 和 Z(_x) 进行操作，就像式(3.41)中那样。

1

“multinoulli''这个术语是最近被Gustavo Lacerdo发明、被Murphy (2012)推广的。Multinoulli分布是多 项式分布(multinomial distribution)的一个特例。多项式分布是{0,…，n}k中的向量的分布，用于表示当 对Multinoulli分布采样n次时k个类中的每一个被访问的次数。很多文章使用“多项式分布”而实际上说的 是 Multinoulli 分布，但是他们并没有说是对 n = 1 的情况，这点需要注意。



3.11 贝叶斯规则
我们经常会需要在已知P(y | x)时计算P(x | y)。幸运的是，如果还知道P(x)， 我们可以用贝叶斯规则(Bayes' rule)来实现这一目的：

P(x| y)=


P(x)P(y | x) p (y)


(3.42)


注意到 P(y) 出现在上面的公式中，它通常使用 P(y) = xP(y | x)P(x) 来计算， 所以我们并不需要事先知道 P(y) 的信息。

贝叶斯规则可以从条件概率的定义直接推导得出，但我们最好记住这个公式的 名字，因为很多文献通过名字来引用这个公式。这个公式是以牧师 Thomas Bayes 的名字来命名的， 他是第一个发现这个公式特例的人。这里介绍的一般形式由 Pierre-Simon Laplace 独立发现。

3.12 连续型变量的技术细节
连续型随机变量和概率密度函数的深入理解需要用到数学分支 测度论( measure theory)的相关内容来扩展概率论。测度论超出了本书的范畴，但我们可以简要勾勒 一些测度论用来解决的问题。

在第3.3.2节中，我们已经看到连续型向量值随机变量x落在某个集合S中的 概率是通过p(a)对集合S积分得到的。对于集合S的一些选择可能会引起悖论。例 如，构造两个集合Si和S2使得p(a e Si)+ p(a e S2) > 1并且Si n S2 = 0是可能 的。这些集合通常是大量使用了实数的无限精度来构造的，例如通过构造分形形状 (fractal-shaped) 的集合或者是通过有理数相关集合的变换定义的集合。 1 测度论的 一个重要贡献就是提供了一些集合的特征使得我们在计算概率时不会遇到悖论。在 本书中，我们只对相对简单的集合进行积分，所以测度论的这个方面不会成为一个 相关考虑。

对于我们的目的，测度论更多的是用来描述那些适用于 Rn 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的 点集。这种集合被称为 “零测度( measure zero)'' 的。我们不会在本书中给出这个 概念的正式定义。然而，直观地理解这个概念是有用的，我们可以认为零测度集在 我们的度量空间中不占有任何的体积。例如，在 R2 空间中，一条直线的测度为零 而填充的多边形具有正的测度。类似的，一个单独的点的测度为零。可数多个零测 度集的并仍然是零测度的 (所以所有有理数构成的集合测度为零)。

另外一个有用的测度论中的术语是“几乎处处(almost everywhere) ”。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都 是成立的。因为这些例外只在空间中占有极其微小的量，它们在多数应用中都可以 被放心地忽略。概率论中的一些重要结果对于离散值成立但对于连续值只能是 ‘‘几 乎处处”成立。

连续型随机变量的另一技术细节，涉及到处理那种相互之间有确定性函数关系

的连续型变量。假设我们有两个随机变量x和y满足y = g(x)，其中g是可逆的、 连续可微的函数。可能有人会想Py(y) = Px(g-1(y))。但实际上这并不对。

举一个简单的例子，假设我们有两个标量值随机变量x和y，并且满足y = x 以及x〜U(0,1)。如果我们使用py(y) = px(2y)，那么Py除了区间［0,1］以外都为 0，并且在这个区间上的值为 1。这意味着

J Py (y)dy = 1,    (3.43)

而这违背了概率密度的定义(积分为 1)。这个常见错误之所以错是因为它没有考虑 到引人函数g后造成的空间变形。回忆一下，x落在无穷小的体积为dx的区域内的 概率为p(x)dx。因为g可能会扩展或者压缩空间，在x空间内的包围着x的无穷小 体积在 y 空间中可能有不同的体积。

为了看出如何改正这个问题，我们回到标量值的情况。我们需要保持下面这个

性质：

|Py(g(x))dy|=|Px(x)dx|.    (3.44)

求解上式，我们得到

dx

Py(y)= Px(g-1 (y)) dy    (3.45)

或者等价地，

Px(x) = Py(g(x))    .    (3.46)

在高维空间中，微分运算扩展为Jacobian矩阵(Jacobian matrix)的行列式-

矩阵的每个元素为Jij = g。因此，对于实值向量x和y，

Px(x) = Py(g(x))

(3.47)


3.13 信息论
信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行

量化。它最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消

息，例如通过无线电传输来通信。在这种情况下，信息论告诉我们如何对消息设计

最优编码以及计算消息的期望长度，这些消息是使用多种不同编码机制、从特定

的概率分布上采样得到的。在机器学习中，我们也可以把信息论应用于连续型变量，

此时某些消息长度的解释不再适用。信息论是电子工程和计算机科学中许多领域的

基础。在本书中，我们主要使用信息论的一些关键思想来描述概率分布或者量化概 率分布之间的相似性。有关信息论的更多细节，参见 Cover and Thomas (2006) 或

者 MacKay (2003)。

信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事

件发生，能提供更多的信息。消息说：‘‘今天早上太阳升起'' 信息量是如此之少以至 于没有必要发送，但一条消息说：‘‘今天早上有日食'' 信息量就很丰富。

我们想要通过这种基本想法来量化信息。特别地，

•    非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件 应该没有信息量。

•    较不可能发生的事件具有更高的信息量。

•    独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量 应该是投掷一次硬币正面朝上的信息量的两倍。

为了满足上述三个性质，我们定义一个事件 x = x 的 自信息( self-information) 为

I(x) = -log P(x).    (3.48)

在本书中，我们总是用log来表示自然对数，其底数为e。因此我们定义的I(x)单 位是奈特(nats)。一奈特是以|的概率观测到一个事件时获得的信息量。其他的材 料中使用底数为2的对数，单位是比特(bit)或者香农(shannons);通过比特度 量的信息只是通过奈特度量信息的常数倍。

当 x 是连续的，我们使用类似的关于信息的定义，但有些来源于离散形式的性 质就丢失了。例如，一个具有单位密度的事件信息量仍然为 0，但是不能保证它一定

发生。

自信息只处理单个的输出。我们可以用香农摘(Shannon entropy )来对整个概 率分布中的不确定性总量进行量化：

H(x) = Ex〜p[I(x)] = —Ex^p [log P (x)],    (3.49)

也记作H(P)。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信 息总量。它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义 上的下界(当对数底数不是2时，单位将有所不同)。那些接近确定性的分布(输出几 乎可以确定) 具有较低的熵；那些接近均匀分布的概率分布具有较高的熵。图3.5给

出了一个说明。当 x 是连续的，香农熵被称为微分熵( differential entropy)。

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0

sc^u-xdojpus uou§qg


0.0    0.2    0.4    0.6    0.8    1.0

图 3.5: 二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而更

接近均匀分布的分布是如何具有较高的香农熵。水平轴是p表示二值随机变量等于1的概率。熵 由(p -1) log(1 - p) - plogp给出。当p接近0时，分布几乎是确定的，因为随机变量几乎总是 0。当p接近1时，分布也几乎是确定的，因为随机变量几乎总是1。当p = 0.5时，熵是最大的, 因为分布在两个结果(0 和 1)上是均匀的。

如果我们对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，我们可 以使用KL散度(Kullback-Leibler (KL) divergence)来衡量这两个分布的差异：

Dkl(P||Q) = Ex〜p log    = Ex^p[logP(x) - logQ(x)].    (3.50)

Q(x)

在离散型变量的情况下， KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号 的消息时，所需要的额外信息量 (如果我们使用底数为 2 的对数时，信息量用比特衡 量，但在机器学习中，我们通常用奈特和自然对数。 )

KL 散度有很多有用的性质，最重要的是它是非负的。 KL 散度为 0 当且仅当 P和Q在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎 处处'' 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某 些P和Q，Dkl(P||Q) = Dkl(Q||P)。这种非对称性意味着选择Dkl(P||Q)还是

DKL (Q||P ) 影响很大。更多细节可以看图3.6

q* 二 argmi〜Dkl(p||q)

x-ssu9a-scpqo£


x


q* 二 argmi〜Dkl (q||p)



-p(x)

A

——p(x)

--q*(x)

>>

1 \

1 \

q*(x)


图3.6: KL散度是不对称的。假设我们有一个分布p(x)，并且希望用另一个分布q(x)来近似它。 我们可以选择最小化DKL(p||q)或最小化DKL(q||p)。为了说明每种选择的效果，我们令p是两 个高斯分布的混合，令 q 为单个高斯分布。选择使用 KL 散度的哪个方向是取决于问题的。一些 应用需要这个近似分布 q 在真实分布 p 放置高概率的所有地方都放置高概率，而其他应用需要这 个近似分布 q 在真实分布 p 放置低概率的所有地方都很少放置高概率。 KL 散度方向的选择反映 了对于每种应用，优先考虑哪一种选择。 (左) 最小化 DKL(p||q) 的效果。在这种情况下，我们选 择一个q使得它在p具有高概率的地方具有高概率。当p具有多个峰时，q选择将这些峰模糊到 一起，以便将高概率质量放到所有峰上。f右」最小化DKL(q||p)的效果。在这种情况下，我们选 择一个q使得它在p具有低概率的地方具有低概率。当p具有多个峰并且这些峰间隔很宽时，如 该图所示，最小化 KL 散度会选择单个峰，以避免将概率质量放置在 p 的多个峰之间的低概率区 域中。这里，我们说明当q被选择成强调左边峰时的结果。我们也可以通过选择右边峰来得到KL 散度相同的值。如果这些峰没有被足够强的低概率区域分离，那么KL散度的这个方向仍然可能 选择模糊这些峰。

一个和KL散度密切联系的量是交叉熵(cross-entropy ) H(P, Q) = H(P) + Dkl(PIIQ)，它和KL散度很像但是缺少左边一项：

H(P, Q) = _Ex^p log Q(x).    (3.51)

针对 Q 最小化交叉熵等价于最小化 KL 散度，因为 Q 并不参与被省略的那一项。

当我们计算这些量时，经常会遇到0 log0这个表达式。按照惯例，在信息论中， 我们将这个表达式处理为limx^oxlogx = 0。

3.14 结构化概率模型
机器学习的算法经常会涉及到在非常多的随机变量上的概率分布。通常，这些概 率分布涉及到的直接相互作用都是介于非常少的变量之间的。使用单个函数来描述 整个联合概率分布是非常低效的 (无论是计算上还是统计上)。

我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表 示概率分布。例如，假设我们有三个随机变量a, b和c，并且a影响b的取值，b影 响 c 的取值，但是 a 和 c 在给定 b 时是条件独立的。我们可以把全部三个变量的概 率分布重新表示为两个变量的概率分布的连乘形式：

p(a, b, c) = p(a)p(b | a)p(c | b).    (3.52)

这种分解可以极大地减少用来描述一个分布的参数数量。每个因子使用的参数

数目是它的变量数目的指数倍。这意味着，如果我们能够找到一种使每个因子分布

具有更少变量的分解方法，我们就能极大地降低表示联合分布的成本。

我们可以用图来描述这种分解。这里我们使用的是图论中的 ‘‘图'' 的概念：由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分 解，我们把它称为结构化概率模型(structured probabilistic model)或者图模型

graphical model)。

有两种主要的结构化概率模型：有向的和无向的。两种图模型都使用图G，其中 图的每个节点对应着一个随机变量，连接两个随机变量的边意味着概率分布可以表 示成这两个随机变量之间的直接作用。

有向(directed)模型使用带有有向边的图，它们用条件概率分布来表示分解， 就像上面的例子。特别地，有向模型对于分布中的每一个随机变量 xi 都包含着一个 影响因子，这个组成xi条件概率的影响因子被称为xi的父节点，记为PaG(xi)：

p(x) = p(xi | PaG(xi)).    (3.53)

i

图3.7给出了一个有向图的例子以及它表示的概率分布的分解。

无向(undirected)模型使用带有无向边的图，它们将分解表示成一组函数；不

像有向模型那样，这些函数通常不是任何类型的概率分布。 G 中任何满足两两之 间有边连接的顶点的集合被称为团。无向模型中的每个团 C(i) 都伴随着一个因子 4(i)(C(i))。这些因子仅仅是函数，并不是概率分布。每个因子的输出都必须是非负


图3.7: 关于随机变量 a,b,c,d 和 e 的有向图模型。这幅图对应的概率分布可以分解为

p(a, b, c, d, e) = p(a)p(b | a)p(c | a, b)p(d | b)p(e | c).    (3.54)

该图模型使我们能够快速看出此分布的一些性质。例如，a和c直接相互影响，但a和e只有通 过c间接相互影响。

的，但是并没有像概率分布中那样要求因子的和或者积分为 1。

随机变量的联合概率与所有这些因子的乘积成比例(proportional)-意味着

因子的值越大则可能性越大。当然，不能保证这种乘积的求和为 1。所以我们需要除 以一个归一化常数Z来得到归一化的概率分布，归一化常数Z被定义为函数乘 积的所有状态的求和或积分。概率分布为：


i

图3.8给出了一个无向图的例子以及它表示的概率分布的分解。

请记住，这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互

相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特 殊描述(description)所具有的特性，而任何概率分布都可以用这两种方式进行描

述。

在本书第一部分和第二部分中，我们仅仅将结构化概率模型视作一门语言，来

描述不同的机器学习算法选择表示的直接的概率关系。在讨论研究课题之前，读者

不需要更深入地理解结构化概率模型。在第三部分的研究课题中，我们将更为详尽

地探讨结构化概率模型。

本章复习了概率论中与深度学习最为相关的一些基本概念。我们还剩下一些基



图 3.8: 关于随机变量 a,b,c,d 和 e 的无向图模型。这幅图对应的概率分布可以分解为 p(a，b，c，d，e) = z1 冷⑴(a,b, c)冷⑺(b, d)冷⑶(c,e).

(3.56) e 只有通


该图模型使我们能够快速看出此分布的一些性质。例如，a和c直接相互影响，但a和

过 c 间接相互影响。

本的数学工具需要讨论：数值方法。

1

Banach-Tarski定理给出了这类集合的一个有趣的例子。译者注：我们这里把“the set of rational numbers''翻

译成 ‘‘有理数相关集合''，理解为 ‘‘一些有理数组成的集合''，如果直接用后面的翻译读起来会比较拗口。









* * *





# COMMENT
