# REF
1. 《解析卷积神经网络》魏秀参




# TODO






  * aaa



* * *




# INTRODUCTION






  * aaa






















网络参数初始化



俗话说“万事开头难”，放在卷积神经网络训练中也是如此。通过上一篇基础

理论篇的介绍，我们知道神经网络模型一般依靠随机梯度下降法进行模型训练

和参数更新，网络的最终性能与收敛得到的最优解直接相关，而收敛效果实际

上又很大程度取决于网络参数最开始的初始化。理想的网络参数初始化使模型

训练事半功倍，相反，糟糕的初始化方案不仅会影响网络收敛甚至会导致“梯

度弥散”或“爆炸”致使训练失败i。面对如此重要而充满技巧性的模型参数 初始化，对于没有任何训练网络经验的使用者，他们往往不敢从头训练(from scratch)自己的神经网络。那么，网络参数初始化都有哪些方案？哪些又是相

对有效

络参数



章将逐一介绍和比较目前实践中常用的几种网


作为非线性激活函数，若参数初始化为过大值，前向运算

全为o或1的二值，而导致在反向运算时的对应梯度全部 度弥散”现象。无独有偶，不理想的初始化对于ReLU函数也会产生问

初始化，前向运算时的输出结果会有可能全部为负，经过ReLU函数后此 算时则毫无响应。这便是ReLU函数的“死区”现象。

7.1全零初始化
通过合理的数据预处理和规范化，当网络收敛到稳定状态时，参数(权值)在

理想情况下应基本保持正负各半的状态(此时期望为0)。因此，一种简单且听 起来合理的参数初始化做法是，干脆将所有参数都初始化为o,因为这样可使 得初始化全零时参数的期望(expectation)与网络稳定时参数的期望一致为零。

不过，细细想来则会发现参数全为0时网络不同神经元的输出必然相同，相 同输出则导致梯度更新完全一样，这样便会令更新后的参数仍然保持一样的状 态。换句话说，如若参数进行了全零初始化，那么网络神经元将毫无能力对此 做出改变，从而无法进行模型训练。

7.2随机初始化
将参数随机化自然是打破上述“僵局”的一个有效手段，不过我们仍然希望所有

参数期望依旧接近0。遵循这一原则，我们可将参数值随机设定为接近o的一个 很小的随机数(有正有负)。在实际应用中，随机参数服从高斯分布(Gaussian distribution)或均匀分布(uniform distribution)都是较有效的初始化方式。

假设网络输人神经元个数为nin，输出神经元个数位nQut,则服从高斯分布 的参数随机初始化为：

1    % Parameter initialization following

2    % the Gaussian distribution

3    w = 0.001 .* randn(n_in, n_out);

其中的高斯分布为均值为0,方差为1的标准高斯分布(standard normal distribution)。式中的“0+001”为控制参数量纲的因子，这样可使得参数期望 能保持在接近0的较小数值范围内。

但是，且慢！上述做法仍会带来一个问题，即网络输出数据分布的方差会随 着输人神经元个数改变(原因参见公式7.1至7.5)。为解决这一问题，会在初始 化的同时加上对方差大小的规范化，如：

7.2.随机初始化

1    % Calibrating the variances (the Xavier method)

2    w = (0.001 .* randn(n_in, n_out)) ./ sqrt(n);

其中，n为输人神经元个数nin (有时也可指定为(nin + nout)/2)。这便是著名 的“Xavier参数初始化方法” [27],实验对比发现此初始化方法的网络相比未做 方差规范化的版本有更快的收敛速率。Xavier这样初始化的原因在于维持了输 人输出数据分布方差的一致性，具体分析而言有下式：(其中假设 s 为未经非线 性变换的该层网络输出结果，^为该层参数，x为该层输人数据)

Var(s) = Var(y^ ^iXi)    (7.1)

i

n

=Var(^iXi)    (7+2)

i

n

=[E(^i)]2 Var(xi)十[E(xi)]2 Var(wi)十 Var(Xi)Var(^i)    (7+3)

i

n


i

=(nVar(^))Var(x).    (7.5)

因为输出s未经过非线性变換，故s = ^n wxi。又因x各维服从独立同 分布的假设，可得式7.1到式7.2,后由方差公式展开得式7.3。大家应该还记 得，在本章一开始，我们就提到理想情况下处于稳定状态的神经网络参数和 数据均值应为0,则式7.3中的E(Wi) = E(xi) = 0,故式7.3可简化为式7.4, 最终得到式7.5。为保证输人数据Var(s)和输出数据Var(x)方差一致，需令 nVar(w) = 1,即 n，Var(w) = n• Var(awz) = n• a• Var(wz) = 1,则 a = (1/n), 其中为方差规范化后的参数。这便是Xavier参数初始化的由来。

不过，细心的读者应该能发现Xavier方法仍有不甚完美之处，即该方法并 未考虑非线性映射函数对输人s的影响。因为使用如ReLU函数等非线性映射 函数后，输出数据的期望往往不再为0,因此Xavier方法解决的问题并不完全

符合实际情况。2015年He等人［34］14对此提出改进——将非线性映射造成的影 响考虑进参数初始化中，他们提出原本Xavier方法中方差规范化的分母应为 V（n/2）而不是vn。

文献［34］中给出了 He参数初始化方法与Xavier参数初始化方法的收敛结 果对比，如图7+1,可以看出因为考虑了 ReLU非线性映射函数的影响，He参 数初始化方法（图中红线）比Xavier参数初始化方法（图中蓝线）拥有更好的 收敛效果，尤其是在33层这种更深层的卷积网络上，Xavier方法不能收敛而 He方法可在第9轮（epoch）收敛到较好的（局部）最优解。




（b） 30层卷积神经网络上的收敛结果对比。


(a) 22 -


图7+1: Xavier参数初始化方法与He参数初始化方法对比［34］。纵轴为误差， 横轴为训练轮数。

以上是参数初始化分布服从高斯分布的情形。刚才还提到均匀分布也是一种 很好的初始化分布，当参数初始化服从均匀分布（uniform distribution），由于 分布性质的不同，均匀分布需指定其取值区间，则Xavier初始化方法和He初



% Parameterrifinitializationimfollowing

% the uniform distribution % The Xavier method

7.3.其他初始化方法

4    low = -sqrt(3/n); high = sqrt(3/n);

5    % The interval is [low,high].

e rand_param = a + (b-a) .* rand(n_in, n_out); 7 w = 0.001 .* rand_param;

1    % Parameter initialization following

2    % the uniform distribution s % The He method

4    low = -sqrt(6/n); high = sqrt(6/n);

5    % The interval is [low,high].

e rand_param = a + (b-a) .* rand(n_in, n_ou 7 w = 0.001 .* rand_param;

7.3其他初始化方法
除了直接随机初始化网络参数，一种简便易行且十分有效的方式则是利用预训

练模型(pre-trained model)-将预训练模型的参数作为新任务上模型的参

数初始化。由于预训练模型已经在原先任务(如ImageNet3、Places2。54等数据 集)上收敛到较理想的局部最优解，加上很容易获得这些预训练模型5,用此最 优解作为新任务的参数初始化无疑是一个优质首选。

另外，2016年美国加州伯克利分校和卡内基梅隆大学的研究者提出了一种 数据敏感的参数初始化方式[50]6,是一种根据自身任务数据集量身定制的参数 初始化方式，读者在进行自己训练任务时不妨尝试一下。

ImageNet 数据集：www.image-net.org/

Places205 数据集：http://places.csail.mit.edu/。

许多深度学习开源工具都提供了预训练模型的下载，具体内容请参见第14章“深度学习开源工

数据敏感的参数初始化方式”代码链接：


https://github.com/philkr/magic_init。


7.4小结
§ 网络参数初始化的优劣极大程度决定了网络的最终性能；

§比较推荐的网络初始化方式为He方法，将参数初始化为服从高斯分布或 均匀分布的较小随机数,同时对参数方差需施加规范化；

§ 借助预训练模型中参数作为新任务参数初始化的方式是一种简便易行且十 分有效的模型参数初始化方法。













* * *




# COMMENT
