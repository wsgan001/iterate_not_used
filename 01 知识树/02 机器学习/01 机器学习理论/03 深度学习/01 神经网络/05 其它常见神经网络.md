
# TODO
- <span style="color:red;">这些都可以拆开来好好学习</span>






其他常见神经网络

神经网络模型、算法繁多，本节不能详尽描述，只对特别常见的几种网络 稍作简介

# 1. RBF 网絡

<span style="color:red;">这个现在还有用吗？好处在哪里？没怎么理解？</span>

RBF (Radial Basis Function,径向基函数) 网络 [Broomhead and Lowe， 1988] 是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合.假定输入为 d 维向量x，输出为实值，则 RBF 网络可表示为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/KmBkJGC74h.png?imageslim)

其中 q 为隐层神经元个数，$c_i$ 和 $w_i$ 分别是第 i 个隐层神经元所对应的中心和权重，$\rho (x,c_i)$ 是径向基函数，这是某种沿径向对称的标量函数，通常定义为样本 x 到数据中心 $c_i$ 之间欧氏距离的单调函数.常用的高斯径向基函数形如：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/9AB5iJ3Bjj.png?imageslim)

［Park and Sandberg, 1991］证明，具有足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数.

通常采用两步过程来训练RBF网络：

1. 第一步，确定神经元中心 $c_i$ ，常用的 方式包括随机采样、聚类等;
2. 第二步，利用BP算法等来确定参数 $w_i$ 和 $\beta_i$ 。

# 2 ART网络

竞争型学习(competitive learning) 是神经网络中一种常用的无监督学习 策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称“胜者通吃” (winner-take-all)原则。<span style="color:red;">只有一个胜利的被激活？</span>


ART (Adaptive Resonance Theory,自适应谐振理论)网络［Carpenter and Grossberg, 1987］是竞争型学习的重要代表。该网络由比较层、识别层、识别阈值和重置模块构成。其中，比较层负责接收输入样本，并将其传递给识别层神经元.识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类.

在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜。获胜神经元将向其他识别层神经元发送信号，抑制其激活。若输入向量与获胜神经元所对应的代表向量之间的相 似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络 连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的 相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量.

显然，识别阈值对ART网络的性能有重要影响.当识别阈值较高时，输入样 本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比 较少、比较粗略的模式类.




ART比较好地缓解了竞争型学习中的 “可塑性-稳定性窘境” (stability-plasticity dilemma)，可塑性是指神经网络要有学习新知识的能力，而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆。这就使得ART网络具有一个很重要的优点：可进行増量学习(incremental learning) 或在线学习 (online learning).

增量学习是指在学得模型后，再接收到训练样例时，仅需根据新样例对模型进行更新，不必重新训练整个模型，并且先前学得的有效信息不会被“冲掉”；在线学习是指每获得一个新样本就进行一次模型更新。显然，在线学习 是增量学习的特例，而增量学习可视为“批模式” (batch-mode)的在线学习。

早期的 ART 网络只能处理布尔型输入数据，此后ART发展成了一个算法族，包括能处理实值输入的 ART2 网络、结合模糊处理的 FuzzyART 网络，以及可进行监督学习的 ARTMAP 网络等。<span style="color:red;">嗯，感觉还是很厉害的，要学习下。</span>

# 3. SOM网络
<span style="color:red;">没明白？</span>
SOM(Self-Organizing Map,自组织映射)网络［Kohonen，1982］是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二维)，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。<span style="color:red;">怎么做到的？</span>

如图5.11所示，SOM 网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一权向量，网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置.SOM的训练目标就是为 每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/EjDif2aL66.png?imageslim)

SOM 的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者,称为最佳匹配单元(best matching unit)。然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。


# 4. 级联相关网络

—般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。

与此不同，结构自适应网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。<span style="color:red;">嗯，nice 。</span>级联相关(Cascade-Correlation)网络[Fahlman and Lcbiere, 1990] 是结构自适应网络的重要代表.



![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/2H76FdJ5i1.png?imageslim)


级联相关网络有两个主要成分：“级联” 和 “相关”。

级联是指建立层次连接的层级结构。在开始训练时，网络只有输入层和输出层，处于最小拓扑结构；随着训练的进行，如图5.12所示，新的隐层神经元逐渐加入，从而创建起层级结构.当新的隐层神经元加入时，其输入端连接权值是冻结固定的。

相关是指通过最大化新神经元的输出与网络误差之间的相关性(correlation)来训练相关的参数.

与一般的前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目，且训练速度较快，但其在数据较小时易陷入过拟合。<span style="color:red;">这个很有意思啊，不知道现在怎么样了。</span>

# 5. Elman网络

与前馈神经网络不同，“递归神经网络”  (recurrent neural networks)允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。

这样的结构与信息反馈过程，使得网络在 t 时刻的输出状态不仅与 t 时刻的输入有关，还与 t-1 时刻的网络状态有关，从而能处理与时间有关的动态变化。<span style="color:red;">嗯，这个就是著名的 RNN。</span>

Elman网络［Elman, 1990］是最常用的递归神经网络之一，其结构如图 5.13 所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。隐层神经元通常采用 Sigmoid 激活函数，而网络的训练则常通过推广的 BP算法 进行［Pineda, 1987］.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/mfH3faje4D.png?imageslim)


# 6. Boltzmann机

神经网络中有一类模型是为网络状态定义一个 “能量” (energy)，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数. Boltzmann 机［Ackley et al.5 1985］就是一种“基于能量的模型”(energy-based model)。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/83cg18H7me.png?imageslim)

常见结构如图5.14(a)所示，其神经元分为两层：显层与隐层.显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达.Boltzmann 机中的神经元都是布尔型的，即只能取0、1两种状态，状态1表示激活，状态0表示抑制.令向量 $\mathbb{s}\in \{0,1\}^n$ 表示 n 个神经元的状态，$w_{ij}$ 表示神经元 i 与 j 之间的连接权，$\theta_i$ 表示神经元 i 的阈值，则状态向量 s 所对应的Boltzmann机能量定义为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/BBe9dBae4I.png?imageslim)

若网络中的神经元以任意不依赖于输入值的顺序进行更新，则网络最终将达到 Boltzmann 分布，此时状态向量s出现的概率将仅由其能量与所有可能状态向量的能量确定：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/mAAhKC2E6d.png?imageslim)

Boltzmann机的训练过程就是将每个训练样本视为一个状态向量，使其出现的概率尽可能大。

标准的Boltzmann机是一个全连接图，训练网络的复杂度很高，这使其难以用于解决现实任务。现实中常采用受限 Boltzmann 机(Restricted Boltzmann Machine,简称 RBM)。如图 5.14(b)所示，受限 Boltzmann 机仅保留显层与隐层之间的连接，从而将 Boltzmann 机结构由完全图简化为二部图。<span style="color:red;">没看懂</span>

受限 Boltzmann 机常用 “对比散度”  (Contrastive Divergence，简称 CD)算法［Hinton, 2010］来进行训练.假定网络中有 d 个显层神经元和 q 个隐层神经元，令 v 和 h 分别表示显层与隐层的状态向量，则由于同一层内不存在连接，有:

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/52b004k82E.png?imageslim)

CD 算法对每个训练样本 v ，先根据式(5.23)计算出隐层神经元状态的概率分布 , 然后根据这个概率分布采样得到 h ;此后，类似地根据式(5.22)从 h 产生 v'，再 从 v' 产生 h' ；连接权的更新公式为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/j959DFkeG3.png?imageslim)









# REF
1. 《机器学习》周志华
