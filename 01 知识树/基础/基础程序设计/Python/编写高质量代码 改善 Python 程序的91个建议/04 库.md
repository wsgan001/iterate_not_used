### 库

Python具有丰富的库，掌握所有的库的用法和使用基本上是不可能的，更好的方法是掌 握一些常见库的使用和注意事项，对于使用较少的库可以在具体应用时参考其文朽以及使用 范例。本章主要讨论一些常用库的使用和技巧。

##### 旨议36:掌握字符串的基本用法

无名氏说：编程有两件事，一件是处理数值，另一件是处理字符串。要我说，对于商业 应用编程来说，处理字符串的代码可能超过八成，所以掌握字符串的基本用法尤其重要。通 过Python教程，读者已经掌握了基本的字符串字面M语法，比如u、r前缀等，但对于怎么 更好地编写多行的字符串字面量，仍然有个小技巧值得向大家推介。

»> s = ( 'SELECT * •

…    *FROM atable *

…    'WHERE afield=MvalueM ')

\>» s

'SELECT * FROM atable WHERE afield="value"'

这就是利用Python遇到未闭合的小括号时会自动将多行代码拼接为一行和把相邻的两 个字符串字面量拼接在一起的特性做到的。相比使用3个连续的单(双)引号，这种方式不 会把换行符和前导空格也当作字符串的一部分，则更加符合用户的思维习惯。

除了这个小技巧，也许你已经听说过Python中的字符串其实宥str和Unicode两种。是 的，的确如此，虽然在Python 3中已经简化为一种，但如果你还在编写运行在Python 2上的 程序，当需要判断变量是否为字符串时，需要注意了。判断一个变量s是不是字符串应使用

isinstance(s,basestring),注意这里的参数是 basestring 而不是 str。

〉>> a = Mhin

\>>> isinstance(az str)    .........①

True

»> b =u"Hi"

〉>> isinstance(b, str)    .........②

False

\>>> isinstance(b,basestring)

True

\>>> isinstance(b,Unicode)

True

\>>> isinstance(a,Unicode)    .........③

False

»>

如标注①所示：isinStance(a，Str)用于判断一个字符串是不是普通字符串，也就是说其类 型是否为str;因此当被判断的字符串为Unicode的时候，返回False,如标注②所示。同样， 标注③中isinstance(a,Unicode)用来判断一个字符串是不是Unicode。因此要正确判断一个变 量是不是字符串，成该使用isinstance(s，basestring)，因为basestring才是是str和Unicode的 基类，包含了普通字符串和Unicode类型o

接下来正式开始学习字符串的基本用法。与其他书籍、手册不同，我们将通过性质判 定、查找替换、分切与连接、变形、填空与删减等5个方面来学习。首先是性质判定，str对 象有以下几个方法:isalnum()、isalpha()、isdigitO、islower()、isupper()、isspace()、istitle()、 startswith(prefix[，start[, end]])、endswith(suffix[，start[, end]]),前面几个 is*()形式的闲数很简 单，顾名思义无非是判定是否数字、字母、大小写、空白符之类的，istideO作为东方人用得 少些，它是判定字符串是否每个单词都有且只有第一个字母是大写的。

»> assert *Hello World! 1 . istitle () M True

»> assert •HEllo World! 1 . istitle () == False

相对于is*()这些“小儿科”来说，窬要注意的是*with()函数族可以接受可选的start、 end参数，善加利用，可以优化性能。另外，自Python 2.5版本起，*with()函数族的prefix 参数可以接受tuple类型的实参，当实参中的某个元素能够匹配时，即返回True。

接下来是査找与替换，count( sub[, start[, end]])、fmd( sub[, start[, end]])、index( sub[, start[, end]])、rfind( sub[, start[，end]])、rindex( sub[, start[, end]])这些方法都接受 start, end参数，善加利用，可以优化性能。其中count()能够査找子串sub在字符串中出 现的次数，这个数值在调用replace方法的时候用得着。此外，需要注意find()和index()方 法的不同：find()函数族找不到时返回-1, index()函数族则抛出ValueError异常。但对于判 定是否包含子串的判定并不推荐调用这些方法，而是推荐使用in和not in操作符。

\>» str = "Test if a string contains some special substrings1’

\>» if str .find ("some") !- -1: # 使用 find 方法进行判断

...    print "Yes, it contains•’

• • •

Yesr it contains

»> if "some" in str: #使用in方法也可以判断 ...    print '•Yes, it contains using in"

• • •

Yes,it contains using in

replace(old，new[，⑶unt])用以替换字符串的某些子串，如果指定count参数的话，就最多 替换count次，如果不指定，就全部替换(跟其他语言不太一样，要注意了)。

然后要掌握字符串的分切与连接，关于连接，会有一节专门进行讲述，在这里，专讲分切。 partition(sep)、rpartition(sep)、splitlines([keepends])、split([sep [，maxsplit]])、rsplit([sep[，maxsplit]])， 别看这些方法好像很多，其实只要弄清楚partitkmO和split()就可以了。*partitiOn()函数族是 2.5版木新增的方法，它接受一个字符串参数，并返回一个3个元素的元组对象。如果sep没 出现在母串巾，返冋值是(sep，”,”)；否则，返回值的第一个元素是sep左端的部分，第二个 元素是sep自身，第三个元岽是sep右端的部分。而split()的参数maxsplit是分切的次数，即 最大的分切次数，所以返回值蕺多有maxsplit+1个元索。但split()有不少小陷阱，需要注意， 比如对于字符串s、s.split()和s.Split(”)的返回值是不相同的。

\>>> 1 hello world!1.split()

[•hello., fworld!•]

\>>> 1 hello world!*.split (* •)

[•、•，，.hello.,    ’world!.】

产生差异的原因在于：当忽略sep参数或sep参数为None时与明确给sep赋予字符串值 时，split()采用两种不同的算法。对于前者，split()先去除字符串两端的空白符，然后以任意 长度的空内符串作为界定符分切字符串(即连续的空白符串被当作单一的空白符看待)；对于 后者则认为两个连续的sep之间存在一个空字符串。因此对于空字符串(或空白符串)，它们 的返回值也是不同的。

\>>> ".split()

[]

»> • • .split (• •)

[• • J

掌握了 splito，可以说字符串最大的陷阱已经跨过去了。下面是关于变形的内容。 lower()、upper()、capitalize。、swapcase()、title()这些无非是大小写切换的小事，不过需要 注意的是titile()的功能是将每一个单词的首字母大写，并将单词中的非首字母转换为小写 (英文文章的标题通常是这种格式)。

\>» * hello wORld! * .title

1 Hello World!*

因为title()函数并不去除字符串两端的空白符也不会把连续的空白符替换为一个空格，

所以不能把titleO理解先以空白符分切字符串，然后调用capitalizeO处理每个字词以使其 首字母大写，再用空格将它们连接在一起。如果你有这样的需求，建议使用string模块屮的 CapwOrds(S)函数，它能够去除两端的空白符，再将连续的空白符用一个空格代替。

»> * hello world! * . title ()

•    Hello World!1

»> string.capwords (1 hello world! 1)

\*    Hello World!*

看，它们的结果是不相同的！最后，是删减与填充。删减在文本处理是很常用，我们 常常得把字符串描头去尾，就用得上它们。如果strip([chars])、lstrip([chars])、rstrip([chars]) 中的chars参数没有指定，就是删除空白符，空白符由string.whitespace常量定义。填充则 常用于字符串的输出，借助它们能够排出漂亮的版面。center(width[, fillchar]). ljust(width [,fillchar]), rjust(width[, fillchar])、zfill(width)、expandtabs([tabsize】)，看，有了它们，居中、 左对齐、右对齐什么的完全不在话下，这些方法中的fillchar参数是指用以填充的字符，默认 是空格。而中的z是指zero,所以顾名思义，即是以字符0进行填充，在输出数 值时比较常用。expandtabsO的tabsize参数默认为8,它的功能是把字符串中的制表符(tab) 转换为适当数量的空格。

##### 建议37:按需选择sort()或者sorted()

各种排序算法以及它们的时间复杂度分析是很多企业面试人员在面试时候经常会问到的 问题，这也不难理解，在实际的应用过程中确实会遇到各种需要排序的情况，如按照字母表 输出一个序列、对记录的多个字段排序等。还好，Python中的排序相对简申-，常用的函数有 sort()和sortedO两种。这两种函数并不完全相同，各有各的用武之地。我们来具体分析一下。

1)相比于sort()，sortedO使用的范围更为广泛，两者的函数形式分别如下：

sorted(iterable[z cmp[, key[, reverse]】】> s.sort(【cmp[, key[r reverse]J])

这两个方法有以下3个共同的参数：

□    cmp为用户定义的任何比较函数，函数的参数为两个可比较的元索(来自iterable或 者list)，函数根据第一个参数与第二个参数的关系依次返回-1、0或者+1 (第一个参 数小于第二个参数则返回负数)。该参数默认值为None。

□    key是带一个参数的函数，用来为每个元素提取比较值，默认为None (即直接比较毎 个元素)。

□    reverse表示排序结果是否反转。

e

m

aJ n ) • 3 { 2



Jon•r 1 age *: 32}, {•name1: 1 Alan 1,    1 age1: 50}, {•name•:

»> sorted (persons, key=lambda x: (x [ *name ' ] / - xt’age1】))

[{•age•: 50, ’name•: ’Alan1}, (1 age *: 23, •name*: 1 Bob 1}, {*age *: 32, •name’： * J on. H

从函数的定义形式可以看出，sortedO作用于任意可迭代的对象，而sortO—般作用于列 表。因此下面的例子中针对元组使用sort()方法会抛出AttributeError,而使用sortedO函数则 没有这个问题。

\>» a = (1,2,4,2,3)

\>〉> a.sort()

Traceback (most recent call last):

File "<stdin>”， line 1, in <module>

AttributeError: 1 tuple * object has no attribute r sort'

»> sorted (a)

[1，2, 2, 3, 4]

2)当排序对象为列表的时候两者适合的场景不同。sortedO函数是在Python2.4版本中 引人的，在这之前只有sort()函数。sortedO函数会返回一个排序后的列表，原有列表保持不 变；而Sort()函数会直接修改原有列表，函数返回为None。来看下面的例子：

| >>>  |                | •n1]  |
| ---- | -------------- | ----- |
| »>   | sorted(a)      |       |
| [1,  | 3, 7, T,       | •n1]  |
| >>>  | a              |       |
| [■1. | r 1/ y, 3, 7,  | •n门  |
| >>>  | print a.sort() |       |
| None |                |       |
| >>>  | a              |       |
| [1,  | 3, 7, T,       | •nf】 |

\>»

###### 因此如果实际应用过程中需要保留原有列表，使用sortedO函数较为适合，否则可以选 择SOrt()函数，因为Sort()函数不需要复制原有列表，消耗的内存较少，效率也较髙。

3 )无论是sort()还是sorted()函数，传人参数key比传人参数cmp效率要高。cmp传人 的函数在整个排序过程中会调用多次，函数幵销较大；而key针对每个元素仅作一次处理, 因此使用key比使用cmp效率要髙。下面的测试例子显示使用key比cmp约快50%。

\>» from timeit import Timer

\>>> Timer(stmt="sorted(xs,key=lambda x:x[1】)"，setup=Mxs=range(100);xs=zip(xs,xs)

;••) .timeit (10000)

0.2900448249509081

»>

\>>> Timer(stmt="sorted(xs,cmp=lambda a,b: cmp(a[1],b[1])) ••,setup="xs=range(100); xs=zip(xS/xs);M) .timeit (10000)

0.47374972749250155

\>»

###### 4) sortedO函数功能非常强大，使用它可以方便地针对不同的数据结构进行排序，从而

###### 满足不同需求。来看下列例子。

□对字典进行排序：下面的例子中根据字典的值进行排序，即将phonebook对应的电话 号码按照数字大小进行排序。

\>>> phonebook = {* Linda 1: 177501, 1 Bob 1: 193451z * Carol1: 15834•}

\>>> from operator import itemgetter

\>>> sorted_pb = sorted(phonebook.iteritems()r key=itemgetter(1))

\>» print sorted一pb

【(•Carol., 15834 •)r (1 Linda1r l7750,)r f Bob 1r 193451)]

\>>>

□多维list排序：实际情况下也会碰到需要对多个字段进行排序的情况，如根据学生的 成绩、对应的等级依次排序。当然这在DB里面用SQL语句很容易做到，但使用多 维列表联合sorted()函数也可以轻易达到类似的效果。

\>>> from operator import itemgetter

»> gameresult - [[ .Bob., 95.00, .A.】,[.Alan., 86.0r •(：•],['Mandy’f 82.5, .A1 】,[.Rob\ 86, fEf ] ] #分别表示学生的姓名，成绩，等级

\>>> sorted(gameresult , key=operator.itemgetter(2, 1))

[['Mandy* ,    82.5,    .A，], [.Bob\ 95.0,    W], [-Alan1 , 86.0,    •<：•】，[’Rob、86,

•E’] #当第二个字段成绩相同的时候按照等级从低到离排序

]

□字典中混合list排序：如果字典中的key或者值为列表，需要对列表中的某一个位置 的元素排序也是可以做到的。下面的例子中针对字典mydict的value结构[n，m]中的 n按照从小到大的顺序排列。

\>» mydict = { ’Li •:

•    • .    1 Zhang* : [，E、2】，

•••    •Wang1: [，Pf,3],

»>

»> from operator import itemgetter

»> sorted(mydict.iteritems(), key=lambda (k,v): operator.itemgetter(1)(v))

[(fZhang\ CE1, 2]), (f Du \2】)，(•Wang1, (fP\ 3]), CLi1, CMf, 7]), (• Zhe\ CH1, 7]), (fMa\ [fC\ 9])]

□ List中混合字典排序：如果列表中的每一个元索为字典形式，需要针对字典的多个 key值进行排序也不难实现。下面的例子是针对list中的字典元素按照rating和name 进行排序的实现方法。

\>>> gameresult = [ { ’•name" : "Bob", "wins": 10, ’.losses" : 3, "rating” ： 75.00 },

{ "name”："David”， "wins":3, "losses":5, ”rating":57•00 },

…    { "name":"Carol", "wins":4, "losses":5, "rating":57•00

•    ••    { ’’name": "Patty", "wins” ：9, "losses”： 3, "rating••: 71.48 }]

»> from operator import itemgetter

»> sorted(gameresult , key=operator.itemgetter("rating","name"))

[{•wins’： 4, 1 losses 1: 5, 1 name1: 1 Carol1, •rating1: 57.0}, {1 wins': 3, * losses 1 :5r •name *: 1 David1, 1 rating1: 57.0}, {* wins *: 9, ’losses•: 3, •name•: 1 Patty*,

• rating 1: 71.48}, {’wins■: 10, •losses•: 3, ’name1: 'Bob1, 'rating1: 75.0}]

»>

##### b义38:使用copy模块深拷贝对象

###### 在正式讨论本节内容之前我们先来了解一下浅拷贝和深拷贝的概念：

□浅拷贝(shallow copy)：构造一个新的复合对象并将从原对象中发现的引用插人该对象 中。浅拷贝的实现方式有多种，如工厂函数、切片操作、copy模块中的copy操作等。

□深拷贝(deepcopy):也构造一个新的复合对象，但是遇到引用会继续递归拷贝其所指 向的具体内容，也就是说它会针对引用所指向的对象继续执行拷贝，因此产生的对象 不受其他引用对象操作的影响。深拷贝的实现需要依赖copy模块的deepcopyO操作。 下面我们通过一段简单的程序来说明浅拷贝和深拷贝之间的区别。

import copy

class Pizza(object):

def —init_(selfzname,sizerprice):

self.name-name self.size=size self.price=price

def getPizzalnfo (self):    ①获取 Pizza 相关信息

return self.name,self•size,self.price

def showPizzalnfo (self):    ②里示 Pizza 信息

print "Pizza name:H+self.name print "Pizza size:" + str (self.size) print MPizza price:w+str(self.price)

def changeSize (self,size):

self.size=size

def changePrice(self,price):

self.price=price

class Order (object):    ③订单类

def _init_(selfrname):

self.customername=name self.pizzaList«(]

self.pizzaList.append(Pizza("Mushroom”，12,30))

def ordermore(selfzpizza):

self.pizzaList.append(pizza)

def changeName(self,name):

self.customername=name

def getorderdetail (self):

print "customer name:"+self.customername for i in self.pizzaList:

i.showPizzaInfo()

def getPizza(self,number}:

return self.pizzaList[number]

customerl=Order ("zhang’.}

customerl.ordermore(Pizza("seafood", 9r 40)) customer1.ordermore(Pizza("fruit", 12,35” print "customerl order infomation:n customerl.getorderdetail()

print ••-------------------------------••

###### 程序描述的是客户在Pizza店里下了一个订单，并将具体的订单信息打印出来的场景。 运行输出结果如下：

customer name:zhang Pizza name:Mushroom Pizza size:12 Pizza price:30 Pizza name:seafood Pizza size:9 Pizza price:40 Pizza name:fruit Pizza size:12 Pizza price:35

###### 假设现在客户2也想下一个跟客户1一样的订单，只是要将预定的水果披萨的尺寸和价 格进行相应的修改。于是服务员拷贝了客户1的订单信息并做了一定的修改，代码如下：

customer2=copy.copy(customerl)

print "order 2 customer name:"+customer2•customername

customer2.changeName("li")

customer2.getPizza(2).changeSize(9)

customer2.getPizza(2).changePrice(30)

print ••customer2 order infomation:"

customer2.getorderdetail()

print w-------------------------------------w

###### 上面这段程序的输出也没有什么问题，完全满足了客户2的需求。输出结果如下所示:

order 2 customer name:zhang customer2 order infomation: customer name:li

Pizza name:Mushroom

Pizza size:12

Pizza price:30

Pizza name:seafood

Pizza size:9

Pizza price:40 Pizza name:fruit Pizza size:9 Pizza price:30

###### 在修改完客户2的订单信息之后，现在我们再来检査一下客户1的订单信息：

print "customerl order information:M

customerl.getorderdetail ()

###### 你会发现客户1的订单内容除了客户姓名外，其他的居然和客户2的订单具体内容一样了。

customerl order infomation:

customer name:zhang

Pizza name:Mushroom

Pizza size:12

Pizza price:30

Pizza name:seafood

Pizza size:9

Pizza price:40

Pizza name:fruit

Pizza size:9

Pizza price:30

这是怎么回事呢？客户1根本没要求修改订单的内容，这样的结果必定会直接影响到客 户满意度。问题出现在哪里？这是我们木节要重点讨论的内容。我们先来分析客户1和客户 2订单内容的关系图，如图4-1所示。

图4-1客户1和客户2订单的关系示意图

customerl中的pizzaList是一个由Pizza对象组成的列表，其中存放的实际是对一个个 具体Pizza对象的引用，在内存中就是一个具体的地址，可以通过査看id得到相关信息。

出出出出

辕餘输埔



14099440

14101392

14115344

13914800



print id(customerl.pizzaList(0]) print id(customerl.pizzaList[1]) print id(customerl.pizzaList[2]) print id(customerl.pizzaList)

customer2的订单通过copy.copy(customerl)获得，通过id闲数査看customer2中 pizzaList的具体Pizza对象你会发现它们和customerl中的输出是一样的(读者可以自行验 证)。这是由于通过copy.copyO得到的customer2是customerl的一个浅拷贝，它仅仅拷贝了 pizzalist里面对象的地址而不对对应地址所指向的具体内容(即具体的pizza)进行拷贝，因 此customer2中的pizzaList所指向的具体内容是和customerl中一样的，如图4-1所示。所 以对pizza fruit的修改直接影响了 customerl的订单内容。实际上在包含引用的数据结构中， 浅拷贝并不能进行彻底的拷贝，当存在列表、字典等不可变对象的时候，它仅仅拷贝其引用 地址。要解决上述问题需要用到深拷贝，深拷贝不仅拷贝引用也拷贝引用所指向的对象，因 此深拷贝得到的对象和原对象是相互独立的。

上面的例子充分展示了浅拷贝和深拷贝之间的差异，在实际应用中要特别注意这两者之 间的区别。实际上Python copy模块提供了与浅拷贝和深拷贝对应的两种方法的实现，通过 名字便可以轻易进行区分，模块在拷贝出现异常的时候会抛出copy.erroro

copy.copy(x) ： Return a shallow copy of x.

copy. deepcopy (x) : Return a deep copy of x.

exception copy .error ： Raised for module specific errors.

实际上，上面的程序应该将customer2=copy.c叩y(customerl)改为copy.deepcopy()来灾 现(请读者自行验证)。关于对象拷贝读者可以査看网页http://en.wikipedia.org/wiki/Object_ copy进行阅读扩展。

##### 塞议39:使用Counter进行计数统计

计数统计相信大家都不陌生，简单地说就是统计某一项出现的次数。实际应用中很多需 求都需要用到这个模型，如检测样本中某一值出现的次数、日志分析某一消息出现的频率、 分析文件中相同字符串出现的概率等。这种类似的需求有很多种实现方法。我们逐一来看一 下使用不同数据结构时的实现方式。

1 )使用 diet。

»> some一data = [ • a •, • 2 •, 2, 4 f 5, • 2 \ • b\4,7, • a •, 5, • d •, • a \ • z •] >>〉count_frq = diet()

\>>> for item in some_data:

"• if item in count frq:

...    count_frq[itemj +=1

... else:

•    "    count_f rq [ item J = 1

###### 馨 •鲁

\>>> print count frq

{-a*: 3, 2: 1, •b*: 1, 4: 2r 5: 2r 7: 1, f2•: 2, •z.: 1, •d1: 1}

###### 2 )使用 defaultdicto

»> from collections import defaultdict

»> some—data =【• a \ • 2 •' 2, 4 f 5, • 2 • f • b •, 4,7f • a •, 5,    \ • a •, • z •】

»> count_frq = defaultdict (int)

»> for item in some_data:

... count_frq[item] += 1

•    • •

»> print count_frq

defaultdict(<type •int,>,    { 1 a1 : 3, 2: 1, ,b,: 1, 4: 2, 5: 2, 7: 1,    ，2•: 2t 1z

lr fd•: 1})

###### 3)使用 set 和 list。

»> some—data = [ .a • f • 2 •, 2, 4'5, • 2 \ • b •' 4'7, • a •' 5f .d \ • a、• z •】

»> count_set = set (some_data)

\>» count_list =[]

\>» for item in count^set:

•••    count_list•append({item,some_data.count(item)))

###### • •壽

\>» print count_list

[(•a\ 3)，(2, 1),    1), (4, 2), <5r 2), (7, 1), (.2、2), Cz\ 1), CdS 1)]

###### 上面的方法都比较简单，但有没有更优雅、更Pythonic的解决方法呢？答案是使用 collections.Countero

\>>> from collections import Counter

\>» some—data - [ • a •，• 2 \ 2,4，5, • 2 曹，.b’，4,7, .a、5, •d*，• a \ • z ’ 】

\>>> print Counter(some_data)

Counter((•a*: 3, 4: 2, 5: 2, '2.: 2, 2: 1, •b': 1, 7: 1, •z1: 1, :d•: 1})

###### Counter类是自Python2.7起增加的，属于字典类的子类，是一个容器对象，主要用来统 计散列对象，支持集合操作+、-、&、|,其中&和|操作分别返回两个Counter对象各元素 的最小值和最大值。它提供了3种不同的方式来初始化：

Counter ("success")    # 可迭代对象

Counter (s=3, c=2, e=l, u=l)    # 关域字参数

Counter ({ "s" : 3, "c" : 2, ”u" : l,"e" : 1})    # 字典

###### 可以使用elements()方法来获取Counter中的key值。

»> list (Counter (some_data) .elements ())

I fa\    2r    4, 4, 5, 5, 7,    -d* J

利用most_common()方法可以找出前N个出现频率最高的元素以及它们对应的次数。

»> Counter (some_data) .most^common (2)

[(.a., 3),    (4, 2)]

当访问不存在的元素时，默认返回为0而不是抛出KeyError异常。

\>>> (Counter(some一data))[•y’]

0

update()方法用于被统计对象元素的更新，原有Counter计数器对象与新增元素的统计 计数值相加而不是直接替换它们。

subtract()方法用于实现计数器对象中元素统计值相减，输入和输出的统计值允许为0或 者负数。

\>>> c = Counter ("success")

\>>> c.update("successfully">

»> c

Counter({* s *: 6f *c•: 4f *u•: 3, »> c. subtract (H success full yM) >» c

Counter ({* s1: 3, fc1: 2, *e1: 1,



IfCounter ({ • s •: 3, •c1 : 2f • e • : 1, •u1 iPs.: 3, .cf: 2, T: 2,    2' 'e1: lr "f1

\#s的值为变为6,为上面s中对应值的和 •e.: 2,    •I1: 2r : lr •y1: 1})



•u1: lr *f1: 0,    0r •y*: 0})



1，)

V: 1



##### 建议40:深入掌握ConfigParser

儿乎所有的应用程序真正运行起来的时候，都会读取一个或几个配置文件。配®文件 的意义在于用户不需要修改代码，就可以改变应用程序的行为，让它更好地为应用服务。比 如pylint就带有一个参数-rcfile用以指定配置文件，实现对自定义代码风格的检测。常见 的配S文件格式有XML和ini等，其中在MS Windows系统上，ini文件格式用得尤其多， 甚至操作系统的API也都提供了相关的接口函数来支持它。类似ini的文件格式，在Lmux 等操作系统中也是极常用的，比如pylint的配置文件就是这个格式。但凡这种常用的东西， Python都有个标准库来支持它，也就是ConfigParser。

ConfigParser的基本用法通过手册可以掌握，但是仍然有几个知识点值得在这里跟大家 说一下。首先就是getboolean()这个函数。getboolean()根据一定的规则将配置项的值转换为 布尔值，如以下的配置：

[sectionl] optionl=0

当调用 getboolean('section 1’optioinl.)时，将返回 False。不过 getboolean()的真值规则 值得一说：除广0之外，no、false和off都会被转义为False，而对应的丨、yes、true和on 则都被转义为True，其他值都会导致抛出ValueError异常。这样的设计非常贴心，使得我们 能够在不同的场合使用yes/no、true/false, on/off等更切合自然语言语法的词汇，提升配置文

件的可维护性。

除了 getboolean()之外，还需要注意的是配置项的査找规则。首先，在ConfigParser 支持的配置文件格式里，有一个PEFAULT]节，当读取的配置项在不在指定的节里时， ConfigParser将会到[DEFAULT]节中査找。举个例子，有如下配H文件：

$ cat example.conf

[DEFAULT]

in_default = 1 an option value in default *

(sectionl]

简单地编写一段程序尝试通过sectionl获取in default的值。

$ cat readini.py

import ConfigParser

conf = ConfigParser .ConfigParser ()

conf.read(1 example.conf 1)

print conf.get(•sectionl•,    •in_default•)

运行结果如下：

$ python readini.py

•an option value in default *

可见ConfigParser的行为跟上文描述是一致的。不过，除此之外，还有一些机制导致 项0对配置项的査找更复杂，这就是class ConfigParser构造函数中的defaults形参以及其 get(section, option[, raw[, vars]])中的全名参数vars。如果把这些机制全部用上，那么配置项 值的査找规则如下：

1 )如果找不到节名，就抛出NoSectionError。

2)    如果给定的配置项出现在get()方法的vars参数中，则返回vars参数中的值。

3)    如果在指定的节中含有给定的配置项，则返回其值。

4)    如果在[DEFAULT]中有指定的配置项，则返回其值。

5)    如果在构造函数的defaults参数中有指定的配置项，则返回其值。

6 )抛出 NoOptionError。

因为篇幅所限，这里就不提供示例了，大家可以自行构造相应的例子来验证。接下来要 讲的是第三个特点。大家知道，在Python中字符串格式化可以使用以下语法：

\>» • % (protocol) s ://% (server) s: % (port) s/f % { 1 protocol1: • http * / 1 server *:f example. com1 / •port.:1080}

1 http "/example • com: 1080"

其实ConfigParser支持类似的用法，所以在配置文件中可以使用。如，有如下配tt选项：

$ cat format.conf

[DEFAULT]

conn str = % (dbn) s://% (user) s : % (pw) s@% (host) s : % (port) s/% (db) s

dbn = mysql user — root host=localhost port = 3306

[dbl]

user = aaa

pw=ppp

db-example

[db2]

host=192.168.0.110

pw=www

db=example

这是一个很常见的SQLAlchemy应用程序的配置文件，通过这个配置文件能够获取不同 的数据库配置相应的连接字符串，即conn_str。如你所见，COnn_str定义在[DEFAULT]中, 但当它通过不同的节名来获取格式化后的值时，根据不同配置，得到不同的值。先来看以下 代码：

$ cat readformatini.py

import ConiigParser

conf = ConfigParser . ConfigParser ()

conf.read(1 format.conf1)

print conf.get(’dbl•, *conn_str*)

print conf .get (*cib2 f #    1 conn_str1)

非常简单的代码，就是通过ConfigParser获取dbl和db2两个节的配置。然后看如下输出：

$ python readformatini.py

mysql://aaa : ppp@localhost: 3306/example

mysql://root: [www@192.168.0.110](mailto:www@192.168.0.110): 3306/example

可以看到，当通过不同的节名调用get()方法时，格式化cOnn_str的参数是不同的，这 个规则跟上述查找配置项的规则相同。

##### ■议41:使用argparse处理命令行参数

尽管应用程序通常能够通过配置文件在不修改代码的情况下改变行为，但提供灵活易用 的命令行参数依然非常有意义，比如：减轻用户的学成本，通常命令行参数的用法只需要 在应用程序名后面加-help参数就能获得，而配置文件的配S方法通常需要通读手册才能掌 握；同一个运行环境中有多个配置文件存在，那么需要通过命令行参数指定当前使用哪一个 配置文件，如pylint的-rcfile参数就是做这个事的。

为了做好命令行处理这件事，Pythonista尝试好几个方案，标准库中留下的getopt、 optparse和argparse就是证明。其中getopt是类似UNIX系统中get叩t()这个C函数的实现， 可以处理长短配置项和参数。如有命令行参数4-1)-0负0-(16&1^132,在处理之后的结果是 两个列表，其中一个是配置项列表［c-a’，••)，c-b.，••)，C-C’，Too*), c-d’，W)］，毎一个元素都由配 置项名和其值(默认为空字符串)组成；另一个是参数列表［*ar/a2*］,每一个元素都是一个 参数值。getopt的问题在于两点，一个是长短配置项需要分开处理，二是对非法参数和必填 参数的处理需要手动。如：

try:

opts, args = getopt.getopt(sys.argv［1:］, nho:v", 「’help”， "output-"］｝ except getopt.GetoptError as err:

print str (err) # 此处瑜出类似"option -a not recognized"的出错信息 usage () sys.exit (2)

output = None verbose = False for o, a in opts:

if o == w-v••:

verbose = True

elif o in    "--help”)：

usage () sys.exit ()

elif o in (••--output"): output = a

else:

assert False, "unhandled option”

从for循环处可以看到，这种处理非常原始和不便，而从getopt.getopt(sys.argv［ 1 :］， ••ho:v"，［"help"，"output=”］)函数调用时的"ho:v"和［"help", "output="］两个实参可以看出，要 编写和维护还是比较困难的，所以optparse就登场了。optparse比getopt要更加方便、强劲， 与C风格的getopt不同，它采用的是声明式风格，此外，它还能够自动生成应用程序的帮助 信息。下面是一个例子：

from optparse import OptionParser parser = OptionParser()

parser. add_option ("-f    ••--file", dest=wfilenameM,

help=nwrite report to FILE", metavar=wFILE")

parser.add option (n-q,\ •’--quiet”，

action="store false", dest="verbose”， default=True, help爾"don1t print status messages to stdout")

(options, args) = parser.parse args()

可以看到add option()方法非常强大，同时支持长短配置项，还有默认值、帮助信息 等，简单的几行代码，可以支持非常丰富的命令行接口。如，以下几个都是合法的应用程序 调用：

<yourscript>

<yourscript>

<yourscript>

<yourscript>



-f outfile --quiet 一一quiet ——file outfile -q -foutfile

-qfoutfile

除此之外，虽然没有声明帮助参数，但默认给加上了-h或--help支持，通过这两个参数 调用应用程序，可以看到自动生成的帮助信息。

Usage: <yourscript> [options]

Options:

-h,——help    show this help message and exit

-f FILE, --file=FILE write report to FILE

-qz ——quiet    don•t print status messages to stdout

不过optparse虽然很好，但是后来出现的argparse在继承了它声明式风格的优点之外， 又多了更丰富的功能，所以现阶段最好用的参数处理标准库是argparse，使叩tparse成为了 一个被弃用的库。

因为argparse自optparse脱胎而来，所以用法倒也大致相同，都是先生成一个parser实 例，然后增加参数声明。如上文中get叩t的那个例子，可以用其改造为如下形式：

import argparse

parser = argparse.ArgumentParser() parser.add^argument(•-o•,    •--output *)

parser.add argument(•-v* # dest=•verbose•, action=•store true1) args = parser.parseargs()

可以看到，代码大大地减化了，代码更少，bug更少。与optparse中的add option()类 似，add_argument()方法用以增加一个参数声明。与add_option()相比，它有儿个方面的改进， 其中之一就是支持类型增多，而且语法更加直观。表现在type参数的值不再是一个字符串， 而是一个可调用对象，比如在add_option()调用时是type="int"，而在add_argument()调用时 直接写type=int就可以了。除了支持常规的int/float等基本数值类型外，argparse还支持文件 类型，只要参数合法，程序就能够使用相应的文件描述符。如：

\>>> parser - argparse.ArgumentParser()

\>» parser. add^argument (•bar、type=argparse. FileType (1 w*))

〉>> parser.parse_args ([’out.txt•])

Namespace(bar=<open file ’out.txt’， mode ’w* at Ox...>)

另外，扩展类型也变得更加容易，任何可调用对象，比如函数，都可以作为type的实 参。与type类似，choices参数也支持更多的类型，而不是像add_option那样只有字符串。 比如下面这句代码是合法的：

parser.add_argument(1 door 1, type=int, choices=range (1, 4"

此外，add_argument()提供了对必填参数的支持，只要把required参数没置为True传递 进去，当缺失这一参数时，argparse就会自动退出程序，并提示用户。

如果仅仅是add_argument()比add_option()更加强大一点，并不足以让它把optparse踢 出标准库，ArgumentParser还支持参数分组。add_argument_group()可以在输出帮助信息时更

加清晰，这在用法复杂的CLI应用程序中非常有帮助，比如setuptools配套的semp.py文件, 如果运行python setup.py help可以看到它的参数是分组的。下面是一个简单的示例：

add_help=False) groupl description 1)

group2 description•)



\>>> parser = argparse.ArgumentParser(prog=1 PROG1, »> groupl = parser.add_argument_group(•groupl \ >>> groupl.addargument(1foo'r help=•foo help” >>> group2 = parser.add_argument_group(fgroup2•z >>> group2.add_argument(1bar1, help=,bar help1) >>> parser.print_help ()

usage: PROG [--bar BAR] foo

groupl:

groupl description foo foo help

group2:

group2 description bar BAR bar help

如果仅仅是更加漂亮的帮助信息输出不够吸引你，那么add mutually exclusive group (required=False)就非常实用：它确保组中的参数至少有一个或者只有一个(required=True)。

argparse 也支持子命令，比如 pip 就有 instaH/uninstall/freeze/list/show 等子命令， 这些子命令又接受不同的参数，使用ArgumentParser.add_subparsers()就可以实现类似的 功能。

»> import argparse

»> parser = argparse.ArgumentParser(prog=•PROG*)

»> subparsers = parser•add_subparsers(help=1 sub-command help’)

»> parser a = subparsers.add_parser(•a•, help=,a help1)

»> parser a.add_argument(1——bar•, type=int, help= *bar help1)

»> parser .par se_args ([ 'a* #    •麵祕bar1 r ' 11 ])

Namespace(bar=l)

看，就是这么简单！除了参数处理之外，当出现非法参数时，用户还需要做一些处理， 处理完成后，-•般是输出提示信息并退出应用程序。ArgumentParser提供了两个方法函数， 分别是 exit(status=O, message=None)和 error(message),可以省了 import sys 再调用 sys.exit() 的步骤。

虽然argparse已经非常好用，但是上进的Pythonista并没有止步，所以他们发明了 docopt,可以认为，它是比argparse更先进更易用的命令行参数处理器。它甚至 不需要编写代码，只要编写类似argparse输出的帮助信息即可。这是因为它根据 常见的帮助信息定义了一套领域特定语言(DSL),通过这个DSL Parser参数生成 处理命令行参数的代码，从而实现对命令行参数的解释。因为docopt现在还不是 标准库，所以在此不多介绍，有兴趣的读者可以自行去其官网(<http://docopt.org/>) 学习。

##### 建议42:使用pandas处理大型CSV文件

CSV ( Comma Separated Values)作为一种逗号分隔型值的纯文本格式文件，在实际应 用中经常用到，如数据库数据的导入导出、数据分析中记录的存储等。因此很多语言都提供 了对CSV文件处理的模块，Python也不例外，其模块csv提供了一系列与CSV处理相关的 API。我们先来看一下其中几个常见的API:

1 ) reader(csvfile[, dialect=*excer][, fmtparam]),主要用于 CSV 文件的读取，返回一个 reader对象用于在CSV文件内容上进行行迭代。

参数csvfile,需要是支持迭代(Iterator)的对象，通常对文件(file)对象或者列表(list) 对象都是适用的，并且毎次调用next()方法的返回值是字符串(string);参数dialect的默认 值为excel，与excel兼容；fmtparam是一系列参数列表，主要用于需要覆盖默认的Dialect 设置的情形。当dialect设置为excel的时候，默认Dialect的值如下：

class excel(Dialect): delimiter - ', * quotechar - • doublequote = True skipinitialspace = False lineterminator = *\r\n• quoting = QUOTE^MINIMAL



\#单个字符，用于分隔字段

\#用于对特殊符号加引号，常见的引号为"

\#用于控制quotechar符号出现的时候的表现形式 #设麗为true的时候delimiter后面的空格将会忽略 #行结束符

\#是否在字段前加引号.QUOTE_MINIMAL表示仅当一个字段包 含引号或者定义符号的时候才in引号

2 ) csv.writer(csvfile，dialect=,excer, **fmtparams)，用于写人 CSV 文件。参数同上。来 看一个使用例子。

with open (•data.csv*,    ' wb1) as csvfile:

csvwriter = csv.writer(csvfile, dialect^•excel1,delimiter:"I ",quotechar:，

quoting=csv.QUOTE_MINIMAL)    .

csvwriter . writerow ([f,l/3/09 14 : 44", •" Productl "V1200 • ••Visa"，Gouya"】)

\#写入行

输出形式为：1/3/09 14:44 1 -Productl* 11200 1 1 IVisalGouya

3    ) csv.DictReader(csvfile, fieldnames=None，restkey=None，restval=None，dialect-excel1, ♦args, ♦♦kwds),同reader()方法类似，不同的是将读人的信息映射到一个字典中去，其中字 典的key由fieldnames指定，该值省略的话将使用CSV文件第一行的数据作为key值。如果 读人行的字段的个数大于filednames中指定的个数，多余的字段名将会存故在restkey中，而 restval主要用于当读取行的域的个数小于fieldnames的时候，它的值将会被用作剩下的key 对应的值。

4    ) csv.DietWriter(csvfile, fieldnames, restval-*, extrasaction='raise,, dialect='excel’，*args, **kwds),用于支持字典的写人。

import csv #DictWriter

with open (•test.csv*,    1 wb 1) as csv一file:

\#设置列名称    "

FIELDS = [•Transaction_date *,    1 Product•,    1 Price *,    1 Payment一Type■]

writer = csv. DictWriter (csv file, fieldnames=FIELDS)

\#写入列名称    '

writer.writerow(diet(zip(FIELDS, FIELDS)))

d = {•Transaction_date•1/2/09 6:17•,1 Product*:1Productl1,1 Price■1200，， ’ Payment_Type1:•Mastercard*}

\# 写入一行 Writer .write row (d) with open (1 test .csv* 9    • rb*) as csv file:

for d in csv. DictReader (csv一file): print d

\#output d is:{1 Product1: 1Productl*/ * Transaction_date*: 11/2/09 6:17，， • Price•: 11200 f 9 •Payment_Type1: •Mastercard*}

csv模块使用非常简单，基本可以满足大部分需求。但你有没有思考过这个问题：有些 应用中需要解析和处理的CSV文件可能有上百MB甚至几个GB,这种情况下csv模块是否 能够应付呢？先来做个实验，临时创建一个1GB的CSV文件并将其加载到内存中，看看会 有什么问题发生。

»> f = open (1 large. csv1, nwbf,)

\#创建大文件的技巧



\#输出文件的大小



»> f .seek(1073741824-l)

»> f.writer、。1。

»> f . close ()

»> import os

»> os.stat ("large.csv") .st_size

1073741824L

\>» with open ("large.csv", "rbn> as csvfile: ...    mycsv = csv. reader (csvfile, delimit er=

...    for row in mycsv:

•.•    print row

Traceback (most recent call last):

File "<stdin>", line 3, in <module>

MemoryError    #发生了内存异常

»>

上面的例子中当企图读人这个CSV文件的时候抛出了 MemoryError异常。这是为什 么？因为csv模块对于大型CSV文件的处理无能为力。这种情况下就需要考虑其他解决方案 了，pandas模块便是较好的选择。

Pandas即Python Data Analysis Library，是为丫解决数据分析而创建的第三方T.具，它 不仅提供了丰富的数据模型，而且支持多种文件格式处理，包括CSV、HDF5、HTML等， 能够提供髙效的大型数据处理。其支持的两种数据结构一Series和DataFrame—是数据处 理的基础。下面先来介绍这两种数据结构。

□ Series:它是一种类似数组的带索引的一维数据结构，支持的类型与NumPy兼容。如 果不指定索引，默认为0到N-1。通过obj.valuesO和obj.indexO可以分别获取值和索

弓I。当给Series传递一个字典的时候，Series的索引将根据字典中的键排序。如果传 人字典的时候同时重新指定了 index参数，当index与字典中的键不匹配的时候，会 出现时数据丢失的情况，标记为NaN。

在pandas中用函数isnull()和notnull()来检测数据是否丢失。

\>» obj 1 = Series ([1,    •a* #    (1,2), 3] 9 index=【fa’, •b\    ’c.'    •d* ])

\>>> obj 1 #value 和 index ---匹配

dtype: object

»> obj2=Series ({"Book”： "Python", .’Author" : "Dan% "ISBN” ： "011334", "Price" : 25 h inde x=[•book1,，Author1,•ISBM，，•Price*])

»> obj2 . isnull ()

book    True    #指定的index与字典的鍵不匹配，发生数据丢失

Author    False

ISBM    True    #指定的index与字典的键不匹配，发生数据丢失

Price    False

dtype: bool

\>>>

□ DataFrame :类似于电子表格，其数据为排好序的数据列的集合，每一列都可以是 不同的数据类型，它类似于一个二维数据结构，支持行和列的索引。利ISeries— 样，索引会自动分配并且能根据指定的列进行排序。使用最多的方式是通过一个长 度相等的列表的字典来构建。构建一个DataFrame最常用的方式是用一个相等长度 列表的字典或NumPy数组。DataFrame也Pf以通过columns指定序列的顺序进行 排序。

\>» data = {'OrderDate': ['1-6-10，， '1-23-10*, ,2-9-10,, '2-26-10', '3-15-10'], ...    'Region': ['East *, 'Central1, 'Central', 'West', 'East'],

...    ’Rep1: [1 Jones1, ’Kivell., •Jardine', .Gill’，’Sorvino']}

\>>>

\>>> DataFrame (data, columns=['OrderDate1,'Region1,'Rep1】}# 通过字典构建，按照 cloumns 指定的顺序排序

|      | OrderDate | Region  | Rep     |
| ---- | --------- | ------- | ------- |
| 0    | 1-6-10    | East    | Jones   |
| 1    | 1-23-10   | Central | Kivell  |
| 2    | 2-9-10    | Central | Jardine |
| 3    | 2-26-10   | West    | Gill    |
| 4    | 3-15-10   | East    | Sorvino |

Pandas中处理CSV文件的函数主要为read_csv()和to_csv()这两个，其中read_csv()读 取CSV文件的内容并返回DataFrame, to_CSv()则是其逆过程。两个函数都支持多个参数， 由于其参数众多且过于复杂，本节不对各个参数一一介绍，仅选取几个常见的情形结合具体 例子介绍。下面举例说明，其中需要处理的CSV文件格式如图4-2所示。

OxderDatef Region,ReprItem,Units,Unit Cost,Total

1-6-10/Eastr Jonesf Pencil/95/ 1.99 • 189.05

1—    23-10,Central,Bi湖XI,ainder,50, 19.99 , 999.50

2-    9-10/Centralr Jardine# Pencil/36/ 4.99 , 179.64

2-    26-10,Central,Gill,Pen,27r 19.99 z 539.73

3-    15-lOr Westr    Pencil,S6, 2.99 r 167.44

4-    1-10,East,Jones,Binder,60, 4.99 , 299.40 4-18-10,Cemtral，Andrews,Pencil,75, 1.99 • 149.2S

图4-2 CSV文件示例

###### 1 ）指定读取部分列和文件的行数。具体的实现代码如下:

\>>> df = pd.read csv(HSampleData.csvH,nrows=5/usecols=[*OrderDate1r1 Item1x'Total

»> df

|      | OrderDate | Item   | Total  |
| ---- | --------- | ------ | ------ |
| 0    | 1-6-10    | Pencil | 189.05 |
| 1    | 1-23-10   | Binder | 999.50 |
| 2    | 2-9-10    | Pencil | 179.64 |
| 3    | 2-26-10   | Pen    | 539.73 |
| 4    | 3-15-10   | Pencil | 167.44 |

方法read_csv（）的参数nrows指定读取文件的行数，usecols指定所要读取的列的列名， 如果没有列名，可直接使用索引0、1、...、n-1。上述两个参数对大文件处理非常有用，可 以避免读人整个文件而只选取所需要部分进行读取。

2 ）设置CSV文件与excel兼容。dialect参数可以是string也可以是csv.Dialect的实例。 如果将图4-2所示的文件格式改为使用分隔符，则需要设置dialect相关的参数。error^ badjines设置为False,当记录不符合要求的时候，如记录所包含的列数与文件列设置不相 等时可以直接忽略这些列。下面的代码用于设置CSV文件与excel兼容，其中分隔符为“|”， 而error_bad_Iines=False会直接忽略不符合要求的记录。

»> dia = csv.excel ()

»> dia .delimiter:" |    # 设置分隔符

»> pd. read_csv (HSD.csv,1)

OrderDate I Region I Rep I Item I Units I Unit Cost I Total

0    1-6-101 East I Jones I Pencil 19511.99 1189.05

1    1-23-10|Central IKivellI Binder I 50 119.99 1999.50...

\>〉> pd.read csv("SD.csvn,dialect = dia,error bad lines=False)

Skipping line 3: expected 7 fields, saw 10    #所有不符合格式要求的列将直接忽咚

OrderDate Region Rep Item Units Unit Cost Total 0    1-6-10 East Jones Pencil    95    1.99    189.05

\>»

3）对文件进行分块处理并返回一个可迭代的对象。分块处理可以避免将所有的文件载人 内存，仅在使用的时候读人所需内容。参数chunksize设置分块的文件行数，10表示每一块 包含10个记录。将参数iterator设置为True时，返回值为TextFileReader,它是一个可迭代对

###### 象。来看下面的例子，当chunksize=10、iterator=True时，每次输出为包含10个记录的块。

\>>> reader = pck read_table("SampleData•csv",chunksize=10,iterator=True) >>> reader

<pandas.io.parsers.TextFileReader object at 0x0314BE70>

»> iter (reader) . next ()    # 将 Tex tFileReader 转换为迭代器并调用 next 方法

Order Date, Region, Rep, Item, Units, Unit Cost, Total #每次读入 10 行

| 0    | 1-6-10,East,Jones,Pencil,95,        | 1.99 ,  | 189.05 |
| ---- | ----------------------------------- | ------- | ------ |
| 1    | 1-23-10,Central,Kiveil,Binder, 50,  | 19.99 , | 999.50 |
| 2    | 2-9-10,Central,Jardine, Pencil, 36, | 4.99 ,  | 179.64 |
| 3    | 2-2 6-10/Central,Gill / Pen, 21,    | 19.99 , | 539.73 |
| 4    | 3-15-10/Westr Sorvino/Pencil, 56,   | 2.99 ,  | 167.44 |
| 5    | 4-1-10,East,Jones,Binder, 60,       | 4.99 ,  | 299.40 |
| 6    | 4-18-10,Central,Andrews,Pencil, 75, | 1.99 r  | 149.25 |
| 7    | 5-5-10,Central,Jardine,Pencil,90,   | 4.99 r  | 449.10 |
| 8    | 5-22-10/West/Thompson, Pencil, 32   | r 1.99  | ,63.68 |
| 9    | 6—8-10,East/Jones,Binder,60,        | 8.99 ,  | 539.40 |

###### 4）当文件格式相似的时候，支持多个文件合并处理。以下例子用于将3个格式相同的 文件进行合并处理。

\>» filelst = os . list dir (M test*1)

»> print filelst    #同时存在3个格式相同的文件

[•si•csv1,    •s2.csv•'    * s3.csv•】

| >» os . chdir ("test") | for f in filelst]    |                 |        |        |             |                |       |
| ---------------------- | -------------------- | --------------- | ------ | ------ | ----------- | -------------- | ----- |
| >>〉                   | dfs =[pd.read_csv(f) |                 |        |        |             |                |       |
| >»                     | total^df             | =pd.concat(dfs) |        |        | #将文件合并 |                |       |
| >»                     | total_df             |                 |        |        |             |                |       |
| OrderDate              | Region               | Rep             | Item   | Units  | Unit Cost   | Total          |       |
| 0                      | 1-6-10               | East            | Jones  | Pencil | 95          | 1.99    189.05 |       |
| 1                      | 1-23-10              | Central         | Kivell | Binder | 50          | 19.99          | 999.5 |

###### 了解完pandas后，读者可以自行实验一下使用pandas处理前面生成的1GB的文件，看 看还会不会抛出Memory Error异常。

在处理CSV文件上，特别是大型CSV文件，pandas不仅能够做到与csv模块兼容，更 重要的是其CSV文件以DataFrame的格式返回，pandas对这种数据结构提供了非常丰富的 处理方法，同时pandas支持文件的分块和合并处理，非常灵活，由于其底层很多算法采用 Cython实现运行速度较快。实际上pandas在专业的数据处理与分析领域，如金融等行业已 经得到广泛的应用。

##### 菅议43: —般情况使用ElementTree解析XML

xml.dom.minidom和xml.sax大概是Python中解析XML文件最广为人知的两个模块了， 原因一是这两个模块自Python 2.0以来就成为Python的标准库；二是网上关于这两个模块的 使用方面的资料最多。作为主要解析XML方法的两种实现，DOM需要将整个XML文件加 载到内存中并解析为一棵树，虽然使用较为简单，但占用内存较多，性能方面不占优势，并 且不够Pythonic;而SAX是基于事件驱动的，虽不需要全部装人XML文件，但其处理过程 却较为复杂。实际上Python中对XML的处理还有更好的选择，ElementTree便是其中一个， —般情况下使用ElementTree便已足够。它从Python2.5开始成为标准模块，cElementTree是 ElementTree的Cython实现，速度更快，消耗内存更少，性能上更占优势，在实际使用过程 中应该尽ft优先使用cElemcntTreeo由于两者使用方式上完全兼容本文将两者看做一个物件， 除非说明不再刻意区分。ElementTree在解析XML文件上具有以下特性：

□使用简单。它将整个XML文件以树的形式展示，每一个元素的属性以字典的形式表 示，非常方便处理。

□内存上消耗明显低于DOM解析。由于ElementTree底层进行了一定的优化，并且它 的iterparse解析工具支持SAX事件驱动，能够以迭代的形式返回XML部分数据结 构，从而避免将整个XML文件加载到内存中，因此性能上更优化，相比于SAX使 用起来更为简单明了。

□支持XPath查询，非常方便获取任意节点的值。

这里需要说明的是，一般情况指的是：XML文件大小适中，对性能要求并非非常严格。 如果在实际过程中需要处理的XML文件大小在GB或近似GB级别，第三方模块lxml会获 得较优的处理结果。关于lxml模块的介绍请参考本章后续小节或者参考文章“使用由Python 编写的 lxml 实现商性能 XML 解析”，可通过链接 <http://www.ibm.com/developerworks/cii/xml/> x-hiperfparse/ 可以访问。

下面结合具体的实例来说明elementtree解析XML文件常用的方法。需要解析的XML 实例如下：

<systems>

〈system platform="linux" name="linuxtest”>

<purpose>automation test</purpose> <system_type>virtual</system_type>

<ip_address/>

<commands_on_boot>

<command_details>

<!—— Set root password.——>

<command>echo root:mytestpwd I sudo /usr/

sbin/chpasswd</command>

<userid>root2</userid>

<password>PasswOrd</password>

</command_details>

<command_details>

<conmand>mkdir /TEST; chmod 777 /TEST</command> 〈/command details>

</commands_on_boot>

</system>

〈system platform="aix" name=t,aixtestM>

<purpose>manual test</purpose〉

<system_type>virtual</system__type>

<ip_address/>

<commands_on_boot>

<command_details>

<!-- Set root password.-->

<command>Gcho root:mytestpwd | sudo /usr/

sbin/chpasswd</command>

<userid>root2</userid>

<password>PasswOrd</password>

</command_details>

<command_details>

<command>mkdir /TEST; chmod 777 /TESTc/command〉 </comniand_details>

</commands_on_boot>

</system>

〈/systems〉

###### 模块ElementTree主要存在两种类型ElementTree和Element,它们支持的方法以及对应 的使用示例如表4-1和表4-2所示。

| 表4-1 ElementTree主要的方法和使用示例                  |                                                              |
| ------------------------------------------------------ | ------------------------------------------------------------ |
| 主要的方法、属性                                       | 方法说明以及示例                                             |
| getroot()                                              | 返回xml文档的根节点»> import xml.etree.ElemcntTree as ET »> tree = ET.ElementTree(file ="test.xml”)»> root = tree.getroot()»> print root〈Element 'systems' at 0x26cbff0>»> print root.tag systems |
| find(match)findall(match)findtext(match, default=None) | 同Ekment相关的方法类似，只是从跟节点开始搜索(见表4-2)»> for i in root.findall(,,system/purposeH):... print i.text• • •automation testmanual test»> print root.findtext(wsystem/purposeM)automation test>» print root.find(,fsystem/purposeH)〈Element 'purpose* at 0x26d2170> |
| iter(tag=None)                                         | 从xml根结点开始，根据传人的元素的tag返回所有的元索集合的迭代器 »> for i in tree.iter(tag = "command"):... print i.text• ••echo root:mytestpwd \| sudo /usr/sbin/chpasswdmkdir /TEST; chmod 777 /TESTecho root:myteslpwd \| sudo /usr/sbin/chpasswdmkdir /TEST; chmod 777 /TEST |

|                               | （续）                                                       |
| ----------------------------- | ------------------------------------------------------------ |
| 主要的方法、属性              | 方法说明以及示例                                             |
| iterfind(match)               | 根据传人的lag名称或者path以迭代器的形式返回所有的子元素 〉>〉for i in tree.iterfindC'system/purpose1*):... print i.text• •蠹automation testmanual test |
| 主要的方法、属性              | 表4-2 Element主要的方法和使用示例方法说明以及示例            |
| tag                           | 字符串，用来表示元素所代表的名称 »> print root[l].tag # 输出 system |
| text                          | 表示元索所对应的具体值 »> print rootf 1 ].text # 输出空申    |
| attrib                        | 用字典表示的元素的属性»> print root[l].attrib # 瑜出{•platform1: *aix\ 'name*: 'aixtest'} |
| get(key, dcfault=Nonc)        | 根据元索属性字典的key值获取对应的值，如果找不到对应的域性，则返回 default»> print root[l].attrib.get（,lplatformM） # 输出 aix |
| itcms()                       | 将元素属性以(名称，值)的形式返回»> print root[l].items() #[(,platform,, 'aix'), (.name’，’aixtest’)】 |
| keys()                        | 返回元索属性的key值集合»> print root[l].keys() # 输出「platform.，'name'] |
| find(match)                   | 根据传人的tag名称或者path返回第一个对应的element对象，或者返冋 None |
| findall(match)                | 根据传入的tag名称或者path以列表的形式返回所存符合条件的元素  |
| findtext(match, default=None) | 根据传人的tag名称或者path返回第一个对应的e lernent对象对应的值，即 text属性，如果找不到则返回default的没置 |
| list(elem)                    | 根据传入的元素的名称返间其所有的子节点 »> for i in list(root.findall("system/system type")):... print i.text# 输出 virtual virtural |

###### 前面我们提到elementree的iterparse工具能够避免将整个XML文件加载到内存，从 而解决当读人文件过大内存而消耗过多的问题。iterparse返回一个可以迭代的由元组（时

间，元素）组成的流对象，支持两个参数-source和events,其中event有4种选择-

start、end、startns 和 endns （默认为 end）,分別与 SAX 解析的 startElement、endElement、 startElementNS 和 endElementNS 一一对应。

本节最后来看一下iterparse的使用示例：统计userid在整个XML出现的次数。

\>>〉count = 0

\>» for event, elem in ET. iterparse ("test • xml") : # 对 iterparse 的返回值进行迭代 ...    if event ==,end,:

...    if elem.tag ==f userid1:

...    count+=l

...    elem.clear()

»> print count

2

##### 建议44:理解模块pickle优劣

在实际应用中，序列化的场景很常见，如：在磁盘上保存当前程序的状态数据以便重启 的时候能够重新加载；多用户或者分布式系统中数据结构的网络传输时，可以将数据序列化 后发送给一个可信网络对端，接收者进行反序列化后便可以重新恢复相同的对象；session和 cache的存储等。序列化，简单地说就是把内存中的数据结构在不丢失其身份和类型信息的 情况下转成对象的文本或二进制表示的过程。对象序列化后的形式经过反序列化过程应该能 恢复为原有对象。Python中有很多支持序列化的模块，如pickle、json、marshal和shelve等。 最广为人知的为pickle,我们来仔细分析一下这个模块。

pickle估计是最通用的序列化模块了，它还有个C语言的实现cPickle，相比pickle来说 具有较好的性能，其速度大概是pickle的1000倍，因此在大多数应用程序中应该优先使用 cPickle (注：cPickle除了不能被继承之外，它们两者的使用基本上区别不大，除有特殊情况， 本节将不再做具体区分)。pickle中最主要的两个函数对为dump()和load()，分别用来进行对 象的序列化和反序列化。

□    pickle.dump(obj, file[, protocol]):序列化数据到一•个文件描述符(一个打开的文件、 套接字等)。参数obj表示需要序列化的对象，包括布尔、数字、字符串、字节数组、 None,列表、元组、字典和集合等基本数据类型，此外picike还能够处理循环，递归 引用对象、类、函数以及类的实例等。参数file支持write()方法的文件句柄，可以为 真实的文件，也可以是StringlO对象等。protocol为序列化使用的协议版本，0表示 ASCII协议，所序列化的对象使用可打印的ASCII码表示；1表示老式的二进制协议； 2表示2.3版本引人的新二进制协议，比以前的更髙效。其中协议0和1兼容老版本 的 Python。protocol 默认值为 0。

□    load(file)：表示把文件中的对象恢复为原来的对象，这个过程也被称为反序列化。

来看一下load()和dump()的示例。

»> import cPickle as pickle

\>>> my_data = {"name" : "Python”， "type” : "Language", "version" : ”2.7.5”}

\>» fp = open ("picklefile • dat", "wb")    參打开要写入的文件

\>>〉pickle.dump (my_dataz fp)    ♦使用 dump 进行序列化

\>» fp.close ()

\>»

»> fp = open ("picklefile.dat", "rb")

»> out = pickle, load (fp)    #反序列化

»> print (out)

{•version•: •2.7.51r •type•: 1 Language * 9    * name *: •Python 1}

»> fp.close ()

pickle之所以能成为通用的序列化模块，与其良好的特性是分不开的，总结为以下几点：

###### 1)    接口简单，容易使用。通过dump()和load()便可轻易实现序列化和反序列化。

###### 2)    pickle的存储格式具有通用性，能够被不同平台的Python解析器共享，比如，Linux 下序列化的格式文件可以在Windows平台的Python解析器上进行反序列化，兼容性较好。

3)    支持的数据类型广泛。如数字、布尔值、字符串，只包含可序列化对象的元组、字 典、列表等，非嵌套的函数、类以及通过类的_<1以_或者_getstate_()可以返回序列化对 象的实例等。

4)    pickle模块是可以扩展的。对于实例对象，pickle在还原对象的时候一般是不凋 用」!^_()函数的，如果要调用进行初始化，对于古典类可以在类定义中提供 _getinitargs_()函数，并返回一个元组，当进行unpickle的时候，Python就会自动调用 _init_(),并把_getinitargs_()中返回的元组作为参数传递给_init_(),而对于新式类， 可以提供_getnewargs_()来提供对象生成时候的参数，在unpickle的时候以Class._new_ (Class，*arg)的方式创建对象。对于不可序列化的对象，如sockets、文件句柄、数据库连 接等，也可以通过实现pickle协议来解决这些局限，主要是通过特殊方法_getState_()和

setstate_()来返回实例在被pickle时的状态。来看以下示例：

import cPickle as pickle

class TextReader:

def _init_(self, filename):

self .filename = filename    # 文件名称

self .file = open (filename)    # 打开文件的句柄

self .postion = self .file. tell ()    # 文件的位置

def readline(self):

line = self .file, readline () self .postion = self .file. tell () if not line:

return None

if line.endswith(1\n1): line = line[:-l】

return w%i: %s" % (self.postion, line) def _getstate_ (self) :    #记承文件被pickle时候的状态

state = self._diet_. copy ()    # 获取被 pickle 时的字典信息

del state【’file* ] return state

def _setstate_(selfr state) :    # 设反序列化后的状态

self._diet_.update(state)

file = open (self .filename) self .file = file

reader = TextReader (f,zen.

print(reader.readline ()) print(reader.readline ()) s = pickle.dumps(reader)

\#在dumps的时候会默认调用_getstate #在loads的时候会默认调用 setstate



newsreader = pickle.loads (s) print(new reader.readline ())

5)能够自动维护对象间的引用，如果一个对象上存在多个引用，pickle后不会改变对象 间的引用，并且能够自动处理循环和递归引用。

\>» a = (, 'b']

\#b引用对象a



\#反序列化对al对象的修改仍然会影响到bl



\>» b = a

»> b.append (1 c 1)

\>» p = pickle. dumps ( (ar b)) >>> al,bl « pickle.loads(p) »> al

»> bl

[•a，， ，b., 'c，J >» al .append (1 d1) »> bl

[-c1,    -d1]

但pickle使用也存在以下一些限制：

□    pickle不能保证操作的原子性。pickle并不是原子操作，也就是说在一个pickle调用 中如果发生异常，可能部分数据已经被保存，另外如果对象处于深递归状态，那么可 能超出Python的最大递归深度。递归深度可以通过sys.setrecursionlimit()进行扩展。

□    pickle存在安全性问题。Python的文档清晰地表明它不提供安全性保证，因此对于一 个从不可信的数据源接收的数据不要轻易进行反序列化。由于loads()可以接收字符 串作为参数，这意味着精心设计的字符串给人侵提供了一种可能。在Pthon解释器中 输人代码pickle.loads("cos\nsystem\n(S'dir'\ntR.")便可査看当前目录下所有文件。如果 将dir替换为其他更具有破坏性的命令将会带来安全隐患。如果要进一步提髙安全性, 用户可以通过继承类pickle.Unpickler并重写find_class()方法来实现。

□    pickle协议是Python特定的，不同语言之间的兼容性难以保障。用Python创建的 pickle文件可能其他语言不能使用，如Perl、PHP、Java等。

##### 建议45:序列化的另一个不错的选择——JSON

JSON ( JavaScript Object Notation)是一种轻量级数据交换格式，它基于JavaScript编程 语言的一个子集，于1999年12月成为一个完全独立于语言的文本格式。由于其格式使用了 其他许多流行编程的约定，如C、C++、C#、Java, JS, Python等，加之其简单灵活、可

读性和互操作性较强、易于解析和使用等特点，逐渐变得流行起来，甚至有代替XML的趋 势。关于JSON和XML之间的优劣，一直有很多争论，本书并不打算对这两者之间的是是 非非做详尽的分析（笔者的观点是两者各有所长，在相当长的时间里还会共存共荣），这里关 注的是JSON用于序列化方面的优势。在进行详细讨论之前，我们先来看看Python语言中对 JSON的支持现状。

Python中有一系列的模块提供对JSON格式的支持，如simplejson、cjson、yajl、ujson，自 Python2.6后又引人了标准库JSON。简单来说cjson和ujson是用C来实现的，速度较快。据 cjson的文档表述：其速率比纯Python实现的json模块大概要快250倍。yajl是Cpython版本的 JSON实现，而simplejson和标准库JSON本质来说无多大区别，实际上Python2.6中的json模 块就是simplejson减去对Python2.4、2.5的支持以充分利用最新的兼容未来的功能。不过相对 于simplejson，标准库更新相对较慢，Python2.7.5中simplejson对应的版本为2.0.9，而最新的 simplejson的版本为3.3.0。在文际应用过程中将这两者结合较好的做法是采用如下import方法。

try: import simplejson as json except ImportError: import json

本节仍采用标准库JSON来做一些探讨。Python的标准库JSON提供的最常用的方法 与pickle类似，dump/dumps用来序列化，load/loads用来反序列化。需要注意的json默认不 支持非ASCIhbased的编码，如load方法可能在处理中文字符时不能正常显示，则需要通过 encoding参数指定对应的字符编码。在序列化方面，相比pickle, JSON具有以下优势：

1    ）使用简单，支持多种数据类型。JSON文档的构成非常简单，仅存在以下两大数据结构。

□名称/值对的集合。在各种语言中，它被实现为一个对象、记录、结构、字典、散列

表、键列表或关联数组。

□值的有序列表。在大多数语言中，它被实现为数组、向量、列表或序列。在Python 中对应支持的数据类型包括字典、列表、字符串、整数、浮点数、True、False. None 等。JSON中数据结构和Python中的转换并不是完全——对应，存在一定的差异，读 者可以自行查阅文档。Python中一个JSON文档可以分解为如图4-3所示形式。

2    ）存储格式可读性更为友好，容易修改。相比于pickle来说，json的格式更加接近程序 员的思维，修改和阅读上要容易得多。dumpsO函数提供了一个参数indent使生成的json文件 可读性更好，0意味着“每个值单独一行”；大于0的数字意味着“每个值单独一行并且使用 这个数字的空格来缩进嵌套的数据结构”。但需要注意的是，这个参数是以文件大小变大为代 价的。如图4-4展示的是这两种格式之间的对比，其中json.dumps（）使用了 indent参数输出。

3    ） json支持跨平台跨语言操作，能够轻易被其他语言解析，如Python中生成的json文 件可以轻易使用JavaScript解析，互操作性更强，而pickle格式的文件只能在Python语言中 支持。此外json原生的JavaScript支持，客户端浏览器不需要为此使用额外的解释器，特別 适用于Web应用提供快速、紧凑、方便的序列化操作。此外，相比于pickle, json的存储格 式更为紧凑，所占空间更小。

用》形式表示的对象

一个或多个用，隔开的 键值对

键名    ：    值

| TrueZFalse |      | None |      | 列表/元组 |      | 整数/浮点数 | str/jiaiss^ |
| ---------- | ---- | ---- | ---- | --------- | ---- | ----------- | ----------- |
|            |      |      |      |           |      |             |             |

图4-3 json文档分解图

| S * ordorlD1          | {                       |
| --------------------- | ----------------------- |
| p2                    | "orderlD": 12345r       |
| 112345                | ”3hopperNaxne": "Tom",  |
| s3 * shopperName *    | "orderCompleted": true. |
| p3                    | "contents"s [           |
| S*Tom•                | {                       |
| P4                    | "productName'*: "Soap"r |
| sSf orderCompletedf   | "quantity": 2,          |
| p5                    | "productID": 34         |
| 101                   |                         |
| s39 contents w        | (                       |
| p6                    | "productName": "SHOES'  |
| (lp7                  | ••quantity" : 1,        |
| (dp日                 | "productID••: 56        |
| S’productNain© ’      |                         |
| P9                    |                         |
| s’soap.    pickle格式 | "shopporEmail":         |
| plO                   | }                       |
| s3* quantity1         | json^S 式               |

阁4-4 pickle和json文件格式对比

4)具有较强的扩展性。json模块还提供了编码(JSONEncoder)和解弼类(JSONDecoder) 以便用户对其默认不支持的序列化类型进行扩展。来看一个例子：

\>>> d-datetime.datetime.now()

»> d

datetime.datetime(2013/ 9, 15, 8, 54, 59, 851000)

〉>> json.dumps(d)

raise TypeError(repr(o) + " is not JSON serializable")

TypeError: datetime.datetime(2013, 9, 15r 8, 54, 59, 851000) is not JSON seriali zable

5 ) json在序列化datetime的时候会抛出TypeError异常，这是因为json模块本身不支持 datetime的序列化，因此需要对json本身的JSONEncoder进行扩展。有多种方法可以实现， 下面的例子是其中实现之一。

import datetime

from time import mktime

try: import simplejson as json except ImportError: import j son

class DateTimeEncoder (json. JSONEncoder) :    # 对 JSONEncoder 逬行扩展

def default (selfz obj):

if isinstance(obj r datetime.datetime):

return obj.strftime(* %Y-%m-%d %H:%M:%S1)

elif isinstance(obj/ date):

return obj.strftime(1)

return json.JSONEncoder.default(self, obj)

d=datetime.datetime.now()

print json .dumps (dr cis = DateTimeEncoder)    # 使用 cis 指定编每器的名称

最后需要提醒的是，Python中标准模块json的性能比pickle与cPickle稍逊。如果对序 列化性能要求非常高的场景，可以选择cPickle模块。图4-5显示的是这三者序列化时随着数 据规模增加所消耗时间改变的图例。

0.1



###### 0.08



茗0.

0.04



0.02



json

pickle

cPidkle



![img](12 1699d828cfe301 3984Python0b8f84912afaae-56.jpg)



oooooooooo OOOOOOOQOO ——cm <s m m    <n <n

数据規模



0098

0018

009£

00U

0099

0019



as



图4-5



pickle、json、cPickle序列化文件时性能比较



##### 旨议46:使用traceback获取栈信息



###### 当程序产生异常的时候，最需要面对异常的其实是开发人员，他们需要更多的异常提示 的信息，以便调试程序中潜在的错误和问题。先来看一个简单的例子：



gList = [.a\.d、.f \ .g.]

def f():

gList【5】 return g()

def g ():

return h()

def h():

del gList[2] return i()

def i ():

gList.append(1i1) print gList[7]

if _name_ == ’_main_1 :

try:

f 0

except IndexError as ex:

print "Sorry,Exception occured,you accessed an element out of range" print ex

###### 上述程序运行输出如下：

Sorry,Exception occuredr you accessed an element out of range list index out of range

信息提示有异常产生，对于用户这点还算是较为友好的，那么对于开发人员，他如何快 速地知道错误发生在哪里呢？逐行检查代码吗？对于简单的程序，这也不失为一个办法，但 在较为复杂的应用程序中这个方法就显得有点低效了。面对异常开发人员最希望看到的往往 是异常发生时候的现场信息，traceback模块可以满足这个需求，它会输出完整的栈信息。将 上述代码异常部分修改如下：

except IndexError as ex:

print "Sorry,Exception occuredr you accessed an element out of range" print ex

traceback.print_exc()

###### 程序运行会输出异常发生时候完整的栈信息，包括调用顺序、异常发生的语句、错误类 型等。

Sorry,Exception occuredF you accessed an element out of rangelist index out of range Traceback (most recent call last):

| File ’’trace .py". | line | 20,  | in <module> |
| ------------------ | ---- | ---- | ----------- |
| f ()               |      |      |             |
| File wtrace.pyMr   | line | 5r   | in f        |
| return g ()        |      |      |             |
| File ,,trace.py,t/ | line | 8r   | in g        |

return h ()

| File ’’trace . py", | line | 12,  | in   | h    |
| ------------------- | ---- | ---- | ---- | ---- |
| return i()          |      |      |      |      |
| File "trace.pyMr    | line | 16,  | in   | 攀1  |

print gList[7]

IndexError: list index out of range

traceback.print_exc()方法打印出的信息包括3部分：错误类型(IndexError)、错误对应 的值(list index out of range)以及具体的trace信息，包括文件名、具体的行号、函数名以及 对应的源代码。Traceback模块提供了一系列方法来获取和显示异常发生时候的trace相关信 息，下面列举几个常用的方法：

1    ) traceback.print_exception(type，value, traceback[,    file]])，根据 limit 的没置打

印栈信息，file为None的情况下定位到sys.stderr，否则则写人文件；其中type、value、 traceback这3个参数对应的值可以从sys.exc_info()中获取。

2    ) raceback.print_cxc([limit[, file]])，为 print_exception()困数的缩写，不需要传入 type、 value、traceback 这 3 个参数。

3)    traceback.format_exc([limit]),与print_exc()类似，区别在于返回形式为字符串。

4)    traceback.extract_stack([file,[, limit]]),从当前栈巾贞中提取 trace 信息。

读者可以参看Python文档获取更多关于traceback所提供的抽取、格式化或者打印程序 运行时候的栈跟踪信息的方法。本质上模块trackback获取异常相关的数据都是通过sys.exc_ info()函数得到的。当有异常发生的时候，该函数以元组的形式返回(type，value, traceback), 其中type为异常的类型，value为异常本身，traceback为异常发生时候的调用和堆栈信息， 它是一个traceback对象，对象中包含出错的行数、位置等数据。上面的例子中也可以通过如 下方式输出异常发生时候的详细信息：

tbtype, tbval, exc_tb = sys.exc_info()

for filename, linenum, funcname, source in traceback.extract_tb(exc_tb): print M%-23s: %s * %s1 in %s ()n % (filename, linenum, source, funcname)

实际上除f traceback模块本身，inspect模块也提供了获取traceback对象的接口， inspecurace^context】)可以返回当前帧对象以及异常发生时进行捕获的帧对象之间的所有栈 帧记录列表，因此第一个记录代表当前调用对象，最后一个代表异常发生时候的对象。其 中每一个列表元素都是一个由6个元素组成的元组：(frame对象，文件名，当前行号，函数 名，源代码列表，当前行在源代码列表中的位置)。本节开头的例子在异常部分使用inspect. trace()来获取异常发生时候的堆栈信息，其部分输出结果如下：

[(<frame object at Ox022CB480>, •testinspect.py1,

23,

•〈module〉•，

[•\t f O\nMr

0)r

• • • • • • J

此外如果想进一步追踪函数调用的情况，还可以通过inspect模块的inspect.stack()函数

査看函数层级调用的栈相信信息。因此，当异常发生的时候，合理使用上述模块中的方法可 以快速地定位程序中的问题所在。

##### 建议47:使用logging记录曰志信息

仅仅将栈信息输出到控制台是远远不够的，更为常见的是使用日志保存程序运行过程中 的相关信息，如运行时间、描述信息以及错误或者异常发生时候的特定上下文信息。Python 中自带的logging模块提供了日志功能，它将logger的level分为5个级别(如表4-3所示), 可以通过Logger.setLevel(lvl)来设置，其中DEBUG为最低级别，CRITICAL为最髙级别， 默认的级别为WARNING。

表4-3日志级别

| Level    | 使用情形                                                     |
| -------- | ------------------------------------------------------------ |
| DEBUG    | 洋细的信息.在追踪问题的时候使用                              |
| INFO     | 正常的信息                                                   |
| WARNING  | 一些不可预见的问题发生.或者将要发生.如磁盘空间低等，佝不彫响程序的运行 |
| ERROR    | 由于某些严里的问题.程序中的一些功能受到影响                  |
| CRITICAL | 严进的错误.或者程序本身不能够继续运行                        |

logging lib包含以下4个主要对象：

丨)logger。logger是程序信息输出的接口，它分散在不同的代码中，使得程序可以在 运行的时候记录相应的信息，并根据设置的日志级别或filtei•来决定哪些信息®要输出，并 将这些信息分发到其关联的handler。常用的方法有Logger.setLevel()、Logger.addHandler()、 Logger.removeHandler()s Logger.addFilter(), Logger.debug()、Logger.info()、Logger. waming()、Logger.error()、etLogger()等。

2) Handler。Handler用来处理信息的输出，可以将信息输出到控制台、文件或者网络。 可以通过 Logger.addHandler()来给 logger 对象添加 handler,常用的 handler 有 StreamHandler 和FileHandler类。StreamHandler发送错误信息到流，而FileHandler类用于向文件输出日志 信息，这两个handler定义在logging的核心模块中。其他的handler定义在logging.handles 模块中，如 HTTPHandler、SocketHandler。

3 ) Formatter。决定log信息的格式，格式使用类似于％(< dictionary key >)s的形式来 定义，如 ’％(asctime)s - %(levelname)s - %(message)s',支持的 key 可以在 Python 自带的文样 LogRecord attributes 中査看 o

4) Filter。用来决定哪些信息需要输出。可以被handler和logger使用，支持层次关系， 比如，如果设置了 filter名称为A.B的logger,则该logger和其子logger的信息会被输出， 如 A.B、A.B.C.

logging.basicConfig([**kwargs])提供对日志系统的基本配置，默认使用StreamHandler和 Formatter并添加到root logger,该方法自Python2.4开始可以接受字典参数，支持的字典参 数如表4-4所示。

表44字典参数格式类型

| 格    式 | 描    述                                                |
| -------- | ------------------------------------------------------- |
| filename | 指定FileHandler的文件名，而不是默认的StreamHandler      |
| filemode | 打开文件的模式，同open函数中的同名参数，默认为’ a’      |
| format   | 输出格式字符串                                          |
| datefmt  | 日期格式                                                |
| level    | 设置根logger的日志级别                                  |
| stream   | 指定StreamHandler。这个参数若与filename冲突，忽略stream |

###### 我们通过修改上一节的例子来看如何结合traceback和logging,记录程序运行过程中的 异常。

import traceback

import sys    .

import logging

gList = [ .a •，.b* .c •，•(！’r’e ’，•    ，fg •】

logging.basicConfig ( # R置日志的格出方式及格式 level=logging.DEBUG, filename」log. txt •, filemode=, w1,

format:，％(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s•,

)

def f():

gList[5]

logging.info (1 [INFO] :calling method g () in f {) •) # 记录正常的信患 return g()

def g ():

logging.info(1[INFO]:calling method h() in g 01) return h()

def h ():

logging, info (•【INFO】 ：Delete element in gList in h () 1) del gList[2】

logging.info(1[INFO]:calling method i() in h()1) return i ()

def i ():

logging.info(1[INFO]:Append element i to gList in i()•) gList.append(1i1) print gList[7]

if _name_ ■■ 1_main_•:

logging.debug(1 Information during calling f ():1)

try:

f 0

except IndexError as ex:

print ’• Sorry,Exception occured,you accessed an element out of rangew ttraceback.printexc() ty，tv，tb » sys.exc_info()

logging.error (•• [ERROR] :Sorry, Exception occured,you accessed an element out of range") #记录昇常错误係息

logging.critical(1 object info:%s• %ex)

logging.critical (1 Error Type: {0}, Error Information: {1} •. fornat (ty, tv))

\#记录异常的类型和对应的值

logging.critical (*1. join(traceback.fonnat_tb(t±>))) # 记录具体的 trace 信息 sys.exit(1)

修改程序后在控制台上对用户仅显示错误提示信息“ Sorry,Exception occured，you accessed an element out of range”，而开发人员如果需要debug可以在日志文件中找到具体运行过程中 的信息。

\#为了节省篇愔仅显示部分日志

2013-06-26 12:05:18, 923 traceexample.py[line:41J CRITICAL object info:list index out of range

2013-06-26 12:05:18,923 traceexample.py[line:42] CRITICAL Error Type:<type 1 exceptions.IndexError•>,Error Information:list index out of range

2013-06-26 12:05:18,924 traceexample.py[line:43] CRITICAL File ”traceexample.py”, line 35, in <module> f ()

| File "traceexample.py", return g () | line | 15r  | in   | f    |
| ----------------------------------- | ---- | ---- | ---- | ---- |
| File "traceexample.py", return h()  | line | 19,  | in   | g    |
| File ,ftraceexample.py", return i() | line | 25,  | in   | h    |
| File •• traceexample .py",          | line | 30,  | in   |      |

print gList［7］

上面的代码中控制运行输出到console上用的是printO,但这种方法比较原始， logging模块提供了能够同时控制输出到console和文件的方法。下面的例子中通过添加 StreamHandler并没置日志级别为logging.ERROR，可以在控制台上输出错误信息。

console = logging.StreamHandler O console.setLevel(logging.ERROR)

formatter = logging.Formatter(•%(name)-12s: %(levelname)-8s %(message)s f)

console.setFormacter(formatter)

logging.getLogger(•*).addHandler(console)

为广使Logging使用更为简单可控，logging支持loggin.config进行配置，支持dictConfig 和fileConfig两种形式，其中fileConfig是基于configparser()函数进行解析，必须包含的内容 为［loggers］、［handlers】和［formatters］。具体例子示意如下：

[loggers]

keys=root

[loggerroot]

level=DEBUG

handlers=hand01

(handlers]

keys=hand01

[handler_hand01J class-StreamHandler level-INFO formatter=form01 args=(sys.stderr,>

[formatters] keys=form01 [formatter formOl]

format=% (asctime) s % (filename) s【line : % (lineno) d] % (levelname) s % (message) s datefmt=%a, %d %b %Y %H:%M:%S

最后关于logging的使用，提以下几点建议：

1 )尽fi为logging取一个名字而不是采用默认，这样当在不同的模块中使用的时候，其 他模块只需要使用以下代码就可以方便地使用同一个logger,因为它本质上符合单例模式。

import logging

logging.basicConfig (level-logging. DEBUG) logger = logging.getLogger( _name_ )

2)    为了方便地找出问题所在，logging的名字建议以模块或者class来命名。Logging名 称遵循按划分的继承规则，根是root logger，loggera.b的父logger对象为a。

3)    Logging只是线程安全的，不支持多进程写人同一个日子文件，因此对于多个进程， 需要配置不同的日志文件。

##### 宣议48:使用threading模块编写多线程程序

GIL的存在使得Python多线程编程暂时无法充分利用多处理器的优势，这种限制也许 使很多人感到沮丧，但事实上这并不意味着我们需要放弃多线程。的确，对于只含纯Python 的代码也许使用多线程并不能提高运行速率，但在以下几种情况，如等待外部资源返回，或 者为了提髙用户体验而建立反应灵活的用户界面，或者多用户应用程序中，多线程仍然是 一个比较好的解决方案。Python为多线程编程提供了两个非常简单明了的模块：thread和 threading。那么，这两个模块在多线程处理上有什么区別呢？简单一点说：thread模块提 供丫多线程底层支持模块，以低级原始的方式来处理和控制线程，使用起来较为复杂；而 threading模块基于thread进行包装，将线程的操作对象化，在语言层面提供了丰富的特性。 Python多线程支持用两种方式来创建线程：一种是通过继承Thread类，電写它的nm(>方 法(注意，不是start()方法)；另一种是创建一个threading.Thread对象，在它的初始化函数 (_init_())屮将可凋用对象作为参数传人。实际应用中，推荐优先使用threading模块而不

是thread模块，(除非有特殊需要)。下面来具体分析一下这么做的原因。

1 ) threading模块对同步原语的支持更为完善和丰富。就线程的同步和互斥来说，thread 模块只提供了一种锁类型thread.LockType,而threading模块中不仅有Lock指令锁、RLock 可重人指令锁，还支持条件变量Condition、信号量Semaphore、BoundedSemaphore以及 Event事件等。

2) threading模块在主线程和F线程交互上更为友好，threading中的join()方法能够阻 塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout (可选参数)。 利用该方法可以方便地控制主线程和子线程以及子线程之间的执行。来看一个简单示例：

import threading, time,sys class test(threading.Thread):

def



— init_(selfr name,delay)

threading.Thread. init

(self)



self.name = name self.delay = delay

def run(self):

print "%s delay for %s" % (self.namer self.delay) time.sleep(self.delay) c = 0

while True:

print "This is thread %s on line %s” %(self.namerc) c = c + 1 i f c == 3:

print "End of thread %s" % self.name break

tl = test (1 Thread 1、2) t2 = test (f Thread 2、2) tl.start ()

print "Wait tl to end” tl.join() t2.start () print 1 End of main1

Thread 1 delay for    tl to end

^nd of tht*ead Thread 1 I Thread 2 dela</ for 2End of nain



上面的例子中，主线程main在tl上使用 join()的方法，主线程会等待tl结束后才继续运 行后面的语句，由于线程t2的启动在join语句 之后，t2—直等到tl退出后才会幵始运行。输 出结果如图4-6所示。

rhis is thread Thread 2 on line 0 rhis is thread Thread 2 on line 1 This is thread Thread 2 on line 2 End of thread Thread 2

图4-6多线程示例输出结果



3 ) thread模块不支持守护线程。thread模 块中主线程退出的时候，所有的子线程不论是否 还在工作，都会被强制结束，并且没有任何警告

###### 也没冇任何退出前的清理T作。来看一个例子:

from thread import start new thread

import time

def myfunc(a,delay):

print nI will calculate square of %s after delay for %s" % (a,delay)

time.sleep(delay)

print "calculate begins...”

result = a*a

print result

return result

start_new_thread (myfuncr (2,5)}# 同时启动两个浅程

start_new_thread(myfunc^ (6,8)}

time.sleep(1)

###### 运行程序，输出结果如下，你会发现子线程的结果还未返回就已经结束了。

I will calculate square of 2 after delay for 51 will calculate square of 6 after delya for 2

这是一种非常野蛮的主线程和子线程的交互方式。如果把主线程和子线程组成的线程组 比作一个团队的话，那么主线程应该是这个团队的管理者，它了解每个线程所做的事情、所 需的数据输人以及子线程结束时的输出，并把各个线程的输出组合形成有意义的结果。如果 一个团队中管理者采取这种强硬的管理方式，相信很多下属都会苦不堪言，因为不仅没有被 尊重的感觉，而且还有可能因为这种强势带来决策上的失误。实际上很多情况下我们可能希 望主线程能够等待所有子线程都完成时才退出，这时应该使用threading模块，它支持守护线 程，可以通过setDaemon()函数来设定线程的daemon属性。当daemon属性设置为True的 时候表明主线程的退出可以不用等待子线程完成。默认情况下，daemon标志为False，所有 的非守护线程结束后主线程才会结束。来看具体的例子，当daemon属性设置为False，默汄 主线程会等待所有子线程结束才会退出。将t2的daemon属性改为True之后即使t2运行未 结束主线程也会直接退出。

import threading

import time

def myfunc(azdelay):

print flI will calculate square of %s after delay for %s" % (a,delay)

time.sleep(delay)

print "calculate begins...11

result = a*a

print result

return result

tl-threading.Thread(target=myfuncr args=(2^5)) t2=threading.Thread(target=myfunc,args=(6r 8)) print tl.isDaemon()

print t2•isDaemon()

t2.setDaemon(True) tl.start () t2.start ()

\#设置守护线程



4 ) Python3中已经不存在thread模块。thread模块在Python3中被命名为_thread，这种 更改主要是为了更进一步明确表示与thread模块相关的更多的是具体的实现细节，它更多展 示的是操作系统层面的原始操作和处理。在一般的代码中不应该选择thread模块。

##### 建议49:使用Queue使多线程编程更安全

曾经有这么一个说法，程序中存在3种类型的bug:你的bug、我的bug和多线程。这 虽然是句调侃，但从某种程度上道出了一个事实：多线程编程不是件容易的事情。线程间的 同步和互斥，线程间数据的共享等这些都是涉及线程安全要考虑的问题。纵然Python中提供 了众多的同步和互斥机制，如mutex、condition、event等，但同步和互斥本身就不是一个容 易的话题，稍有不慎就会陷人死锁状态或者威胁线程安全。我们来看一个经典的多线程同步 问题：生产者消费者模型。如果用Python来实现，你会怎么写？大概思路是这样的：分别创 建消费者和生产者线程，生产者往队列里面放产品，消费者从队列里面取出产品，创建一个 线程锁以保证线程间操作的互斥性。当队列满的时候消费者进人等待状态，当队列空的时候 生产者进人等待状态。我们来看一个具体的Python实现：

import Queue import threading import random

writelock = threading.Lock()

class Producer(threading.Thread):

def _init_(self, q,con,name):



\#创建锁对象用子控制椅出



def run(self):

while 1:



super《Producer, self)._init_() self.q = q self.name = name self.con = con

print ,f Producer n+self. name+n St



Started11



global writelock self.con.acquire{) if self.q.full():



\#获取锁对象 #队列满



with writelock: # 翰出信息

print *Queue is full,producer wait!

self .con. wait ()    # 等待资源



with



else:



value = random.randint(0,10) with writelock:

print self.name 十"put value



"♦self .name十•’ ： ••+ str (value) + ninto queue"

self .q.put ((self .name+,,:,,+str (value))) # 放入队列中 self .con. notify 0    # 通知消费者

self .con. release ()    # 释放锁对象

class Consumer (threading . Thread) :    # 消费者

| def         | init (self, q,con,name):                                    |                                      |                         |
| ----------- | ----------------------------------------------------------- | ------------------------------------ | ----------------------- |
|             | super(Consumer, self)._init_()                              |                                      |                         |
|             | self.q = q self.con = conself.name = nameprint "Consumer ’’ | +self.name+H started\n               |                         |
| def         | run (self):                                                 |                                      |                         |
|             | while 1:                                                    |                                      |                         |
|             | global writelock                                            |                                      |                         |
|             | self.con                                                    | .acquire()                           |                         |
|             | if self.                                                    | q.empty():                           | #队列为空               |
|             |                                                             | with writelock:                      |                         |
|             |                                                             | print 1 queue                        | is empty,consumer wait! |
|             |                                                             | self.con.wait()                      | #等待资源               |
|             | else:                                                       | value = self.q.get() with writelock: | #获取一个元柰           |
|             |                                                             | print self .name +”get value1* +     |                         |
|             |                                                             | value ♦                              | •• from queue11         |
|             |                                                             | self.con.notify()                    | #发送消息通知生产者     |
|             | self.con                                                    | .release()                           | #释放锁对象             |
| name—       | =="_main_•’ ：                                              |                                      |                         |
| q =         | Queue.Queue(10)                                             |                                      |                         |
| con         | =threading.Condition()                                      |                                      | #条件变量锁             |
| P =         | Producer(qr conz"Pl")                                       |                                      |                         |
| p.start ()  |                                                             |                                      |                         |
| pi =        | :Producer (q^ conr ,,P2,*)                                  |                                      |                         |
| pi.start () |                                                             |                                      |                         |
| cl =        | :Consumer(q,con,"Cl")                                       |                                      |                         |

cl.start ()

上面的程序实现有什么问题吗？回答这个问题之前，我们先来了解一下Queue模块的基 本知识。Python中的Queue模块提供了 3种队列：

□    Queue.Queue(maxsize):先进先出，maxsize为队列大小，其值为非正数的时候为 无限循环队列。

□    Queue.LifoQueue(maxsize)：后进先出，相当于桟。

□    Queue.PriorityQueue(maxsize)：优先级队列。

这3种队列支持以下方法：

□    Queue.qsize():返回近似的队列大小。注意，这里之所以加“近似”二字，是因为 当该值＞0的时候并不保证并发执行的时候get()方法不被阻塞，同样，对于put()方

法有效。

□    Queue.empty()：列队为空的时候返回True,否则返回False。

□    Queue.full()：当设定了队列大小的情况下，如果队列满则返回True,否则返回False。

□    Queue.put(item[, block[, timeout]]):往队列中添加元素 item, block 设置为 False 的 时候，如果队列满则抛出Full异常。如果block设置为True, timeout为None的吋候 则会一直等待直到有空位置，否则会根据timeout的没定超吋后抛出Full异常。

□    Queue.put_nowait(item):等价于 put(item, False).block 设置为 False 的时候，如果队 列空则抛出Empty异常。如果block设置为True、timeout为None的时候则会一直等 待直到有元素可用，否则会根据timeout的设定超时后抛出Empty异常。

□    Queue.get([block[, timeout]])：从队列中删除元索并返回该元素的值。

□    Queue.get_nowait()：等价于 get(False)。

□    Queue.task_done()：发送信号表明人列任务已经完成，经常在消费者线程巾用到。

□    Queue.join()：阻塞直至队列中所有的元索处理完毕。

Queue模块实现了多个生产者多个消费者的队列，当多线程之间需要信息安全的交换的 吋候特别有用，因此这个模块实现了所需要的锁原语，为Python多线程编程提供了有力的支 持，它是线程安全的。需要注意的是Queue模块中的列队和collections.deque所表示的队列 并不一样，前者主要用于不同线程之间的通信，它内部实现了线程的锁机制；而后者主要是 数据结构上的概念，因此支持in方法。

再回过头来看看前面的例子，程序的实现有什么问题呢？答案很明显，作用于queue操 作的条件变量完全是不需要的，W为queue本身能够保证线程安全，因此不需要额外的同步 机制。那么，该如何修改呢？请读者自行思考。下面的多线程下载的例子也许有助于你完成 上面程序的修改。

import os import Queue import threading import urllib2

class DownloadThread(threading.Thread):

def



_init_(self, queue): threading.Thread， ini t

(self)



self.queue = queue

def run (self): while True:

url = self .queue, get ()    #从队列中取出一个url元素

print self • name + "begin download.’+url+" " •

self，download一file (url)    # 进行文件下载

self . queue . task_done ()    #下载完毕发送信号

print self•name十"download completed!!!H

def download_file (selff url) :    #下载文件

urlhandler = urllib2.urlopen(url)

f name = os .path .basename (url) .htmlw #文件名称 with open (fname, "wb") as f:    #打开文件

while True:

chunk = urlhandler.read(1024) if not chunk: break f.write(chunk)

if _name_ m _main_":

urls = I"<http://wiki.python.org/moin/WebProgramming>",

M <https://www>. createspace. com/3611970",

•’<http://wiki> • python.org/moin/DocumentationM

]

queue - Queue.Queue()

\# create a thread pool and give them a queue for i in range (5):

t • DownloadThread (queue)    #启动5个线程同时进行下載

t.setDaemon(True) t.start ()

\#    give the queue some data for url in urls:

queue.put(url)

\#    wait for the queue to finish queue.join ()
