## 第 4 章 文本和字节序列

人类使用文本，计算机使用字节序列。 1

——Esther Nam 和 Travis Fischer “Character Encoding and Unicode in Python”

1PyCon 2014，“Character Encoding and Unicode in Python”演讲的第 12 张幻灯片［幻灯片 (<http://www.slideshare.net/fischertrav/character-encoding-unicode-how-to-with-dignity-33352863>)，视频 ( <http://pyvideo.org/pycon-us-2014/character-encoding-and-unicode-in-python.html>) ］。

Python 3 明确区分了人类可读的文本字符串和原始的字节序列。隐式地把字节序列转换成 Unicode 文本己成过去。本章将要讨论 Unicode 字符串、二进制序列，以及在二者之间转 换时使用的编码。

深入理解 Unicode 对你可能十分重要，也可能无关紧要，这取决于 Python 编程的场景。说 到底，本章涵盖的问题对只处理 ASCII 文本的程序员没有影响。但是即便如此，也不能避 而不谈字符串和字节序列的区别。此外，你会发现专门的二进制序列类型所提供的功能，

有些是 Python 2 中“全功能”的 str 类型不具有的。

本章将讨论下述话题：

•字符、码位和字节表述

•    bytes、bytearray和memoryview等二进制序列的独特特性

•全部Unicode和陈旧字符集的编解码器

•避免和处理编码错误

•处理文本文件的最佳实践

•默认编码的陷阱和标准I/O的问题

•规范化Unicode文本，进行安全的比较

•规范化、大小写折叠和暴力移除音调符号的实用函数

•使用locale模块和PyUCA库正确地排序Unicode文本

•    Unicode数据库中的字符元数据

•能处理字符串和字节序列的双模式API

接下来先从字符、码位和字节序列开始。

### 4.1 字符问题

“字符串”是个相当简单的概念：一个字符串是一个字符序列。问题出在“字符”的定义上。

在 2015 年， “字符”的最佳定义是 Unicode 字符。因此，从 Python 3 的 str 对象中获取的 元素是 Unicode 字符，这相当于从 Python 2 的 unicode 对象中获取的元素，而不是从 Python 2 的 str 对象中获取的原始字节序列。

Unicode 标准把字符的标识和具体的字节表述进行了如下的明确区分。

•字符的标识，即码位，是0〜1 114 111的数字（十进制），在Unicode标准中以4〜6 个十六进制数字表示，而且加前缀“U+”。例如，字母A的码位是U+0041，欧元符号 的码位是U+20AC，高音谱号的码位是U+1D11E。在Unicode 6.3中（这是Python 3+4 使用的标准），约 10% 的有效码位有对应的字符。

•字符的具体表述取决于所用的编码。编码是在码位和字节序列之间转换时使用的算

法。在UTF-8编码中，A （U+0041）的码位编码成单个字节\x41，而在UTF-16LE 编码中编码成两个字节\x41\x00。再举个例子，欧元符号（U+20AC）在UTF-8编 码中是三个字节——\xe2\x82\xac，而在UTF-16LE中编码成两个字

节：\xac\x20。

把码位转换成字节序列的过程是编码；把字节序列转换成码位的过程是解码。示例 4-1

阐释了这一区分。

示例 4-1 编码和解码

\>>> s = 'cafe'

\>>> len(s) # O

4

\>>> b = s.encode('utf8') # © >>> b

b'caf\xc3\xa9' # ©

\>>> len(b) # ©

5

\>>> b.decode('utf8') # ❺ 'cafe

❶ 'cafe' 字符串有 4 个 Unicode 字符。

❷ 使用 UTF-8 把 str 对象编码成 bytes 对象。

❸ bytes 字面量以 b 开头。

❹字节序列b有5个字节（在UTF-8中，“6”的码位编码成两个字节）。 ❺ 使用 UTF-8 把 bytes 对象解码成 str 对象。

如果想帮助自己记住.deeode()和.eneode()的区别，可以把字节序列想成 晦涩难懂的机器磁芯转储，把 Unicode 字符串想成“人类可读”的文本。那么，把字节 序列变成人类可读的文本字符串就是解码，而把字符串变成用于存储或传输的字节 序列就是编码。

虽然 Python 3 的 str 类型基本相当于 Python 2 的 unieode 类型，只不过是换了个新名 称，但是 Python 3 的 bytes 类型却不是把 str 类型换个名称那么简单，而且还有关系紧 密的 bytearray 类型。因此，在讨论编码和解码的问题之前，有必要先来介绍一下二进 制序列类型。

### 4.2 字节概要

新的二进制序列类型在很多方面与 Python 2 的 str 类型不同。首先要知道， Python 内置了 两种基本的二进制序列类型：Python 3引入的不可变bytes类型和Python 2.6添加的可变 bytearray类型。（Python2.6也引入了 bytes类型，但那只不过是str类型的别名，与 Python 3 的 bytes 类型不同。）

bytes 或 bytearray 对象的各个元素是介于 0~255（含）之间的整数，而不像 Python 2 的 str 对象那样是单个的字符。然而，二进制序列的切片始终是同一类型的二进制序 列，包括长度为 1 的切片，如示例 4-2 所示。

示例 4-2 包含 5 个字节的 bytes 和 bytearray 对象

\>>> cafe = bytes('cafe', encoding='utf_8') O

\>>> cafe

b'caf\xc3\xa9'

\>>> cafe[0] ©

99

\>>> cafe[:1] © b'c'

\>>> cafe_arr = bytearray(cafe)

\>>> cafe_arr o bytearray(b'caf\xc3\xa9')

\>>> cafe_arr[-1:] bytearray(b'\xa9')

❶ bytes 对象可以从 str 对象使用给定的编码构建。

❷ 各个元素是 range（256） 内的整数。

❸ bytes 对象的切片还是 bytes 对象，即使是只有一个字节的切片。

❹ bytearray 对象没有字面量句法，而是以 bytearray（） 和字节序列字面量参数的形式

显示。

❺ bytearray 对象的切片还是 bytearray 对象。

my_bytes[0]获取的是一个整数，而my_bytes[:1]返回的是一个长度为1 的 bytes 对象——这一点应该不会让人意外。 s[0] == s[:1] 只对 str 这个序列类 型成立。不过， str 类型的这个行为十分罕见。对其他各个序列类型来说， s[i] 返 回一个元素，而 s[i:i+1] 返回一个相同类型的序列，里面是 s[i] 元素。

虽然二进制序列其实是整数序列，但是它们的字面量表示法表明其中有 ASCII 文本。因 此，各个字节的值可能会使用下列三种不同的方式显示。

•可打印的ASCII范围内的字节（从空格到~），使用ASCII字符本身。

•制表符、换行符、回车符和\对应的字节，使用转义序列\t、\n、\r和\\。

•其他字节的值，使用十六进制转义序列（例如，\x00是空字节）。

因此，在示例4-2中，我们看到的是b'eaf\xe3\xa9':前3个字节b'eaf'在可打印的 ASCII 范围内，后两个字节则不然。

除了格式化方法（format和format_map）和几个处理Unicode数据的方法（包括 easefold、 isdeeimal、 isidentifier、 isnumerie、 isprintable 和 eneode）之 外， str 类型的其他方法都支持 bytes 和 bytearray 类型。这意味着，我们可以使用熟 悉的字符串方法处理二进制序列，如 endswith、 replaee、 strip、 translate、 upper 等，只有少数几个其他方法的参数是 bytes 对象，而不是 str 对象。此外，如果正则表 达式编译自二进制序列而不是字符串， re 模块中的正则表达式函数也能处理二进制序 列。Python 3.0~3.4不能使用％运算符处理二进制序列，但是根据“PEP 461 —Adding % formatting to bytes and bytearray” （ [https ://www.python.org/dev/peps/pep-046 1/](https://www.python.org/dev/peps/pep-0461/) ） ， Python 3.5 应该会支持。

二进制序列有个类方法是str没有的，名为fromhex，它的作用是解析十六进制数字对 （数字对之间的空格是可选的），构建二进制序列：

\>>> bytes.fromhex('31 4B CE A9') b'1K\xee\xa9'

构建 bytes 或 bytearray 实例还可以调用各自的构造方法，传入下述参数。

•    一个str对象和一个eneoding关键字参数。

•    一个可迭代对象，提供0〜255之间的数值。

•    一个整数，使用空字节创建对应长度的二进制序列。[Python 3.5会把这个构造方法标 记为“过时的”，Python 3.6 会将其删除。参见“PEP 467—Minor API improvements for binary sequences” （https://www.python.org/dev/peps/pep-0467/） 。 ]

-一个实现了缓冲协议的对象（如

bytes、bytearray、memoryview、array.array）;此时，把源对象中的字节序

列复制到新建的二进制序列中。

使用缓冲类对象构建二进制序列是一种低层操作，可能涉及类型转换。示例 4-3 做了演 示。

示例 4-3 使用数组中的原始数据初始化 bytes 对象

\>>> import array

\>>> numbers = array.array('h', [-2, -1, 0, 1, 2]) O >>> oetets = bytes(numbers) &

\>>> oetets

b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00' ©

O指定类型代码h，创建一个短整数(16位)数组。

© octets 保存组成 numbers 的字节序列的副本。

© 这些是表示那 5 个短整数的 10 个字节。

使用缓冲类对象创建 bytes 或 bytearray 对象时，始终复制源对象中的字节序列。与之

相反， memoryview 对象允许在二进制数据结构之间共享内存。如果想从二进制序列中提

取结构化信息， struct 模块是重要的工具。下一节会使用这个模块处理 bytes 和

memoryview 对象。

结构体和内存视图

struct 模块提供了一些函数，把打包的字节序列转换成不同类型字段组成的元组，还有 一些函数用于执行反向转换，把元组转换成打包的字节序列。 struct 模块能处理 bytes、bytearray 和 memoryview 对象。

如 2.9.2 节所述， memoryview 类不是用于创建或存储字节序列的，而是共享内存，让你 访问其他二进制序列、打包的数组和缓冲中的数据切片，而无需复制字节序列，例如 Python Imaging Library(PIL) 2 就是这样处理图像的。

| 2Pillow ([https://pillow+readthedocs+org/en/latest/](https://pillow.readthedocs.org/en/latest/))是 PIL 最活跃的派生库。

示例 4-4 展示了如何使用 memoryview 和 struct 提取一个 GIF 图像的宽度和高度。

示例 4-4 使用 memoryview 和 struct 查看一个 GIF 图像的首部

\>>> import struct

\>>> fmt = '<3s3sHH'    # O

\>>> with open('filter.gif', 'rb') as fp:

... img = memoryview(fp.read()) # ©

\>>> header = img[:10] # ©

\>>> bytes(header) # © b'GIF89a+\x02\xe6\x00'

\>>> struct.unpack(fmt, header) # ❺ (b'GIF', b'89a', 555, 230)

\>>> del header #

\>>> del img

❶ 结构体的格式： < 是小字节序， 3s3s 是两个 3 字节序列， HH 是两个 16 位二进制整 数。

❷ 使用内存中的文件内容创建一个 memoryview 对象……

❸ ……然后使用它的切片再创建一个 memoryview 对象；这里不会复制字节序列。

❹ 转换成字节序列，这只是为了显示；这里复制了 10 字节。

❺ 拆包 memoryview 对象，得到一个元组，包含类型、版本、宽度和高度。

❻ 删除引用，释放 memoryview 实例所占的内存。

注意， memoryview 对象的切片是一个新 memoryview 对象，而且不会复制字节序列。 [ 本书的技术审校之一 Leonardo Rochael 指出，如果使用 mmap 模块把图像打开为内存映射 文件，那么会复制少量字节。本书不会讨论mmap，如果你经常读取和修改二进制文件，

可以阅读“ mmap—Memory-mapped file

support” (<https://docs.python.org/3/library/mmap.html>)来进一步学习。]

本书不会深入介绍 memoryview 和 struct 模块，如果要处理二进制数据，可以阅读它们 的文档： “Built-in Types » Memory

Views” ([https://docs.python.org/3/library/stdtypes.html#memory-views](https://docs.python.org/3/library/stdtypes.html%23memory-views))和“struct—Interpret bytes as packed binary data” (<https://docs.python.org/3/library/struct.html>) 。

简要探讨 Python 的二进制序列类型之后，下面说明如何在它们和字符串之间转换。

### 4.3 基本的编解码器

Python自带了超过100种编解码器(codec, encoder/decoder)，用于在文本和字节之间相 互转换。每个编解码器都有一个名称，如’utf_8'，而且经常有几个别名，如 'utf8'、'utf-8•和’U8'。这些名称可以传给

open()、 str.encode()、 bytes.decode() 等函数的 encoding 参数。示例 4-5 使用 3 个编解码器把相同的文本编码成不同的字节序列。

示例4-5使用3个编解码器编码字符串“El Nino”，得到的字节序列差异很大

\>>> for codec in ['latin_1', 'utf_8', 'utf_16']:

… print(codec, 'El Nino'.encode(codec), sep='\t')

latin_1 b'El Ni\xf1o' utf_8 b'El Ni\xc3\xb1o'

utf 16 b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'

图4-1展示了不同编解码器对“A”和高音谱号等字符编码后得到的字节序列。注意，后3 种是可变长度的多字节编码。

图 4-1： 12 个字符，它们的码位及不同编码的字节表述(十六进制，星号表明该编 码不支持表示该字符)

图4-1中的星号表明，某些编码(如ASCII和多字节的GB2312)不能表示所有Unicode 字符。然而， UTF 编码的设计目的就是处理每一个 Unicode 码位。

图 4-1 中展示的是一些典型编码，介绍如下。

latinl (即 iso8859_1)

一种重要的编码，是其他编码的基础，例如ep1252和Unicode (注意，latinl与 ep1252 的字节值是一样的，甚至连码位也相同)。

ep1252

Microsoft 制定的 latin1 超集，添加了有用的符号，例如弯引号和€(欧元)；有些 Windows应用把它称为“ANSI”，但它并不是ANSI标准。 ep437

IBM PC 最初的字符集，包含框图符号。与后来出现的 latin1 不兼容。 gb2312

用于编码简体中文的陈旧标准；这是亚洲语言中使用较广泛的多字节编码之一。

utf-8

目前 Web 中最常见的 8 位编码；3 与 ASCII 兼容(纯 ASCII 文本是有效的 UTF-8 文 本)。

3W3Techs 发布的'“Usage of character encodings for

websites” (<https://w3techs.com/technologies/overview/character_encoding/all>)报告指出，截至 2014 年 9 月，81.4% 的网站 使用 UTF-8;而 Built With 发布的"“Encoding Usage Statistics”(<http://trends.builtwith.com/encoding>)估计的比例则是 79.4%。

utf-16le

UTF-16 的 16 位编码方案的一种形式；所有 UTF-16 支持通过转义序列(称为“代理 对”，surrogate pair)表示超过U+FFFF的码位。

UTF-16取代了 1996年发布的Unicode 1.0编码(UCS-2)。这个编码在很多 系统中仍在使用，但是支持的最大码位是U+FFFF。从Unicode 6.3起，分配的码位中 有超过50%。在U+10000以上，包括逐渐流行的表情符号(emoji pictograph)。

概述常规的编码之后，下面要处理编码和解码过程中存在的问题。

### 4.4 了解编解码问题

虽然有个一般性的 UnicodeError 异常，但是报告错误时几乎都会指明具体的异

常：UnicodeEncodeError （把字符串转换成二进制序列时）或

UnicodeDecodeError （把二进制序列转换成字符串时）。如果源码的编码与预期不符， 加载Python模块时还可能抛出SyntaxError。接下来的几节说明如何处理这些错误。

#### 出现与Unicode有关的错误时，首先要明确异常的类型。导致编码问题的是 UnicodeEncodeError、UnicodeDecodeError，还是如 SyntaxError 的其他错

误？解决问题之前必须清楚这一点。

4.4.1 处理 UnicodeEncodeError

多数非 UTF 编解码器只能处理 Unicode 字符的一小部分子集。把文本转换成字节序列 时，如果目标编码中没有定义某个字符，那就会抛出 UnicodeEncodeError 异常，除非 把 errors 参数传给编码方法或函数，对错误进行特殊处理。处理错误的方式如示例 4-6

所示。

示例 4-6 编码成字节序列：成功和错误处理

\>>> city = 'Sao Paulo'

\>>> city.encode('utf_8') O b'S\xc3\xa3o Paulo'

\>>> city.encode('utf_16')

b'\xff\xfeS\x00\xe3\x00o\x00 \x00P\x00a\x00u\x00l\x00o\x00'

\>>> city.encode('iso8859_1') © b'S\xe3o Paulo'

\>>> city.encode('cp437') ©

Traceback (most recent call last):

File "<stdin>", line 1, in <module>

File "/.../lib/python3.4/encodings/cp437.py", line 12, in encode return codecs.charmap_encode(input,errors,encoding_map) UnicodeEncodeError: 'charmap' codec can't encode character '\xe3' in position 1: character maps to <undefined>

\>>> city.encode('cp437', errors='ignore') o b'So Paulo'

\>>> city.encode('cp437', errors='replace')❺ b'S?o Paulo'

\>>> city.encode('cp437', errors='xmlcharrefreplace') © b'Sao Paulo'

❶ 'utf_?' 编码能处理任何字符串。

❷ 'iso8859_1' 编码也能处理字符串 'Sao Paulo'。

#### ❸'cp437'无法编码’a'（带波形符的“a”）。默认的错误处理方式'strict'抛出 UnicodeEncodeError。

❹ error='ignore' 处理方式悄无声息地跳过无法编码的字符；这样做通常很是不妥。

❺编码时指定error='replace'，把无法编码的字符替换成’？’；数据损坏了，但是用

户知道出了问题。

❻ 'xmlcharrefreplace' 把无法编码的字符替换成 XML 实体。

![img](08414584Python-25.jpg)



编解码器的错误处理方式是可扩展的。你可以为errors参数注册额外的字符 串，方法是把一个名称和一个错误处理函数传给 codecs.register_error 函数。

参见 codecs.register_error 函数的文档 ([https://docs+python+org/3/library/codecs+html#codecs+register_error](https://docs.python.org/3/library/codecs.html%23codecs.register_error)) 。

4.4.2 处理 UnicodeDecodeError

不是每一个字节都包含有效的 ASCII 字符，也不是每一个字符序列都是有效的 UTF-8 或 UTF-16。因此，把二进制序列转换成文本时，如果假设是这两个编码中的一个，遇到无 法转换的字节序列时会抛出 UnicodeDecodeError。

另一方面，很多陈旧的8位编码——如'cp1252'、'iso8859_1'和’koi8_r'——能解

码任何字节序列流而不抛出错误，例如随机噪声。因此，如果程序使用错误的 8 位编码， 解码过程悄无声息，而得到的是无用输出。

![img](08414584Python-26.jpg)



乱码字符称为鬼符(gremlin)或mojibake (文字化什，“变形文本”的日文)。 示例 4-7 演示了使用错误的编解码器可能出现鬼符或抛出 UnicodeDecodeError。 示例 4-7 把字节序列解码成字符串：成功和错误处理

'MontrMal'

\>>> octets.decode('utf_8')❺

Traceback (most recent call last):

File "<stdin>", line 1, in <module>

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte

\>>> octets.decode('utf_8', errors='replace')    ©

'Montral'

❶这些字节序列是使用latinl编码的“Montreal”； ’ \xe9'字节对应1”。

❷ 可以使用 'cp1252' (Windows 1252)解码，因为它是 latin1 的有效超集。

❸ ISO-8859-7 用于编码希腊文，因此无法正确解释 '\xe9' 字节，而且没有抛出错误。

❹KOI8-R用于编码俄文；这里，'\xe9 '表示西里尔字母“H”。

❺ 'utf_8' 编解码器检测到 oetets 不是有效的 UTF-8 字符串，抛出

UnieodeDeeodeError。

❻使用’replaee'错误处理方式，\xe9替换成了“分’（码位是U+FFFD），这是官方指 定的REPLACEMENT CHARACTER （替换字符），表示未知字符。

4.4.3使用预期之外的编码加载模块时抛出的SyntaxError

Python 3默认使用UTF-8编码源码，Python 2 （从2.5开始）则默认使用ASCII。如果加载 的 .py 模块中包含 UTF-8 之外的数据，而且没有声明编码，会得到类似下面的消息：

SyntaxError: Non-UTF-8 eode starting with '\xe1' in file ola.py on line 1, but no eneoding deelared; see <http://python.org/dev/peps/pep-0263/> for details

GNU/Linux和OS X系统大都使用UTF-8，因此打开在Windows系统中使用ep1252编码 的 .py 文件时可能发生这种情况。注意，这个错误在 Windows 版 Python 中也可能会发 生，因为 Python 3 为所有平台设置的默认编码都是 UTF-8。

为了修正这个问题，可以在文件顶部添加一个神奇的 eoding 注释，如示例 4-8 所示。

示例 4-8 ola.py： “你好，世界！”的葡萄牙语版

\# eoding: ep1252 print('Ola, Mundo!')

现在，Python 3的源码不再限于使用ASCII，而是默认使用优秀的UTF-8编 码，因此要修正源码的陈旧编码（如’ep1252'）问题，最好将其转换成UTF-8，别 去麻烦eoding注释。如果你用的编辑器不支持UTF-8,那么是时候换一个了。 源码中能不能使用非 ASCII 名称

Python 3 允许在源码中使用非 ASCII 标识符：

| >>> aqao | = 'PBR' | # aqao | = stoek |
| -------- | ------- | ------ | ------- |
| >>> £ =  | 10**-6  | # £ =  | epsilon |

有些人不喜欢这么做。支持始终使用 ASCII 标识符的人认为，这样便于所有人阅读和 编辑代码。这些人没切中要害：源码应该便于目标群体阅读和编辑，而不是“所有

人”。如果代码属于跨国公司，或者是开源的，想让来自世界各地的人作贡献，那么 标识符应该使用英语，也就是说只能使用 ASCII 字符。

但是，如果你是巴西的一位老师，那么使用葡萄牙语正确拼写变量和函数名更便于学

生阅读代码。而且，这些学生在本地化的键盘中不难打出变音符号和重音元音字母。

现在，Python能解析Unicode名称，而且源码的默认编码是UTF-8,我觉得没有任何 理由使用不带重音符号的葡萄牙语编写标识符。在 Python 2 中确实不能这么做，除非

你也想使用 Python 2 运行代码，否则不必如此。如果使用葡萄牙语命名标识符却不带

重音符号的话，这样写出的代码对任何人来说都不易阅读。

这是我作为说葡萄牙语的巴西人的观点，不过我相信也适用于其他国家和文化：选择

对团队而言易于阅读的人类语言，然后使用正确的字符拼写。

假如有个文本文件，里面保存的是源码或诗句，但是你不知道它的编码。如何查明真正的

编码呢？下一节使用一个推荐的库回答这个问题。

4.4.4 如何找出字节序列的编码

如何找出字节序列的编码？简单来说，不能。必须有人告诉你。

有些通信协议和文件格式，如HTTP和XML，包含明确指明内容编码的首部。可以肯定 的是，某些字节流不是ASCII，因为其中包含大于127的字节值，而且制定UTF-8和 UTF-16 的方式也限制了可用的字节序列。不过即便如此，我们也不能根据特定的位模式 来 100% 确定二进制文件的编码是 ASCII 或 UTF-8。

然而，就像人类语言也有规则和限制一样，只要假定字节流是人类可读的纯文本，就可能 通过试探和分析找出编码。例如，如果 b'\x00' 字节经常出现，那么可能是 16 位或 32 位编码，而不是 8 位编码方案，因为纯文本中不能包含空字符；如果字节序列 b'\x20\x00'经常出现，那么可能是UTF-16LE编码中的空格字符(U+0020)，而不是 鲜为人知的 U+2000 EN QUAD 字符——谁知道这是什么呢！

统一字符编码侦测包Chardet (<https://pypi.python.org/pypi/chardet>)就是这样工作的，它能 识别所支持的 30 种编码。 Chardet 是一个 Python 库，可以在程序中使用，不过它也提供了 命令行工具chardetect。下面是它对本章书稿文件的检测报告：

$ chardetect 04-text-byte.asciidoc 04-text-byte.asciidoc: utf-8 with confidence 0.99

二进制序列编码文本通常不会明确指明自己的编码，但是 UTF 格式可以在文本内容的开 头添加一个字节序标记。参见下一节。

4.4.5 BOM:有用的鬼符

在示例 4-5 中，你可能注意到了， UTF-16 编码的序列开头有几个额外的字节，如下所

示:

\>>> u16 = 'El Nino'.eneode('utf_16')

\>>> u16

b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'

我指的是b' \xff\xfe '。这是BOM，即字节序标记（byte-order mark），指明编码时使 用 Intel CPU 的小字节序。

在小字节序设备中，各个码位的最低有效字节在前面：字母’E'的码位是U+0045 （十进 制数 69），在字节偏移的第 2 位和第 3 位编码为 69 和 0。

\>>> list(u16)

[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]

在大字节序CPU中，编码顺序是相反的；’E'编码为0和69。

为了避免混淆， UTF-16 编码在要编码的文本前面加上特殊的不可见字符 ZERO WIDTH NO-BREAK SPACE （U+FEFF）。在小字节序系统中，这个字符编码为b'\xff\xfe•（十 进制数 255, 254）。因为按照设计， U+FFFE 字符不存在，在小字节序编码中，字节序列

b'\xff\xfe'必定是ZERO WIDTH NO-BREAK SPACE，所以编解码器知道该用哪个字节

序。

UTF-16有两个变种：UTF-16LE，显式指明使用小字节序；UTF-16BE，显式指明使用大 字节序。如果使用这两个变种，不会生成 BOM：

\>>> u16le = 'El Nino'.eneode('utf_16le')

\>>> list(u16le)

[69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0] >>> u16be = 'El Nino'.eneode('utf_16be')

\>>> list(u16be)

[0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]

如果有 BOM， UTF-16 编解码器会将其过滤掉，为你提供没有前导 ZERO WIDTH NOBREAK SPACE 字符的真正文本。根据标准，如果文件使用 UTF-16 编码，而且没有 BOM，那么应该假定它使用的是UTF-16BE （大字节序）编码。然而，Intel x86架构用的 是小字节序，因此有很多文件用的是不带 BOM 的小字节序 UTF-16 编码。

与字节序有关的问题只对一个字（word）占多个字节的编码（如UTF-16和UTF-32）有 影响。 UTF-8 的一大优势是，不管设备使用哪种字节序，生成的字节序列始终一致，因此 不需要BOM。尽管如此，某些Windows应用（尤其是Notepad）依然会在UTF-8编码的 文件中添加BOM;而且，Excel会根据有没有BOM确定文件是不是UTF-8编码，否则， 它假设内容使用Windows代码页（codepage）编码。UTF-8编码的U+FEFF字符是一个三 字节序列： b'\xef\xbb\xbf' 。因此，如果文件以这三个字节开头，有可能是带有 BOM 的 UTF-8 文件。然而， Python 不会因为文件以 b'\xef\xbb\xbf' 开头就自动假定它是 UTF-8 编码的。

下面换个话题，讨论 Python 3 处理文本文件的方式。

### 4.5 处理文本文件

处理文本的最佳实践是“Unicode三明治”(如图4-2所示)。4意思是，要尽早把输入(例 如读取文件时)的字节序列解码成字符串。这种三明治中的“肉片”是程序的业务逻辑，在 这里只能处理字符串对象。在其他处理过程中，一定不能编码或解码。对输出来说，则要 尽量晚地把字符串编码成字节序列。多数 Web 框架都是这样做的，使用框架时很少接触 字节序列。例如，在Django中，视图应该输出Unicode字符串；Django会负责把响应编 码成字节序列，而且默认使用 UTF-8 编码。

Uni code三明治



4我第一次见到‘“Unicode三明治”这种说法是在Ned Batchelder在US PyCon 2012上所做的精彩演讲中：“Pragmatic Unicode”(<http://nedbatchelder.com/text/unipain/unipain.html>)。

0

//

-te ->str

解码输入的字节序列，

只处理文本，

编码输出的文本。

\1007. str



图 4-2:Unicode 三明治——目前处理文本的最佳实践

在 Python 3 中能轻松地采纳 Unicode 三明治的建议，因为内置的 open 函数会在读取文件 时做必要的解码，以文本模式写入文件时还会做必要的编码，所以调用 my_file.read() 方法得到的以及传给 my_file.write(text) 方法的都是字符串对象。 5

5Python2.6或Python 2.7用户要使用io.open()函数才能得到读写文件时自动执行的解码和编码操作。

可以看出，处理文本文件很简单。但是，如果依赖默认编码，你会遇到麻烦。

看一下示例 4-9 中的控制台会话。你能发现问题吗？

示例 4-9 一个平台上的编码问题(如果在你的机器上运行，它可能会发生，也可能 不会)

#### 问题是：写入文件时指定了 UTF-8 编码，但是读取文件时没有这么做，因此 Python 假定 要使用系统默认的编码(Windows 1252)，于是文件的最后一个字节解码成了字符

'A©'，而不是'e'。

我是在 Windows 7 中运行示例 4-9 的。在新版 GNU/Linux 或 Mac OS X 中运行同样的语句 不会出问题，因为这几个操作系统的默认编码是UTF-8,让人误以为一切正常。如果打开 文件是为了写入，但是没有指定编码参数，会使用区域设置中的默认编码，而且使用那个 编码也能正确读取文件。但是，如果脚本要生成文件，而字节的内容取决于平台或同一平

台中的区域设置，那么就可能导致兼容问题。

![img](08414584Python-28.jpg)



需要在多台设备中或多种场合下运行的代码，一定不能依赖默认编码。打开文 件时始终应该明确传入 encoding= 参数，因为不同的设备使用的默认编码可能不 同，有时隔一天也会发生变化。

示例 4-9 中有个奇怪的细节：第一个语句中的 write 函数报告写入了 4 个字符，但是下 一行读取时却得到了 5 个字符。示例 4-10 是对示例 4-9 的扩展，对这个问题以及其他细 节做了说明。

示例 4-10 仔细分析在 Windows 中运行的示例 4-9，找出并修正问题

\>>> fp = open('cafe.txt', 'w', encoding='utf_8')

\>>> fp o

<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'> >>> fp.write('cafe')

4 ©

\>>> fp.close()

\>>> import os

\>>> os.stat('cafe.txt').st_size

5 ©

\>>> fp2 = open('cafe.txt')

\>>> fp2 ©

<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'> >>> fp2.encoding ❺

'cp1252'

\>>> fp2.read()

'cafA©' ©

\>>> fp3 = open('cafe.txt', encoding='utf_8') O >>> fp3

<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'> >>> fp3.read()

'cafe' ©

\>>> fp4 = open('cafe.txt', 'rb')    0

\>>> fp4

<_io.BuferedReader name=' cafe.txt' >    ©

\>>> fp4.read() ® b'caf\xc3\xa9'

❶ 默认情况下， open 函数采用文本模式，返回一个 TextIOWrapper 对象。

❷ 在 TextIOWrapper 对象上调用 write 方法返回写入的 Unicode 字符数。

❸os.stat报告文件中有5个字节；UTF-8编码的W占两个字节，0xc3和0xa9。

❹ 打开文本文件时没有显式指定编码，返回一个 TextIOWrapper 对象，编码是区域设置 中的默认值。

❺ TextIOWrapper 对象有个 eneoding 属性；查看它，发现这里的编码是 ep1252。

❻在Windows ep1252编码中，0xc3字节是“A”（带波形符的A），0xa9字节是版权符 号。

❼ 使用正确的编码打开那个文件。

❽结果符合预期：得到的是四个Unicode字符’eafe'。

❾ 'rb' 标志指明在二进制模式中读取文件。

❿ 返回的是 BufferedReader 对象，而不是 TextIOWrapper 对象。

©读取返回的字节序列，结果与预期相符。

S除非想判断编码，否则不要在二进制模式中打开文本文件；即便如此，也应该 使用Chardet，而不是重新发明轮子（参见4.4.4节）。常规代码只应该使用二进制模 式打开二进制文件，如光栅图像。

示例 4-10 的问题是，打开文本文件时依赖默认设置。默认设置有许多来源，参见下一 节。

编码默认值：一团糟

有几个设置对 Python I/O 的编码默认值有影响，如示例 4-11 中的 default_encodings.py 脚本

所示。

示例 4-11 探索编码默认值

import sys, loeale

expressions = """

loeale.getpreferredeneoding()

type(my_file)

my_file.eneoding

sys.stdout.isatty()

sys.stdout.eneoding

sys.stdin.isatty()

sys.stdin.eneoding

sys.stderr.isatty()

sys.stderr.eneoding

sys.getdefaulteneoding()

sys.getfilesystemencoding()

my_file = open('dummy', 'w')

for expression in expressions.split(): value = eval(expression)

print(expression.rjust(30), '->', repr(value))

#### 示例 4-11 在 GNU/Linux (Ubuntu 14.04)和 OS X(Mavericks 10.9)中的输出一样，表明这

些系统中始终使用 UTF-8:

$ python3 default_encodings.py locale.getpreferredencoding() -> 'UTF-8'

type(my_file) -> <class '_io.TextIOWrapper'> my_file.encoding -> 'UTF-8' sys.stdout.isatty() -> True sys.stdout.encoding -> 'UTF-8' sys.stdin.isatty() -> True sys.stdin.encoding -> 'UTF-8' sys.stderr.isatty() -> True sys.stderr.encoding -> 'UTF-8' sys.getdefaultencoding() -> 'utf-8' sys.getfilesystemencoding() -> 'utf-8'

#### 然而，在 Windows 中的输出有所不同，如示例 4-12 所示。

#### 示例4-12在Windows 7 (SP1)巴西版中的cmd.exe中输出的默认编码；PowerShell

输出的结果相同

| Z:\>chcp O                                                   |                    |
| ------------------------------------------------------------ | ------------------ |
| Pagina de codigo ativa: 850Z:\>python default_encodings.py © |                    |
| locale.getpreferredencoding() -> 'cp1252'                    | ©                  |
| type(my_file) -> <class '                                    | io.TextIOWrapper'> |
| my_file.encoding -> 'cp1252'                                 | o                  |
| sys.stdout.isatty() -> True                                  | ❺                  |
| sys.stdout.encoding -> 'cp850' sys.stdin.isatty() -> True sys.stdin.encoding -> 'cp850'sys.stderr.isatty() -> True sys.stderr.encoding -> 'cp850' sys.getdefaultencoding() -> 'utf-8' sys.getfilesystemencoding() -> 'mbcs' | ©                  |

#### O chcp输出当前控制台激活的代码页：850。

#### ©运行default_encodings.py，把结果输出到控制台。

#### © locale.getpreferredencoding()是最重要的设置。

© 文本文件默认使用 locale.getpreferredencoding()。

❺ 输出到控制台中，因此 sys.stdout.isatty() 返回 True。 © 因此， sys.stdout.encoding 与控制台的编码相同。 如果把输出重定向到文件，如下所示：

Z:\>python default_encodings.py > encodings.log

sys.stdout.isatty() 的返回值会变成 False， sys.stdout.encoding 会设为 locale.getpreferredencoding()，在那台设备中是'cp1252'。

注意，示例 4-12 中有 4 种不同的编码。

•如果打开文件时没有指定encoding参数，默认值由 locale.getpreferredencoding() 提供(在示例 4-12 中是 'cp1252')。

•如果设定了 PYTHONIOENCODING环境变量

( [https://docs.python.org/3/using/cmdline.html#envvar-PYTHONIOENCODING](https://docs.python.org/3/using/cmdline.html%23envvar-PYTHONIOENCODING)%ef%bc%8c)[)，](https://docs.python.org/3/using/cmdline.html%23envvar-PYTHONIOENCODING)%ef%bc%8c)[ sys.stdout/stdin/st](https://docs.python.org/3/using/cmdline.html%23envvar-PYTHONIOENCODING)derr 的编码使用设定的值；否 则，继承自所在的控制台；如果输入 / 输出重定向到文件，则由

locale.getpreferredencoding() 定义。

•    Python在二进制数据和字符串之间转换时，内部使用sys.getdefaultencoding() 获得的编码； Python 3 很少如此，但仍有发生。 6 这个设置不能修改。 [1](#bookmark6)

•    sys.getfilesystemencoding()用于编解码文件名(不是文件内容)。把字符串 参数作为文件名传给 open() 函数时就会使用它；如果传入的文件名参数是字节序 列，那就不经改动直接传给OS API。“Unicode HOWTO”一文

(<https://docs.python.org/3/howto/unicode.html>)中说：“在 Windows 中，Python 使用 mbcs这个名称引用当前配置的编码。”MBCS是Multi Byte Character Set (多字节字符 集)的首字母缩写，在 Windows 中是陈旧的变长编码，如 gb2312 或 Shift_JIS， 而不是 UTF-8。 [ 关于这个话题， Stack Overflow 中有一个很好的回答“， Difference between MBCS and UTF-8 on

[Windows” ](http://stackoverflow.com/questions/3298569/difference-between-mbcs-and-utf-8-on-windows)[(http://stackoverflow.com/questions/3298569/difference-between-mbcs-and-utf-](http://stackoverflow.com/questions/3298569/difference-between-mbcs-and-utf-8-on-windows)8-on-windows)。 ]

6研究这个话题时，我在 Python 内部找不到把字节序列转换成字符串的情况。 Python 核心开发者 Antoine Pitrou 在 comp.python.devel 邮件列表中说(<http://article.gmane.org/gmane.comp.python.devel/110036>)，CPython 的内部函数“在

py3k 中很少这么做”。

在GNU/Linux和OS X中，这些编码的默认值都是UTF-8，而且多年来都是如 此，因此 I/O 能处理所有 Unicode 字符。在 Windows 中，不仅同一个系统中使用不同 的编码，还有只支持 ASCII 和 127 个额外的字符的代码页(如 'cp850' 或 'cp1252')，而且不同的代码页之间增加的字符也有所不同。因此，若不多加小 心， Windows 用户更容易遇到编码问题。

综上， locale.getpreferredencoding() 返回的编码是最重要的：这是打开文件的默 认编码，也是重定向到文件的 sys.stdout/stdin/stderr 的默认编码。然而，文档也

说道(摘录部

分， [https://docs.python.org/3/library/locale.html#locale.getpreferredencoding](https://docs.python.org/3/library/locale.html%23locale.getpreferredencoding)) ：

locale.getpreferredencoding(do_setlocale=True)

根据用户的偏好设置，返回文本数据的编码。用户的偏好设置在不同系统中的设定方

式不同，而且在某些系统中可能无法通过编程方式设置，因此这个函数返回的只是猜

测的编码……

因此，关于编码默认值的最佳建议是：别依赖默认值。

如果遵从 Unicode 三明治的建议，而且始终在程序中显式指定编码，那将避免很多问题。 可惜，即使把字节序列正确地转换成字符串， Unicode 仍有不尽如人意的地方。接下来的

两节讨论的话题对 ASCII 世界来说很简单，但是在 Unicode 领域就变得相当复杂：文本规 范化(即为了比较而把文本转换成统一的表述)和排序。

### 4.6为了正确比较而规范化Unicode字符串

因为 Unicode 有组合字符(变音符号和附加到前一个字符上的记号，打印时作为一个整

体)，所以字符串比较起来很复杂。

例如，“caft”这个词可以使用两种方式构成，分别有4个和5个码位，但是结果完全一

样：

\>>> s1 = 'eafe'

\>>> s2 = 'eafe\u0301' >>> s1, s2 ('eafe', 'eafe')

\>>> len(s1), len(s2) (4, 5)

\>>> s1 == s2 False

U+0301 是 COMBINING ACUTE ACCENT，加在“e”后面得到1”。在 Unicode 标准中，'e'

和'e\u0301'这样的序列叫“标准等价物”(canonical equivalent)，应用程序应该把它们

视作相同的字符。但是， Python 看到的是不同的码位序列，因此判定二者不相等。

这个问题的解决方案是使用 unieodedata.normalize 函数提供的 Unicode 规范化。这个 函数的第一个参数是这4个字符串中的一个：'NFC'、'NFD'、'NFKC•和’NFKD'。下面

先说明前两个。

NFC (Normalization Form C)使用最少的码位构成等价的字符串，而NFD把组合字符分 解成基字符和单独的组合字符。这两种规范化方式都能让比较行为符合预期：

\>>> from unieodedata import normalize

\>>> s1 = 'eafe' #把"e"和重音符组合在一起 >>> s2 = 'eafe\u0301'    # 分解成"e"和重音符

\>>> len(s1), len(s2)

(4, 5)

\>>> len(normalize('NFC', s1)), len(normalize('NFC', s2)) (4, 4)

\>>> len(normalize('NFD', s1)), len(normalize('NFD', s2)) (5, 5)

\>>> normalize('NFC', s1) == normalize('NFC', s2)

True

\>>> normalize('NFD', s1) == normalize('NFD', s2)

True

西方键盘通常能输出组合字符，因此用户输入的文本默认是 NFC 形式。不过，安全起 见，保存文本之前，最好使用 normalize('NFC', user_text) 清洗字符串。 NFC 也是

W3C 的“Character Model for the World Wide Web: String Matching and Searching”规范 (<https://www.w3.org/TR/charmod-norm/>)推荐的规范化形式。

使用NFC时，有些单字符会被规范成另一个单字符。例如，电阻的单位欧姆(Q)会被 规范成希腊字母大写的欧米加。这两个字符在视觉上是一样的，但是比较时并不相等，因

#### 此要规范化，防止出现意外：

\>>> from unicodedata import normalize, name >>> ohm = '\u2126'

\>>> name(ohm)

'OHM SIGN'

\>>> ohm_c = normalize('NFC', ohm)

\>>> name(ohm_c)

'GREEK CAPITAL LETTER OMEGA'

\>>> ohm == ohm_c False

\>>> normalize('NFC', ohm) == normalize('NFC', ohm_c) True

#### 在另外两个规范化形式(NFKC和NFKD)的首字母缩略词中，字母K表

示“compatibility”(兼容性)。这两种是较严格的规范化形式，对“兼容字符”有影响。虽 然 Unicode 的目标是为各个字符提供“规范的”码位，但是为了兼容现有的标准，有些字符 会出现多次。例如，虽然希腊字母表中有>”这个字母(码位是U+03BC，GREEK SMALL LETTER MU)，但是Unicode还是加入了微符号’p' (U+00B5)，以便与latinl相互转 换。因此，微符号是一个“兼容字符”。

在 NFKC 和 NFKD 形式中，各个兼容字符会被替换成一个或多个“兼容分解”字符，即便 这样有些格式损失，但仍是“首选”表述——理想情况下，格式化是外部标记的职责，不应 该由Unicode处理。下面举个例子。二分之一    (U+00BD)经过兼容分解后得到的是

三个字符序列’1/2';微符号’p' (U+00B5)经过兼容分解后得到的是小写字母 ■p' (U+03BC)。8

8微符号是“兼容字符”，而欧姆符号不是，这还真是奇怪。因此，NFC不会改动微符号，但是会把欧姆符号改成大写 的欧米加；而NFKC和NFKD会把欧姆和微符号都改成其他字符。

#### 下面是 NFKC 的具体应用：

\>>> from unicodedata import normalize, name >>> half = '%■

\>>> normalize('NFKC', half)

'1/2'

\>>> four_squared = '42'

\>>> normalize('NFKC', four_squared)

'42'

\>>> micro = 'p'

\>>> micro_kc = normalize('NFKC', micro)

\>>> micro, micro_kc

('p', 'p')

\>>> ord(micro), ord(micro_kc)

(181, 956)

\>>> name(micro), name(micro_kc)

('MICRO SIGN', 'GREEK SMALL LETTER MU')

#### 使用’1/2'替代’X'可以接受，微符号也确实是小写的希腊字母’p'，但是把’42'转换

成’42’就改变原意了。某些应用程序可以把'42'保存为’4<sup>2</sup>'，但是 normalize 函数对格式一无所知。因此， NFKC 或 NFKD 可能会损失或曲解信息，但是 可以为搜索和索引提供便利的中间表述：用户搜索 '1 / 2 inch' 时，如果还能找到包 含’X inch'的文档，那么用户会感到满意。

使用NFKC和NFKD规范化形式时要小心，而且只能在特殊情况中使用，例

如搜索和索引，而不能用于持久存储，因为这两种转换会导致数据损失。

为搜索或索引准备文本时，还有一个有用的操作，即下一节讨论的大小写折叠。

4.6.1 大小写折叠

大小写折叠其实就是把所有文本变成小写，再做些其他转换。这个功能由

str.casefold()方法(Python 3.3 新增)支持。

对于只包含 latin1 字符的字符串 s， s.casefold() 得到的结果与 s.lower() 一样，唯 有两个例外：微符号’p'会变成小写的希腊字母“^’(在多数字体中二者看起来一样)； 德语 Eszett (“sharp s”， B)会变成“ss”。

\>>> micro = '^'

\>>> name(micro)

'MICRO SIGN'

\>>> micro_cf = micro.casefold() >>> name(micro_cf)

'GREEK SMALL LETTER MU'

\>>> micro, micro_cf (’旷，’H')    "

\>>> eszett = 'B'

\>>> name(eszett)

'LATIN SMALL LETTER SHARP S'

\>>> eszett_cf = eszett.casefold() >>> eszett, eszett_cf ('B', 'ss')

自 Python 3.4 起， str.casefold() 和 str.lower() 得到不同结果的有 116 个码位。

Unicode 6.3 命名了 110 122 个字符，这只占 0.11%。

与 Unicode 相关的任何问题一样，大小写折叠是个复杂的问题，有很多语言上的特殊情 况，但是 Python 核心团队尽力提供了一种方案，能满足大多数用户的需求。 接下来的几节将使用这些规范化知识来开发几个实用的函数。

4.6.2 规范化文本匹配实用函数

由前文可知， NFC 和 NFD 可以放心使用，而且能合理比较 Unicode 字符串。对大多数应 用来说， NFC 是最好的规范化形式。不区分大小写的比较应该使用 str.casefold()。

如果要处理多语言文本，工具箱中应该有示例 4-13 中的 nfc_equal 和 fold_equal 函 数。

示例4-13 normeq.py:比较规范化Unicode字符串

Utility funetions for normalized Unieode string eomparison.

Using Normal Form C, ease sensitive:

\>>> s1 = 'eafe'

\>>> s2 = 'eafe\u0301'

\>>> s1 == s2

False

\>>> nfe_equal(s1, s2)

True

\>>> nfe_equal('A', 'a')

False

Using Normal Form C with ease folding:

\>>> s3 = 'StraBe'

\>>> s4 = 'strasse'

\>>> s3 == s4 False

\>>> nfe_equal(s3, s4)

False

\>>> fold_equal(s3, s4)

True

\>>> fold_equal(s1, s2)

True

\>>> fold_equal('A', 'a')

True

from unieodedata import normalize def nfe_equal(str1, str2):

return normalize('NFC', str1) == normalize('NFC', str2)

def fold_equal(str1, str2):

return (normalize('NFC', str1).easefold() ==

normalize('NFC', str2).easefold())

除了 Unicode 规范化和大小写折叠（二者都是 Unicode 标准的一部分）之外，有时需要进 行更为深入的转换，例如把'eafe'变成’eafe'。下一节说明何时以及如何进行这种转 换。

4.6.3 极端“规范化”：去掉变音符号

Google 搜索涉及很多技术，其中一个显然是忽略变音符号（如重音符、下加符等），至 少在某些情况下会这么做。去掉变音符号不是正确的规范化方式，因为这往往会改变词的

意思，而且可能误判搜索结果。但是对现实生活却有所帮助：人们有时很懒，或者不知道

怎么正确使用变音符号，而且拼写规则会随时间变化，因此实际语言中的重音经常变来变

去。

除了搜索，去掉变音符号还能让 URL 更易于阅读，至少对拉丁语系语言是如此。下面是

维基百科中介绍圣保罗市（Sao Paulo）的文章的URL：

<http://en.wikipedia.org/wiki/S%C3%A3o_Paulo>

#### 其中，“°%C3°%A3”是UTF-8编码“a”字母(带有波形符的“a”)转义后得到的结果。下述形

式更友好，尽管拼写是错误的：

<http://en.wikipedia.org/wiki/Sao_Paulo>

#### 如果想把字符串中的所有变音符号都去掉，可以使用示例 4-14 中的函数

#### 示例 4-14 去掉全部组合记号的函数(在 sanitize.py 模块中)

import unicodedata import string

def shave_marks(txt):

"""去掉全部变音符号 """

norm_txt = unicodedata.normalize('NFD', txt) O shaved = ''.join(c for c in norm_txt

if not unicodedata.combining(c)) © return unicodedata.normalize('NFC', shaved) &

#### O 把所有字符分解成基字符和组合记号。

#### © 过滤掉所有组合记号。

#### o重组所有字符。

示例 4-15 是 shave_marks 函数的两个使用示例。

示例 4-15 示例 4-14 中 shave_marks 函数的两个使用示例

\>>> order = '“Herr VoB: • X cup of OEtker™ caffe latte • bowl of aqai.”'

\>>> shave_marks(order)

'“Herr VoB: • X cup of OEtker™ caffe latte • bowl of acai.”' O >>> Greek = 'Zs^upo^, Zefiro'

\>>> shave_marks(Greek)

■Zs中upoq, Zefiro' ©

#### O只替换了 ％”“?”和“r三个字符。

© T和T都被替换了。

示例 4-14 中定义的 shave_marks 函数使用起来没问题，但是也许做得太多了。通常，去 掉变音符号是为了把拉丁文本变成纯粹的ASCII，但是shave_marks函数还会修改非拉 丁字符(如希腊字母)，而只去掉重音符并不能把它们变成 ASCII 字符。因此，我们应该 分析各个基字符，仅当字符在拉丁字母表中时才删除附加的记号，如示例 4-16 所示。

#### 示例4-16删除拉丁字母中组合记号的函数(import语句省略了，因为这是示例4-14 中定义的 sanitize.py 模块的一部分)

def shave_marks_latin(txt):

"""把拉丁基字符中所有的变音符号删除"""

norm_txt = unicodedata.normalize('NFD', txt) O

latin_base = False

keepers = []

for c in norm_txt:

if unicodedata.combining(c) and latin_base: © continue # 忽略拉丁基字符上的变音符号

keepers.append(c)    ©

\# 如果不是组合字符，那就是新的基字符 if not unicodedata.combining(c):    ©

latin_base = c in string.ascii_letters shaved = ''.join(keepers)

return unicodedata.normalize('NFC', shaved) ❺

#### O 把所有字符分解成基字符和组合记号。

#### © 基字符为拉丁字母时，跳过组合记号。

#### © 否则，保存当前字符。

#### © 检测新的基字符，判断是不是拉丁字母。

#### ❺ 重组所有字符。

更彻底的规范化步骤是把西文文本中的常见符号(如弯引号、长破折号、项目符号，等 等)替换成 ASCII 中的对等字符。示例 4-17 中的 asciize 函数就是这么做的。

示例 4-17 把一些西文印刷字符转换成 ASCII 字符(这个代码片段也是示例 4-14 中 sanitize.py 模块的一部分)

single_map = str.maketrans("""‘'“”•--'>""", O

llllll’fll*八<’’1111    >1111")

multi_map = str.maketrans({ ©

'€': '<euro>',

'…': '...',

'OE': 'OE',

'™': '(TM)',

'oe': 'oe',

■知'：'<per mille>',

’j ’：    '**'

})

multi_map.update(single_map) ©

def dewinize(txt):

"""把Win1252符号替换成ASCII字符或序列 return txt.translate(multi_map) ©

z

i

i

e

s

a

ef

d

no_marks = shave_marks_latin(dewinize(txt))

no_marks = no_marks.replaee('B', 'ss') return unieodedata.normalize('NFKC', no_marks)

❶ 构建字符替换字符的映射表。

❷ 构建字符替换字符串的映射表。

❸ 合并两个映射表。

❹ dewinize 函数不影响 ASCII 或 latin1 文本，只替换 Microsoft 在 ep1252 中为 latin1 额外添加的字符。

❺ 调用 dewinize 函数，然后去掉变音符号。

❻把德语Eszett替换成“ss”（这里没有使用大小写折叠，因为我们想保留大小写）。 ❼ 使用 NFKC 规范化形式把字符和与之兼容的码位组合起来。

示例 4-18 是 aseiize 函数的使用示例。

示例 4-18 示例 4-17 中 aseiize 函数的使用示例

\>>> order = '“Herr VoB: • % eup of OEtker™ eaffe latte • bowl of aqai.” >>> dewinize(order)

'"Herr VoB: - % eup of OEtker(TM) eaffe latte - bowl of aqai."' O >>> aseiize(order)

'"Herr Voss: - 1/2 eup of OEtker(TM) eaffe latte - bowl of aeai."' ©

O dewinize函数替换弯引号、项目符号和™ （商标符号）。

© aseiize 函数调用 dewinize 函数，去掉变音符号，还会替换 'B'。

不同语言删除变音符号的规则也有所不同。例如，德语把’U'变成’ue'。 我们定义的 aseiize 函数没这么精确，因此可能适合你的语言，也可能不适合。不

过，它对葡萄牙语的处理是可接受的。

综上， sanitize.py 中的函数做的事情超出了标准的规范化，而且会对文本做进一步处理，

很有可能会改变原意。只有知道目标语言、目标用户群和转换后的用途，才能确定要不要

做这么深入的规范化。

我们对 Unicode 文本规范化的讨论到此结束。

接下来要解决的 Unicode 问题是……排序。

### 4.7 Unicode文本排序

Python 比较任何类型的序列时，会一一比较序列里的各个元素。对字符串来说，比较的是 码位。可是在比较非 ASCII 字符时，得到的结果不尽如人意。

下面对一个生长在巴西的水果的列表进行排序：

\>>> fruits = ['caju', 'atemoia', 'caja', 'aqai', 'acerola']

\>>> sorted(fruits)

['acerola', 'atemoia', 'aqai', 'caju', 'caja']

不同的区域采用的排序规则有所不同，葡萄牙语等很多语言按照拉丁字母表排序，重音符

号和下加符对排序几乎没什么影响。9因此，排序时“cajF视作“caja”，必定排在“caju”前 面。

9变音符号对排序有影响的情况很少发生，只有两个词之间唯有变音符号不同时才有影响。此时，带有变音符号的词排 在常规词的后面。

排序后的 fruits 列表应该是：

['aqai', 'acerola', 'atemoia', 'caja', 'caju']

在 Python 中，非 ASCII 文本的标准排序方式是使用 locale.strxfrm 函数，根据 locale 模块的文档([https://docs.python.org/3/library/locale.html?highlight=strxfrm#locale.strxfrm](https://docs.python.org/3/library/locale.html?highlight=strxfrm%23locale.strxfrm))，

这 个函数会“把字符串转换成适合所在区域进行比较的形式”。

使用 locale.strxfrm 函数之前，必须先为应用设定合适的区域设置，还要祈祷操作系 统支持这项设置。在区域设为 pt_BR 的 GNU/Linux(Ubuntu 14.04)中，可以使用示例 419 中的命令。

示例 4-19 使用 locale.strxfrm 函数做排序键

\>>> import locale

\>>> locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')

'pt_BR.UTF-8'

\>>> fruits = ['caju', 'atemoia', 'caja', 'aqai', 'acerola']

\>>> sorted_fruits = sorted(fruits, key=locale.strxfrm)

\>>> sorted_fruits

['aqai', 'acerola', 'atemoia', 'caja', 'caju']

因此，使用 locale.strxfrm 函数做排序键之前，要调用 setlocale(LC_COLLATE, «your_locale»)。

不过，有几点要注意。

•区域设置是全局的，因此不推荐在库中调用setlocale函数。应用或框架应该在进

程启动时设定区域设置，而且此后不要再修改。

•操作系统必须支持区域设置，否则setlocale函数会抛出locale.Error: unsupported locale setting 异常。

•必须知道如何拼写区域名称。它在Unix衍生系统中几乎己经形成标准，要通过 'language_code.encoding' 获取。 10 但是在 Windows 中，句法复杂一 些：Language Name-Language Variant_Region Name.codepage。注 意，“Language Name” (语言名称)、“Language Variant” (语言变体)和“Region Name” (区域名)中可以包含空格；除了第一部分之外，其他部分的前面是不同的字 符：一个连字符、一个下划线和一个点号。除了语言名称之外，其他部分好像都是可 选的。例如，English_United States.850，它的语言名称是“English”，区域 是“United States”，代码页是“850”。Windows能理解的语言名称和区域名见于MSDN 中的文章“Language Identifier Constants and Strings” ([https://msdn.microsoft.com/en-us/library/dd318693.aspx](https://msdn.microsoft.com/en-us/library/dd318693.aspx)%ef%bc%8c%e8%bf%98%e6%9c%89%e2%80%9cCode)[)，还有“Code](https://msdn.microsoft.com/en-us/library/dd318693.aspx)%ef%bc%8c%e8%bf%98%e6%9c%89%e2%80%9cCode)[ Page Identifiers” (https://msdn.microsoft.com/en-](https://msdn.microsoft.com/en-us/library/windows/desktop/dd317756(v=vs.85).aspx)us/library/windows/desktop/dd317756(v=vs.85).aspx) 一文列出了最后一部分的代码页 数字。 11

•操作系统的制作者必须正确实现了所设的区域。我在Ubuntu 14.04中成功了，但在 OS X(Mavericks 10.9)中却失败了。在两台 Mac 中，调用 setlocale(LC_COLLATE, 'pt_BR.UTF-8') 返回的都是字符串 'pt_BR.UTF-8'， 没有任何问题。但是， sorted(fruits, key=locale.strxfrm) 得到的结果与 sorted(fruits) 一样，是错误的。我还在 OS X 中尝试了 fr_FR、 es_ES 和 de_DE，但是 locale.strxfrm 并未起作用。12

10在Linux操作系统中，中国大陆的读者可以使用zh_CN.UTF-8,简体中文会按照汉语拼音顺序进行排序，它也能对 葡萄牙语进行正确排序。一编者注

I11感谢Leonardo Rochael，他所做的工作超出了身为技术审校的职责，虽然他是GNU/Linux用户，但却研宄了这些 Windows 细节。

12同样，我没找到解决方案，不过却发现其他人也报告了同样的问题。本书技术审校之一 Alex Martel!在他装有OS X 10.9的Mac电脑中使用setlocale和locale.strxfrm时没有遇到问题。综上：结果因人而异。

因此，标准库提供的国际化排序方案可用，但是似乎只支持GNU/Linux (可能也支持 Windows，但你得是专家)。即便如此，还要依赖区域设置，而这会为部署带来问题。 幸好，有个较为简单的方案： PyPI 中的 PyUCA 库。

使用Unicode排序算法排序

James Tauber，一位高产的Django贡献者，他一定是感受到了这一痛点，因此开发了 PyUCA 库(<https://pypi.python.org/pypi/pyuca/>)，这是 Unicode 排序算法(Unicode Collation Algorithm， UCA)的纯Python实现。示例4-20展示了它的简单用法。

示例 4-20 使用 pyuca.Collator.sort_key 方法

\>>> import pyuea

\>>> eoll = pyuea.Collator()

\>>> fruits = ['eaju', 'atemoia', 'eaja', 'aqai', 'aeerola'] >>> sorted_fruits = sorted(fruits, key=eoll.sort_key)

\>>> sorted_fruits

['aqai', 'aeerola', 'atemoia', 'eaja', 'eaju']

这样做更友好，而且恰好可用。我在 GNU/Linux、 OS X 和 Windows 中做过测试。目前， PyUCA 只支持 Python 3.x。 13

132015 年 5 月，PyUCA 重新支持 Python 2.x，参见：http://jktauber.com/2015/05/13/pyuca-supports-python-2-again。-编

者注

PyUCA 没有考虑区域设置。如果想定制排序方式，可以把自定义的排序表路径传给 Collator() 构造方法。 PyUCA 默认使用项目自带的

allkeys .txt (<https://github.com/jtauber/pyuca>)，这就是 Unicode 6.3.0 的“Default Unicode Collation Element Table” (<http://www.unicode.org/Public/UCA/6.3.0/allkeys.txt>)的副本。 顺便说一下，那个表是 Unicode 数据库中众多表中的一个。下一节会讨论这个话题。

Unicode 标准提供了一个完整的数据库(许多格式化的文本文件)，不仅包括码位与字符 名称之间的映射，还有各个字符的元数据，以及字符之间的关系。例如， Unicode 数据库 记录了字符是否可以打印、是不是字母、是不是数字，或者是不是其他数值符号。字符串

的 isidentifier、isprintable、isdecimal 和 isnumeric 等方法就是靠这些信息作 判断的。 str.casefold 方法也用到了 Unicode 表中的信息。

unicodedata 模块中有几个函数用于获取字符的元数据。例如，字符在标准中的官方名称是 不是组合字符(如结合波形符构成的变音符号等)，以及符号对应的人类可读数值(不是 码位)。示例 4-21 展示了 unicodedata.name() 和 unicodedata.numeric() 函数，以 及字符串的 .isdecimal() 和 .isnumeric() 方法的用法。

示例 4-21 Unicode 数据库中数值字符的元数据示例(各个标号说明输出中的各列)

import unicodedata import re

re_digit = re.compile(r'\d')

sample = '1\xbc\xb2\u0969\u136b\u216b\u2466\u2480\u3285'

for char in sample:

print('U+%04x' % ord(char), char.center(6),

're_dig' if re_digit.match(char) else '-', 'isdig' if char.isdigit() else '-',

'isnum' if char.isnumeric() else '-', format(unicodedata.numeric(char), '5.2f'), unicodedata.name(char),

sep='\t')

O U+0000 格式的码位。

© 在长度为 6 的字符串中居中显示字符。

O如果字符匹配正则表达式r'\d'，显示re_dig。

0 如果 char.isdigit()返回 True，显示 isdig。

© 如果 char.isnumeric()返回 True，显示 isnum。

©使用长度为5、小数点后保留2位的浮点数显示数值。 © Unicode标准中字符的名称。

运行示例 4-21 得到的结果如图 4-3 所示。

图 4-3 中的第 6 列是在字符上调用 unieodedata.numerie(ehar) 函数得到的结果。这 表明， Unicode 知道表示数字的符号的数值。因此，如果你想创建一个支持泰米尔数字和

罗马数字的电子表格应用，那就尽管去做吧！

| $ python3 numeri.cs_demo.py |      |        |       |       |       |                               |
| --------------------------- | ---- | ------ | ----- | ----- | ----- | ----------------------------- |
| U+0031                      | 1    | re_dig | isdig | isnum | 1.00  | DIGIT ONE                     |
| U+00bc                      | %    |        |       | isnum | 0.25  | VULGAR FRACTION ONE QUARTER   |
| U+00b2                      | z    |        | isdig | isnum | 2.00  | SUPERSCRIPT TWO               |
| U+0969                      | 廷   | re_dig | isdig | isnum | 3.00  | DEVANAGARI DIGIT THREE        |
| U+136b                      | r    |        | isdig | isnum | 3.00  | ETHIOPIC DIGIT THREE          |
| U+216b                      | XII  |        |       | isnum | 12.00 | ROMAN NUMERAL TWELVE          |
| U+2466                      | ⑦    |        | isdig | isnum | 7.00  | CIRCLED DIGIT SEVEN           |
| U+2480                      | o    |        |       | isnum | 13.00 | PARENTHESIZED NUMBER THIRTEEN |
| U+3285$■                    | ㊅   |        |       | isnum | 6.00  | CIRCLED IDEOGRAPH SIX         |

图4-3: 9个数值字符及其元数据；re_dig表示字符匹配正则表达式r'\d'

图 4-3 表明，正则表达式 r'\d' 能匹配数字“1”和梵文数字 3，但是不能匹配 isdigit 方 法判断为数字的其他字符。 re 模块对 Unicode 的支持并不充分。 PyPI 中有个新开发的

regex 模块，它的最终目的是取代 re 模块，以提供更好的 Unicode 支持。 14 下一节会回 过头来讨论 re 模块。

14不过在这个示例中，它在识别数字方面的表现没有re模块好。

本章使用了 unieodedata 模块中的几个函数，但是还有很多没有用到。详情参阅标准库 文档对 unieodedata 模块的说明([https://docs.python+org/3/library/unicodedata.html](https://docs.python.org/3/library/unicodedata.html))。

在结束对字符串和字节序列的讨论之前，我们还要简要说明一个新的趋势——双模式

API，即提供的函数能接受字符串或字节序列为参数，然后根据类型进行特殊处理。

### 4.9支持字符串和字节序列的双模式API

标准库中的一些函数能接受字符串或字节序列为参数，然后根据类型展现不同的行 为。 re 和 os 模块中就有这样的函数。

4.9.1 正则表达式中的字符串和字节序列

如果使用字节序列构建正则表达式， \d 和 \w 等模式只能匹配 ASCII 字符；相比之下，如 果是字符串模式，就能匹配 ASCII 之外的 Unicode 数字或字母。示例 4-22 和图 4-4 展示了 字符串模式和字节序列模式中字母、 ASCII 数字、上标和泰米尔数字的匹配情况。

示例4-22 ramanujan.py:比较简单的字符串正则表达式和字节序列正则表达式的行 为

import re

re_numbers_str = re.compile(r'\d+') O re_words_str = re.compile(r'\w+') re_numbers_bytes = re.compile(rb'\d+') © re_words_bytes = re.compile(rb'\w+')

text_str = (lRamanujan saw \u0be7\u0bed\u0be8\u0befl © "as 1729 = 13 + 123 = 93 + 103.")    ©

text_bytes = text_str.encode('utf_8') ❺

print('Text', repr(text_str), sep='\n ') print('Numbers')



print(' str : print(' bytes: print('Words') print(' str : print(' bytes:



re_numbers_str.findall(text_str))

re_numbers_bytes.findall(text_bytes))

re_words_str.findall(text_str))

re_words_bytes.findall(text_bytes))



©

&

©

&



❶ 前两个正则表达式是字符串类型。

❷ 后两个正则表达式是字节序列类型。

❸ 要搜索的 Unicode 文本，包括 1729 的泰米尔数字（逻辑行直到右括号才结束）。

❹ 这个字符串在编译时与前一个拼接起来(参见 Python 语言参考手册中的“2.4.2. String [literal concatenation”](https://docs.python.org/3/reference/lexical_analysis.html%23string-literal-concatenation)[， https://docs.python.org/3/reference/lexical_analysis.html#string-literal-](https://docs.python.org/3/reference/lexical_analysis.html%23string-literal-concatenation)concatenation)。

❺ 字节序列只能用字节序列正则表达式搜索。

❻ 字符串模式 r'\d+' 能匹配泰米尔数字和 ASCII 数字。

❼ 字节序列模式 rb'\d+' 只能匹配 ASCII 字节中的数字。

❽ 字符串模式 r'\w+' 能匹配字母、上标、泰米尔数字和 ASCII 数字。

❾ 字节序列模式 rb'\w+' 只能匹配 ASCII 字节中的字母和数字。

© O O    1. bash    Ka

$ python3 ramanujan.py I Text

Ramanujan saw 娜企脉 as 1729 = l3 + 123 = 93 + 103,’

Numbers

str :[’ £b61^.€t» ’，，1729,，’r，.121，f9.，'101] bytes: [b.17291, b'l1, b'lZ., bT9\ b'lO1]

Words

str : E'Ramanujan1, ’saw，，’    \ ’as\ l172^, 'I3',    , f931, 'lG3.]

bytes: [b1 Ramanujan1, t/saw1，b'as1,卜1729\ b'l1, b'lZ1，b'91, b'101]

$ ■    I

图 4-4：运行示例 4-22 中的 ramanujan.py 脚本时的截图

示例 4-22 是随便举的例子，为的是说明一个问题：可以使用正则表达式搜索字符串和字

节序列，但是在后一种情况中， ASCII 范围外的字节不会当成数字和组成单词的字母。

字符串正则表达式有个 re.ASCII 标志，它让 \w、 \W、 \b、 \B、 \d、 \D、 \s 和 \S 只匹

配 ASCII 字符。详情参阅 re 模块的文档([https://docs+python+org/3/library/re+html](https://docs.python.org/3/library/re.html))。

另一个重要的双模式模块是 os。

4.9.2 os函数中的字符串和字节序列

GNU/Linux内核不理解Unicode，因此你可能发现了，对任何合理的编码方案来说，在文 件名中使用字节序列都是无效的，无法解码成字符串。在不同操作系统中使用各种客户端

的文件服务器，在遇到这个问题时尤其容易出错。

为了规避这个问题， os 模块中的所有函数、文件名或路径名参数既能使用字符串，也能 使用字节序列。如果这样的函数使用字符串参数调用，该参数会使用 sys.getfilesystemencoding() 得到的编解码器自动编码，然后操作系统会使用相同 的编解码器解码。这几乎就是我们想要的行为，与 Unicode 三明治最佳实践一致。

但是，如果必须处理(也可能是修正)那些无法使用上述方式自动处理的文件名，可以把 字节序列参数传给 os 模块中的函数，得到字节序列返回值。这一特性允许我们处理任何 文件名或路径名，不管里面有多少鬼符，如示例 4-23 所示。

示例 4-23 把字符串和字节序列参数传给 listdir 函数得到的结果

O第二个文件名是“digits-of-n.txt”(有一个希腊字母n)。

© 参数是字节序列， listdir 函数返回的文件名也是字节序列： b'\xcf\x80' 是希腊字

母n的UTF-8编码。

为了便于手动处理字符串或字节序列形式的文件名或路径名， os 模块提供了特殊的编码 和解码函数。

fsencode(filename)

如果 filename 是 str 类型(此外还可能是 bytes 类型)，使用 sys.getfilesystemencoding() 返回的编解码器把 filename 编码成字节序列；否则， 返回未经修改的 filename 字节序列。

fsdecode(filename)

如果 filename 是 bytes 类型(此外还可能是 str 类型)，使用 sys.getfilesystemencoding() 返回的编解码器把 filename 解码成字符串；否则，返 回未经修改的 filename 字符串。

在 Unix 衍生平台中，这些函数使用 surrogateescape 错误处理方式(参见下述附注 栏)以避免遇到意外字节序列时卡住。 Windows 使用的错误处理方式是 strict。

使用 surrogateescape 处理鬼符

Python 3.1 引入的 surrogateescape 编解码器错误处理方式是处理意外字节序列或 未知编码的一种方式，它的说明参见“PEP 383 — Non-decodable Bytes in System Character Interfaces” (<https://www.python.org/dev/peps/pep-0383/>) 。

这种错误处理方式会把每个无法解码的字节替换成 Unicode 中 U+DC00 到 U+DCFF 之 间的码位(Unicode标准把这些码位称为“Low Surrogate Area”)，这些码位是保留 的，没有分配字符，供应用程序内部使用。编码时，这些码位会转换成被替换的字节 值，如示例 4-24 所示。

示例 4-24 使用 surrogateescape 错误处理方式

\>>> os.listdir('.') O ['abc.txt', 'digits-of-n.txt']

\>>> os.listdir(b'.') ©

[b'abc.txt', b'digits-of-\xcf\x80.txt']

\>>> pi_name_bytes = os.listdir(b'.')[1] ©

\>>> pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape') © >>> pi_name_str ❺

'digits-of-\udccf\udc80.txt'

\>>> pi_name_str.encode('ascii', 'surrogateescape') © b'digits-of-\xcf\x80.txt

O列出目录里的文件，有个文件名中包含非ASCII字符。

©假设我们不知道编码，获取文件名的字节序列形式。

© pi_names_bytes是包含n的文件名。

O使用’aseii'编解码器和’surrogateeseape'错误处理方式把它解码成字符

串。

©各个非ASCII字节替换成代替码位：'\xef\x80'变成了’\udeef\ude80'。

©编码成ASCII字节序列：各个代替码位还原成被替换的字节。

我们对字符串和字节序列的探讨到此结束。如果你坚持读到这里，恭喜你！

### 4.10 本章小结

本章首先澄清了人们对一个字符等于一个字节的误解。随着 Unicode 的广泛使用（80% 的 网站己经使用UTF-8），我们必须把文本字符串与它们在文件中的二进制序列表述区分 开，而 Python 3 中这个区分是强制的。

对 bytes、bytearray 和 memoryview 等二进制序列数据类型做了简要概述之后，我们 转到了编码和解码话题，通过示例展示了重要的编解码器；随后讨论了如何避免和处理臭

名昭著的 UnieodeEneodeError 和 UnieodeDeeodeError，以及由于 Python 源码文件编

码错误导致的 SyntaxError。

讨论源码的编码问题时，我表明了自己对非 ASCII 标识符的观点：如果代码基的维护者想 使用包含非 ASCII 字符的人类语言命名标识符，那就去做，除非还想在 Python 2 中运行代

码。但是，如果项目想吸引世界各国的贡献者，那么标识符应该使用英语单词，此时

ASCII 就够用了。

然后，我们说明了在没有元数据的情况下检测编码的理论和实际情况：理论上，做不到这 一点；但是实际上， Chardet 包能够正确处理一些流行的编码。随后介绍了字节序标记， 这是 UTF-16 和 UTF-32 文件中常见的编码提示，某些 UTF-8 文件中也有。

随后的一节演示了如何打开文本文件，这是一项简单的任务，不过有个陷阱：打开文本文 件时， eneoding= 关键字参数不是必需的，但是应该指定。如果没有指定编码，那么程 序会想方设法生成“纯文本”，如此一来，不一致的默认编码就会导致跨平台不兼容性。然

后，我们说明了 Python 用作默认值的几个编码设置，以及如何检测它

们： loeale.getpreferredeneoding（）、sys.getfilesystemeneoding（）、sys.getd 以及标准I/O文件（如sys.stdout.eneoding）的编码。对Windows用户来说，现实不

容乐观：这些设置在同一台设备中往往有不同的值，而且各个设置相互不兼容。而对

GNU/ Linux 和 OS X 用户来说，情况就好多了，几乎所有地方使用的默认值都是 UTF-8。

文本比较是个异常复杂的任务，因为 Unicode 为某些字符提供了不同的表示，所以匹配文

本之前一定要先规范化。说明规范化和大小写折叠之后，我们提供了几个实用函数，你可

以根据自己的需求改编。其中有个函数所做的是极端转换，比如去掉所有重音符号。随 后，我们说明了如何使用标准库中的 loeale 模块正确地排序 Unicode 文本（有一些注意 事项）；此外，还可以使用外部的 PyUCA 包，从而无需依赖捉摸不定的区域配置。

最后简要介绍了 Unicode 数据库（包含每个字符的元数据），还简单讨论了双模式 API （例如re和os模块，这两个模块中的某些函数可以接受字符串或字节序列参数，返

回不同但合适的结果）。

[1](#footnote1)

Python 2 对 sys.setdefaultencoding 函数的使用方式不当， Python 3 的文档中已经没有这个函数。这个函数是供核 心开发者使用的，用于在内部的默认编码未定时设置编码。在 comp.python.devel 邮件列表的那个话题中 (<http://article.gmane.org/gmane.comp.python.devel/109916>)， Marc-Andre Lemburg 说，用户代码一定不能调用 sys.setdefaultencoding函数，而且对CPython来说，它的值在Python 2中只能是’ascii'，在Python 3中只能是 'utf-8' 。



### 4.11 延伸阅读

Ned Batchelder 在 2012 年的 PyCon US 所做的演讲“Pragmatic Unicode—or—How Do I Stop the Pain?”([http://nedbatchel der+ com/text/unipain+html ](http://nedbatchelder.com/text/unipain.html))非常出色。 Ned 很专业，除了幻灯片 和视频之外，他还提供了完整的文字记录。 Esther Nam 和 Travis Fischer 在 PyCon 2014 做 了一场精影的演讲：“Character encoding and Unicode in Python: How to ( J ° □ °)

[J](http://www.slideshare.net/fischertrav/character-encoding-unicode-how-to-with-dignity-33352863)[-](http://www.slideshare.net/fischertrav/character-encoding-unicode-how-to-with-dignity-33352863)[L with dignity”［幻灯片(http://www+slideshare+net/fischertrav/character-encoding-](http://www.slideshare.net/fischertrav/character-encoding-unicode-how-to-with-dignity-33352863)

unicode-how-to-with-dignity-33352863)，视步页(http://pyvideo+org/pycon-us-2014/character-encoding-and-unicode-in-python+html[ ](http://pyvideo.org/pycon-us-2014/character-encoding-and-unicode-in-python.html)[) ］。本章开头那句简短有力的话就是出自这次演](http://pyvideo.org/pycon-us-2014/character-encoding-and-unicode-in-python.html) 讲： “人类使用文本，计算机使用字节序列。 ”本书的技术审校之一 Lennart Regebro 在“Unconfusing Unicode: What Is

Unicode?” ([https://regebro+wordpress+com/2011/03/23/unconfusing-unicode-what-is-unicode/](https://regebro.wordpress.com/2011/03/23/unconfusing-unicode-what-is-unicode/)) 这篇短文中提出了“Useful Mental Model of Unicode (UMMU) ”。Unicode 是个复杂的标 准， Lennart 提出的 UMMU 是个很好的切入点。

Python 文档中的“Unicode HOWTO”一文([https://docs+python+org/3/howto/unicode+html](https://docs.python.org/3/howto/unicode.html))从几 个不同的角度对本章所涉及的话题做了讨论，从编码历史到句法细节、编解码器、正则表 达式、文件名和 Unicode 的 I/O 最佳实践(即 Unicode 三明治)，而且各节都给出了大量 参考资料链接。Dive into Python 3是一本非常优秀的书(Mark Pilgrim

著， [http: //www+diveintopython3 +net](http://www.diveintopython3.net)) ，其中第 4

章“Strings”([http://www+diveintopython3+net/strings+html](http://www.diveintopython3.net/strings.html))对 Python 3 对 Unicode 的支持做了 [很好的介绍。此外，该书的第 ](http://getpython3.com/diveintopython3/case-study-porting-chardet-to-python-3.html)[15 章( http: //getpython3 +com/diveintopython3/case-study-](http://getpython3.com/diveintopython3/case-study-porting-chardet-to-python-3.html)porting-chardet-to-python-3 +html)阐述了 Chardet 库从 Python 2 移植到 Python 3 的过程，这

是一个宝贵的案例分析，从中可以看出，从旧的 str 类型转到新的 bytes 类型是造成迁 移如此痛苦的主要原因，也是检测编码的库应该关注的重点。

如果你用过Python 2，但是刚接触Python 3，可以阅读Guido van Rossum写的“What's New [in Python 3 +0” ](https://docs.python.org/3.0/whatsnew/3.0.html%23text-vs-data-instead-of-unicode-vs-8-bit)[( https: //docs+python+org/3 +0/whatsnew/3 +0+html#text-vs-data-instead-of-unicode-](https://docs.python.org/3.0/whatsnew/3.0.html%23text-vs-data-instead-of-unicode-vs-8-bit)vs-8-bit)，这篇文章简要列出了新版的15点变化，而且附有很多链接。Guido开门见山 地说道： “你自以为知道的二进制数据和 Unicode 知识全都变了。 ”Armin Ronacher 的博客 [文章](http://lucumr.pocoo.org/2013/7/2/the-updated-guide-to-unicode/)[“The Updated Guide to Unicode on Python” (http://lucumr+pocoo+org/2013/7/2/the-updated-](http://lucumr.pocoo.org/2013/7/2/the-updated-guide-to-unicode/)guide-to-unicode/)深入分析了 Python 3中Unicode的一些陷阱(Armin不是很熟衷于 Python 3)。

《Python Cookbook (第 3 版)中文版》(David Beazley 和 Brian K+ Jones 著)的第 2 章“字 符串和文本”中有几个诀窍谈到了 Unicode 规范化、清洗文本，以及在字节序列上执行面 向文本的操作。第 5 章涵盖文件和 I/O， “5+17 将字节数据写入文本文件”指出，任何文本 文件的底层都有一个二进制流，如果需要可以直接访问。之后的“6+11 读写二进制结构的 数组”用到了 struct 模块。

Nick Coghlan的“Python Notes”博客中有两篇文章与本章的话题十分相关：“Python 3 and ASCII Compatible Binary Protocols”(http://python-

[notes+curiousefficiency+org/en/latest/python3/binary_protocols+html](http://python-notes.curiousefficiency.org/en/latest/python3/binary_protocols.html))和“Processing Text Files in Python 3”(http://python-

[notes+curiousefficiency+org/en/latest/python3/text_file_processing+html ](http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html)) 。强烈推荐阅读。

Python 3.5 将为二进制序列引入新的构造方法和方法，而且会废弃目前使用的构造方法签 名(参见“PEP 467—Minor API improvements for binary

sequences”， [https://www+python+org/dev/peps/pep-0467/](https://www.python.org/dev/peps/pep-0467/))。此外，Python 3.5 还会实现“PEP [461—Adding % formatting to bytes and bytearray”](https://www.python.org/dev/peps/pep-0461/)[(https://www.python.org/dev/peps/pep-](https://www.python.org/dev/peps/pep-0461/)

0461/)。

Python支持的编码列表参见codecs模块文档的“Standard Encodings”一节 ([https://docs.python.org/3/library/codecs.html#standard-encodings](https://docs.python.org/3/library/codecs.html%23standard-encodings)) 。如果需要通过编程的方

式获得那个列表，看看 CPython 源码中 /Tools/unicode/listcodecs.py 脚本 ([https://hg+python+org/cpython/file/6dcc96fa3970/Tools/unicode/listcodecs+py](https://hg.python.org/cpython/file/6dcc96fa3970/Tools/unicode/listcodecs.py))是怎么做的。

Martijn Faassen 的文章“Changing the Python Default Encoding Considered

[Harmful” ](http://blog.startifact.com/posts/older/changing-the-python-default-encoding-considered-harmful.html)[(http://blog.startifact.com/posts/older/changing-the-python-default-encoding-](http://blog.startifact.com/posts/older/changing-the-python-default-encoding-considered-harmful.html)

considered-harmful.html)和 Tarek Ziad^ 的文章“sys.setdefaultencoding Is

Evil” ([http://blog+ziade+org/2008/01/08/syssetdefaultencoding-is-evil/](http://blog.ziade.org/2008/01/08/syssetdefaultencoding-is-evil/))解释了为什么一定不

能修改 sys.getdefaultencoding() 获取的编码，即便知道怎么做也不能改。

Unicode Explained( Jukka K. Korpela 著， O'Reilly 出版

社，[http:"shop+oreilly+com/product/9780596101213+do](http://shop.oreilly.com/product/9780596101213.do))和 Unicode Demystified (Richard [Gillam ](http://www.informit.com/store/unicode-demystified-a-practical-programmers-guide-to-9780201700527)[著， Addison-Wesl ey 出版社， http://www.informit.com/store/unicode-demystified-a-](http://www.informit.com/store/unicode-demystified-a-practical-programmers-guide-to-9780201700527)practical-programmers-guide-to-9780201700527)这两本书不是针对 Python 的，但在我学习 Unicode 相关概念时给了我很大的帮助。 Victor Stinner 的著作 Programming with Unicode([http://unicodebook+readthedocs+org/index+html](http://unicodebook.readthedocs.org/index.html))是一本免费的自出版图书(遵守 CC BY-SA 协议)，其中讨论了一般的 Unicode 话题，以及主流操作系统和几门编程语言

(包括Python)中的相关工具和API。

W3C 网站中的“Case Folding: An

Introduction” ([https://www+w3+org/International/wiki/Case_folding](https://www.w3.org/International/wiki/Case_folding)) ^D“Character Model for [the World Wide Web: String Matching and Searching”](https://www.w3.org/TR/charmod-norm/)[(https://www.w3.org/TR/charmod-](https://www.w3.org/TR/charmod-norm/)norm/)讨论了规范化相关的概念，前者是介绍性文章，后者则是以枯燥的标准用语写就 的工作草案-“Unicode Standard Annex #15—Unicode Normalization

Forms” (<http://unicode.org/reports/tr15/>)也是这种风格。Unicode.org 网站中的“Frequently Asked Questions / Normalization” ([http://www+unicode+org/faq/normalization.html](http://www.unicode.org/faq/normalization.html))更容易理 解，Mark Davis 写的“NFC FAQ” ([http://www+macchiato+com/unicode/nfc-faq](http://www.macchiato.com/unicode/nfc-faq))也是如此。 Mark 是多个 Unicode 算法的作者，在我写作本书时，他还担任 Unicode 联盟的主席。

杂谈

“纯文本”是什么

对于经常处理非英语文本的人来说，“纯文本”并不是指“ASCII”。Unicode词汇表 ([http://www+unicode+org/glossary/#plain_text](http://www.unicode.org/glossary/%23plain_text))是这样定义纯文本的：

只由特定标准的码位序列组成的计算机编码文本，其中不含其他格式化或结构化

信息。

这个定义的前半句说得很好，但是我不同意后半句。 HTML 就是包含格式化和结构化

信息的纯文本格式，但它依然是纯文本，因为 HTML 文件中的每个字节都表示文本 字符（通常使用 UTF-8 编码），没有任何字节表示文本之外的信息。 .png 或 .xsl 文档 则不同，其中多数字节表示打包的二进制值，例如 RGB 值和浮点数。在纯文本中， 数字使用数字符号序列表示。

这本书是我用一种名为 AsciiDoc （<http://www.methods.co.nz/asciidoc/>，很讽刺）的纯 文本格式撰写的，它是 O'Reilly 优秀的图书出版平台 Atlas （https://atlas.oreilly.com/） 的工具链中的一部分。 AsciiDoc 的源文件是纯文本，但用的是 UTF-8 编码，而不是 ASCII。如果不这样做的话，撰写本章必定痛苦不堪。姑且不管名称，AsciiDoc是个 很棒的工具。

Unicode 的世界正在不断扩张，但是有些边缘场景缺少支持工具。因此图 4-1、图 4-3

和图 4-4 中的内容要使用图像，因为渲染本书的字体中缺少一些我想展示的字符。不

过，Ubuntu 14.04和OS X 10.9的终端能正确显示，包括“mojibake”（文字化什）这个

日文的词。

捉摸不透的 Unicode

讨论 Unicode 规范化时，我经常使用“往往”“多数”和“通常”等不确定的修饰语。很遗 憾，我不能提供更可靠的建议，因为 Unicode 规则有很多例外，很难百分之百确定。

例如，（微符号）是“兼容字符”，而Q （欧姆）和A （埃）符号却不是。这种差别 是有真实影响的：NFC规范化形式（推荐用于文本匹配）会把Q （欧姆）替换成 Q （大写希腊字母欧米加），把A （埃）替换成A （上有圆圈的大写字母A）。但 是，作为“兼容字符”的g （微符号）不会替换成视觉等效的g （小写希腊字母^）; 不过在使用更极端的 NFKC 或 NFKD 规范化形式时会替换，但这是有损转换。

我能理解为什么把g （微符号）纳入Unicode，因为latinl编码中有它，如果换成 希腊字母g，会破坏两种编码之间的转换。说到底，这就是微符号是“兼容字符”的原 因。但是，如果是由于兼容原因而没把欧姆和埃符号纳入Unicode，那为什么这两个

符号要存在？ Unicode 己经为 GREEK CAPITAL LETTER OMEGA 和 LATIN CAPITAL LETTER A WITH RING ABOVE 分配了码位，它们的外观一样，而且 NFC 规范化形式 会替换它们。想想看吧。

研究 Unicode 几小时之后，我猜测的原因是： Unicode 异常复杂，充满特殊情况，而

且要覆盖各种人类语言和产业标准策略。

在 RAM 中如何表示字符串

Python 官方文档对字符串的码位在内存中如何存储避而不谈。毕竟，这是实现细节。

理论上，怎么存储都没关系：不管内部表述如何，输出时每个字符串都要编码成字节

序列。

在内存中， Python 3 使用固定数量的字节存储字符串的各个码位，以便高效访问各个 字符或切片。

在 Python 3.3 之前，编译 CPython 时可以配置在内存中使用 16 位或 32 位存储各个码 位。16位是“窄构建”（narrow build） ， 32位是“宽构建”（wide build）。如果想知道

用的是哪个，要查看 sys.maxunieode 的值： 65535 表示“窄构建”，不能透明地处理 U+FFFF 以上的码位。 “宽构建”没有这个限制，但是消耗的内存更多：每个字符占 4 个字节，就算是中文象形文字的码位大多数也只占 2 个字节。这两种构建没有高下之

分，应该根据自己的需求选择。

从 Python 3.3 起，创建 str 对象时，解释器会检查里面的字符，然后为该字符串选择 最经济的内存布局：如果字符都在 latin1 字符集中，那就使用 1 个字节存储每个码 位；否则，根据字符串中的具体字符，选择 2 个或 4 个字节存储每个码位。这是简

述，完整细节参阅“PEP 393—Flexible String

Representation” (<https://www.python.org/dev/peps/pep-0393/>) 。

灵活的字符串表述类似于 Python 3 对 int 类型的处理方式：如果一个整数在一个机 器字中放得下，那就存储在一个机器字中；否则解释器切换成变长表述，类似于 Python 2 中的 long 类型。这种聪明的做法得到推广，真是让人欢喜！
