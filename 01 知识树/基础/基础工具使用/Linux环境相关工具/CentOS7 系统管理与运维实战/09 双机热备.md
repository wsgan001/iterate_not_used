＜奴抓热畐*

在互联网高速发展的今天，尤其是电子商务的发展，要求服务器能够提供不间断服务。在 电子商务中，如果服务器宕机，造成的损失是不可估量的。要保证服务器不间断服务，就需要 对服务器实现冗余。在众多的实现服务器冗余的解决方案中，Pacemaker为我们提供了廉价的、 可伸缩的高可用集群方案。

本章首先介绍高可用性集群技术，然后介绍高可用软件Pacemaker和keepalived的搭建与 应用。

本章主要涉及的知识点有：

•    高可用性集群技术

•    双机热备软件Pacemaker的应用

•    双机热备软件keepalived的应用

9.1高可用性集群技术

随着互联网的增长，互联网已经成为人们生活中的一部分，对网络的依赖不断増加，电子 商务使得订单一天24小时不间断地进行成为可能。如果服务器宕机，造成的损失是不可估量 的。每一分钟的宕机都意味着收入、生产和利润的损失，甚至市场地位的削弱。要保证服务器 不间断服务，就需要对服务器实现冗余。新的网络应用使得各个服务的提供者对计算机的要求 达到了空前的程度，电子商务需要越来越稳定可靠的服务系统。

9.1.1可用性和集群

可用性是指一个系统保持在线并且可供访问，有很多因素会造成系统宕机，包括为了维护 而有计划的宕机以及意外故障等，高可用性方案的目标就是使宕机时间以及故障恢复时间最小 化，高可用性集群，原义为High Availability Cluster,简称HA Cluster,是指以减少服务中断

（宕机）时间为目的的服务器集群技术。

所谓集群，是提供相同网络资源的一组计算机系统。其中的每一台提供服务的计算机，可 以称之为节点。当一个节点不可用或来不及处理客户的请求时，该请求将会转到另外的可用节 点来处理。对于客户端应用来说，不必关心资源调度的细节，所有这些故障处理流程集群系统 可以自动完成。

集群中节点可以以不同的方式来运行，比如同时提供服务或只有其中一些节点提供服务， 另外一些节点处于等待状态。同时提供服务的节点，所有服务器都同时处于活动状态，也就是 在所有节点上同时运行应用程序，当一个节点出现故障时，监控程序可以自动剔除此节点，而 客户端觉察不到这些变化。处于激活状态的节点在故障时由备节点随时接管，由于平时只有一 些节点提供服务，可能会影响应用的性能。在正常操作时，另一个节点处于备用状态，只有当 活动的节点出现故障时该备用节点才会接管工作，但这并不是一个很经济的方案，因为应用必 须同时采用两个服务器来完成同样的事情。虽然当出现故障时不会对应用程序产生任何影响， 但此种方案的性价比并不高。

9.1.2集群的分类

从工作方式出发，集群分为下面3种：

(1)    主/主

这是最常见的集群模型，提供了高可用性，这种集群必须保证在只有一个节点时可以提供 服务，提供客户可以接受的性能。该模型最大程度地利用服务器软硬件资源。每个节点都通 过网络对客户机提供网络服务。每个节点都可以在故障转移时临时接管另一个节点的工作。 所有的服务在故障转移后仍保持可用，而后端的实现客户端并不用关心，所有后端的工作对 客户端是透明的。

(2)    主/从

与主/主模型不同，限于业务特性，主/从模型需要一个节点处于正常服务状态，而另外一 个节点处于备用状态。主节点处理客户机的请求，而备用节点处于空闲状态，当主节点出现故 障时，备用节点会接管主节点的工作，继续为客户机提供服务，并且不会有任何性能上影响。

(3)    混合型

混合型是上面两种模型的结合，可以实现只针对关键应用进行故障转移，这样可以对这些 应用实现可用性的同时让非关键的应用在正常运作时也可以在服务器上运行。当出现故障时, 出现故障的服务器上的不太关键的应用就不可用了，但是那些关键应用会转移到另一个可用的 节点上，从而达到性能和容灾两方面的平衡。

赛I    双机热备开源软件Pacemaker

随着应用的用户量增长，或在一些系统关键应用中，为提供不间断的服务保证系统的高可 用是非常必要的。Pacemaker就是一个用于保证服务高可用性的组件，在行业内得到了广泛应

用。本节将简要介绍Pacemaker的安装与使用。

9.2.1 Pacemaker 概述

Pacemaker是一个集群资源管理器，他可以利用管理员喜欢的集群基础构件提供的消息和 成员管理能力来探测节点或资源故障，并从故障中恢复，从而实现集群资源的高可用性。与之 前广泛使用的Heartbeat相比，Pacemaker配置更为简单，并且支持的集群模式多样、资源管 理的方式更加灵活。

Pacemaker是一个相当庞大的软件，其内部结构如图9.1所示。

图9.1 Pacemaker内部结构

Pacemaker的主要组件及作用如下:

•    stonithd:心跳程序，主要用于处理与心跳相关的事件。

•    lrmd:本地资源管理程序，直接调配系统资源。

•    pengine:政策引擎，依据当前集群状态计算下一步应该执行的操作等。

•    CIB:集群信息库，主要包含了当前集群中所有的资源，及资源之间的关系等。

•    CRMD:集群资源管理守护进程。

Pacemaker工作时会根据CIB中记录的资源，由pengine计算出集群的最佳状态，及如何 达到这个最佳状态，最后建立一个CRMD实例，由CRMD实例来做出所有集群决策。这是 Pacemaker简要工作过程，读者如需详细了解其工作过程，可参考相关文档了解。

9.2.2 Pacemaker安装与配置

为保证系统更高的可用性，常常需要对重要的关键业务做双机热备，比如一个简单的Web 服务需要做双机热备。常见的方案有keepalived、Pacemaker等，本节以Pacemaker为例说明 双机热备的部署过程。

在本示例中，Pacemaker双机热备信息如表9.1所示。

表9.1 Heartbeat双机热备信息说明

| 参数          | 说明        |
| ------------- | ----------- |
| 192.168.3.87  | 主节点    . |
| 192.168.3.88  | 备节点      |
| 192.168.3.118 | 虚拟IP      |

示例实现的功能为：正常情况下由192.168.3.87提供服务，客户端可以根据主节点提供的 VIP访问集群内的各种资源，当主节点故障时备节点可以自动接管主节点的IP资源，即VIP

为 192.168.3.118。

HA的部署要经过软件安装、环境配置、资源配置等几个步骤，本小节将简单介绍软件安 装和环境配置。

(1)最新的CentOS 7已全面支持Pacemaker，可以通过yum进行安装，安装过程如【示 例9-1】所示。

【示例9-1】

\#与之前的LVS配置相同，先要对防火墙和SELinux进行配置

\#在本例中将关闭防火墙和SELinux

\#以下命令用于禁用和关闭防火墙

[root@localhost # systemctl disable firewalld

[root@localhost 〜]# systemctl stop firewalld

\#以下命令用于关闭SELinmc，但重启后失效

[root@localhost    弁 setenforce 0

\#重启后生效，可以修改以下文件中的语句

\#修改为disabled重启即可

[root@localhost ~]# cat /etc/sysconfig/selinux

SELINUX^disabled

并利用yum工具安装Pacemaker

[root@localhost 〜H yum install -y fence-agents-all corosync pacemaker pcs Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile

\*    base: mirrors.sina.cn

\*    extras: mirrors.btte.net

\*    updates: mirrors.sina.cn

经过以上步骤完成Pacemaker软件的安装，主要的软件包及作用如表9.2所示。

表9.2 Pacemaker软件包说明

| 参数             | 说明                   |
| ---------------- | ---------------------- |
| fence-agents-all | fence设备代理          |
| pacemaker        | 集群资源管理器         |
| corosync         | 集群引擎和应用程序接口 |
| pcs              | Pacemaker配置工具      |

(2)两个节点配置主机名和/etc/hosts

配置集群时，通常都会使用主机名来标识集群中的节点，因此需要修改hostnameo如果 使用DNS解析集群中的节点，解析延时会导致整个集群响应缓慢，因此任何集群都建议使用 hosts文件解析而不是DNS，如【示例9-2】所示。

【示例9-2】

\#此处在nodel即192.168.3.87上做的配置，192.168.3.88上则修改node2

[root@localhost ~]# cat /etc/hostname

nodel

\#两个节点上的/etc/hosts文件内容相同

[root@localhost # cat /etc/hosts

127.0.0.1 localhost localhost.localdomain localhost4 localhost4.Iocaldomain4

::1    localhost localhost. localdomain localhost6 localhost6. localdomain6

192.168.3.87 nodel

192.168.3.88 node2

\#修改完成后分别重启两个节点上的network服务

[root@localhost 〜systemctl restart network.service

网络服务重启后，可以通过重新登录的方式查看命令提示符的变化。

(3)配置ssh密匙访问

集群节点之间的通信是通过ssh进行的，但ssh访问需要密码，因此需要配置密匙访问， 如【示例9-3】所示。

【示例9-3]

\#生成密匙

[root@nodel -]# ssh-keygen -t rsa -P 1'

Generating public/private rsa key pair.

Enter file in which to save the key (/root/.ssh/id 一rsa): Created directory */root/.ssh1.

Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is:

cb:80:4c:bf:48:62:9e:fb:88:fa:27:93:a6:2d:c5:92 root@nodel The key * s randomart image is:

+--[ RSA 2048]----+

loo    I

I oo + o S IEoo+ . + .

I oo.. . o

![img](11 CentOS7fbdfa1060ed0f49e18-187.jpg)



I.0=0.

；j;'a=KES=>6=.

\#以上命令将生成一对公匙和私匙 #使用以下命令将公匙发送给node2

[root@nodel -3 # ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2 The authenticity of host * node2 (172.16.45.13)1 can't be established.

ECDSA key fingerprint is 89:d7:26:24:04:le:a9:a5:47：3e:ef:b9:ce:a5:84:be. Are you sure you want to continue connecting (yes/no)? yes

/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s)z to filter out any that are already installed

/usr/bin/ssh-copy~id: INFO: 1 key (s) remain to be installed -- if you are prompted now it is to install the new keys

\#此处需要输入node2的root密码 root@node21s password:

Number of key(s) added: 1

Now try logging into the machine, with:    "ssh 1 root@node2 "，

and check to make sure that only the key(s) you wanted were added.

接下来在node2上重复上述操作，让nodel和node2之间均可以使用密匙访问。

(4)配置集群用户

Pacemaker使用的用户名为hacluster，软件安装时此用户已创建，但还没有设置密码，此 时需要设置其密码，如【示例9-4】所示。

【示例9-4]

\#修改用户hacluster的密码

[root@nodel ~]# passwd hacluster

Changing password for user hacluster.

New password:

Retype new password:

passwd: all authentication tokens updated successfully.

在nodel上修改hacluster的密碍之后，还要修改node2上的hacluster用户密码，此处需 要注意的是两个节点上的hacluster密码应该相同。

(5)配置集群节点之间的认证

接下来应该启动pcsd服务，并配置各节点之间的认证，让节点之间可以互相通信，如【示 例9-5】所示。

【示例9-5】

\#启动pcSd服务并设置自启动

\#需要在两个节邊上都开論pes d服务

[root@nodel -]# systemctl start pcsd.service [rootQnodel # systemctl enable pcsd.service In -s */usr/lib/systemd/system/pcsd.service1 /etc/systemd/system/multi-user.target.wants/pcsd.service #配置节点间的认证

\#以下命令仅需要在nodel上执行即可

[root@nodel # pcs cluster auth nodel node2

\#需要输入用户名hacluster及密码

Username: hacluster

Password:

nodel: Authorized

node2: Authorized

配置完节点间认证后环境配置就完成了，接下来就可以新建集群并向集群中添加资源了。

9.2.3 Pacemaker 资源配置

Pacemaker可以为多种服务提供支持，例如Apache、MySQL、Xen等，可使用的资源类 型有IP地址、文件系统、服务、fence设备等，本小节将以最简单的Apache为例介绍如何添 加集群和资源。

![img](11 CentOS7fbdfa1060ed0f49e18-188.jpg)



fence设备国内通常称为远程控制卡，在节点失效后集群可以通过fence设备将失效节 点服务器重启。不同厂商的fence设备使用方法也不同，使用之前需要阅读相关的硬 件文档了解。

(1)配置 Apache

在节点nodel和node2上配置Apache，如【示例9-6】所示。

【示例9-6】

\# 安装 Apache

[rootgnodel # yum install -y httpd #在配置文件Apache最后加入以下内容

[root@nodel -J# tail /etc/httpd/conf/httpd.conf #配置监听地址和服务器名 Listen 0.0.0.0:80 ServerName [www.test.com](http://www.test.com)

\#设置服务器状态页面以便集群检测 〈Location /server-status>

SetHandler server-status Require all granted 〈/Location〉

\#设置一个最简单的页面示例并测试

[root@nodel 〜]毋 echo "welcome to nodeln >/var/www/html/index.html

[rootgnodel -]# systemctl start httpd.service [rootenodel -]# curl http: "192 .肌3.87

welcome to nodel    ,

Pacemaker可以控制httpd服务的启动和关闭，因此在node 1和node2上配置完Apache并 测试之后要关闭httpd服务。

(2)新建并启动集群

完成以上准备工作后，就可以在节点nodel上新建一个集群并启动，如【示例9-7】所示。

【示例9-7】

\#新建一个名为my cluster的集群

\#集群节点包括nodel和node2

[root@nodel # pcs cluster setup --name mycluster nodel node2 Shutting down pacemaker/corosync services...

Redirecting to /bin/systemctl stop pacemaker.service Redirecting to /bin/systemctl stop corosync.service Killing any remaining services...

Removing all cluster configuration files...

nodel: Succeeded

node2: Succeeded

\#启动集群并设置集群自启动

[root@nodel # pcs cluster start --all

node2: Starting Cluster...

nodel: Starting Cluster...

[root@nodel # pcs cluster enable 一一all

nodel: Cluster Enabled

node2: Cluster Enabled

\#査看集群状态

[root@nodel # pcs status

Cluster name: mycluster

WARNING: no stonith devices and stonith-enabled is not false Last updated: Sat Apr 25 18:43:29 2015 Last change: Sat Apr 25 18:41:40 2015 Stack: corosync

Current DC: nodel ⑴-partition with quorum

Version: 1.1.12-al4efad

2 Nodes configured

0 Resources configured

Online: [ nodel node2 }

Full list of resources:

PCSD Status: nodel: Online node2: Online

Daemon Status:

corosync: active/enabled pacemaker: active/enabled pcsd: active/enabled

![img](11 CentOS7fbdfa1060ed0f49e18-189.jpg)



从上述命令可以看到在nodel上新建集群之后，所有的设置都会同步到node2上，而在集 群状态中可以看到nodel和node2均己在线，集群使用的服务都已激活并启动。

(3)为集群添加资源

从第(2)步集群状态中的"Full list of resources"中可以看到集群还没有任何资源，接下 来将为集群添加VIP和服务，如【示例9-8】所示。

【示例9-8】

\#添加一个名为vip的IP地址资源 #使用heartbeat作为心跳检测 #集群每隔30s检査该资源一次

[root@nodel # pcs resource create VIP ocf:heartbeat: IPaddr2 ip=192.168.3.118 cidr_netmask-24 op monitor interval=30s

\#添加一个名为Web的Apache资源

\#检查该资源通过访问http: //127.0.0.1/server-status来实现 (root@nodel # pcs resource create Web ocf:heartbeat:apache

configfile=/etc/httpd/conf/httpd.conf

statusurl=n<http://127.0.0.1/server-status>" op monitor interval=30s [root@nodel ~]# pcs status Cluster name: mycluster

WARNING: no stonith devices and stonith—enabled is not false

Online: [ nodel node2 ]

\#以下为资源池情况

Full list of resources:

VIP (ocf::heartbeat:IPaddr2):    Stopped

Web (ocf::heartbeat:apache):    Stopped

PCSD Status: nodel: Online node2: Online

(4)调整资源

添加资源后还需要对资源进行调整，让VIP和Web这两个资源“捆绑”在一起，以免出 现VIP在节点nodel上，而Apaghe运行在node2上的情况。另一个情况则是有可能集群先启 动Apache,然后再启用VIP,这是不正确的。调整如【示例9-9】所示。

【示例9-9]

\#方式一：使用组的方式“捆绑”资源

\#将VIP和Web添加到myweb组中

[root@nodel # pcs resource group add myweb VIP

[root@nodel 〜]# pcs resource group add myweb Web

\#方式二：使用托管约束

[root@nodel # pcs constraint colocation add Web VIP INFINITY #设置资源的启动停止顺序 #先启动VIP,然后再启动Web

[root@nodel # pcs constraint order start VIP then start Web

(5)优先级

如果nodel与node2的硬件配置不同，那么应该调整节点的优先级，让资源运行于硬件配 置较好的服务器上，待其失效后再转移至较低配置的服务器上。这就需要配置优先级 (Pacemaker中称为Location),此配置为可选，如【示例9-10】所示。

【示例9-10】

\#调整 Location

\#数值越大表示优先级越高

[root@nodel # pcs constraint location Web prefers nodel=10 [rootQnodel H# pcs constraint location Web prefers node2=5 (root@nodel 〜crm 一simulate ~sL

Current cluster status:

Online: ( nodel node2 J

Resource Group: myweb

VIP    (ocf::heartbeat:IPaddr2)

Started nodel Started nodel



Web    (ocf::heartbeat:apache):

| scores | ;:         |       |      |        |      |
| ------ | ---------- | ----- | ---- | ------ | ---- |
| •: VIP | allocation | score | on   | nodel: | G    |
| :VIP   | allocation | score | on   | node2: | 0    |
| :Web   | allocation | score | on   | nodel: | 20   |
| :Web   | allocation | score | on   | node2: | 10   |



gro 叩一 color group_color group一color group_color

40



native color: VIP allocation score on nodel

一

native一color: VIP allocation score on node2: 20 native color: Web allocation score on nodel: 20

native_color: Web allocation score on node2: -INFINITY

在本例中并没有设置fence设备，集群在启动时可能会提示一些错误，可以使用命令pcs property set stonith-enabled=false禁用fence设备。读者也可以使用在Linux上的虚拟机中 使用虚拟fence设备，关于虚拟fence设备的使用方法可参考相关文档了解。

9.2.4 Pacemaker 测试

经过上面的配置，Pacemaker集群已经配置完成，重新启动集群所有设置可以生效，启动 过程如【示例9-11】所示。

【示例9-11 ]

利亭止所有集群

[root@nodel ~]# pcs cluster 1 stop —-all nodel: Stopping Cluster (pacemaker)...

node2: Stopping

node2: Stopping nodel: Stopping

\#启动所有集群



Cluster (pacemaker).,. Cluster (corosync)... Cluster (corosync)...

[root@nodel -]# pcs cluster start --all nodel: Starting Cluster...

node2: Starting Cluster...

\#査看集群状态

[root@nodel 〜]# pcs status

Cluster name: mycluster

Last updated: Sat Apr 25 23:13:26 2015 Last change: Sat Apr 25 23:04:27 2015 Stack: corosync

Current DC: node2 (2) - partition with quorum Version : 1.1.1.2-al4efad 2 Nodes configured 2 Resources configured

Started nodel Started nodel



![img](11 CentOS7fbdfa1060ed0f49e18-190.jpg)



Full list of resources:

Resource Group: myweb

VIP    (ocf::heartbeat:IPaddr2)

Web    (ocf::heartbeat:apache):

PCSD Status: nodel: Online node2: Online

\#验证VIP是否启用

[rootGnodel # ip address show 1: lo: <LOOPBACKrUP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN

*♦•參■鲁

2: enol6777736: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo一fast state UP qlen 1000

link/ether 00:0c:29:a0:3e:e5 brd

inet 192.168.3.87/24 brd 192.168.3.255 scope global dynamic eno!6777736

valid_lft 25521sec preferred_lft inet 192.168.3.118/24 brd 192.168.3.

\#验证Apache是否启动

[rootQnodel # ps aux | grep httpd root 51300    0.0    1.2 237896    6196 ?

-DSTATUS -f /etc/httpd/conf/httpd.conf -c

[root@nodel ~]# curl <http://192.168.3>. welcome to nodel

25521sec

255 scope global secondary enol6777736



Ss 23:13    0:00 /sbin/httpd

PidFile /var/run//httpd.pid

. ■ ■ ■ , ■

118



启动后正常情况下VIP设置在主节点192.168.3.87上。如主节点故障，则节点node2自动 接管服务，方法是直接重启节点nodel，然后观察备节点是否接管了主机的资源，测试过程如 【示例9-12】所示。

【示例9-12】

\#在节点nodel上执行重启操作 [root@nodel # reboot [root@node2 ~]# pcs status

Cluster name: mycluster

Last updated: Sat Apr 25 23:23:41 2015

Online: [ node2 ] OFFLINE: [ nodel ]

Full list of resources:

Resource Group: myweb

VIP    (ocf::heartbeat:IPaddr2)

Web    (ocf::heartbeat:apache):

PCSD Status: nodel: Offline node2: Online

Daemon Status:

corosync: active/enabled pacemaker: active/enabled

![img](11 CentOS7fbdfa1060ed0f49e18-191.jpg)



![img](11 CentOS7fbdfa1060ed0f49e18-192.jpg)



Started node2 Started node2



![img](11 CentOS7fbdfa1060ed0f49e18-193.jpg)



![img](11 CentOS7fbdfa1060ed0f49e18-194.jpg)



pcsd: active/enabled

当节点nodel故障时，节点node2收不到心跳请求，超过设置的时间后节点node2启用资 源接管程序，上述命令输出中说明VIP和Web已经被节点node2成功接管。如果节点nodel 恢复且设置了优先给，VIP和Web又会重新被节点nodel接管。

1.3    双机热备软件keepalived

关于HA目前有多种解决方案，比如Heartbeat、keepalived等，各有优缺点。本节主要以 keepalived为例说明keepalived的使用方法。

9.3.1 keepalived 概述

keepalived的作用是检测后端TCP服务的状态，如果有一台提供TCP服务的后端节点死 机，或工作出现故障，keepalived及时检测到，并将有故障的节点从系统中剔除，当提供TCP 服务的节点恢复并且正常提供服务后keepalived自动将提供TCP服务的节点加入到集群中， 这些工作全部由keepalived自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。

keepalived可以工作在TCP/IP协议栈的IP层、TCP层及应用层：

(1) IP 层

keepalived使用IP的方式工作时，会定期向服务器群中的服务器发送一个ICMP的数据包， 如果发现某台服务的IP地址没有激活，keepalived便报告这台服务器异常，并将其从集群中剔 除。常见的场景为某台机器网卡损坏或服务器被非法关机。IP层的工作方式是以服务器的IP 地址是否有效作为服务器工作正常与否的标准。

(2) TCP层

这种工作模式主要以TCP后台服务的状态来确定后端服务器是否工作正常。如MySQL 服务默认端口一般为3306,如果keepalived检测到3306无法登录或拒绝连接，则认为后端服 务异常，则keepalived将把这台服务器从集群中剔除。

(3)应用层

如keepalived工作在应用层了，此时keepalived将根据用户的设定检查服务器程序的运行 是否正常，如果与用户的设定不相符，则keepalived将把服务器从集群中剔除。

以上几种方式可以通过keepalived的配置文件实现。

9.3.2 keepalived安装与配置

本节实现的功能为访问192.168.3.118的Web服务时，自动代理到后端的真实服务器

192.168.3.1 和 192.168.3.2, keepalived 主机为 192.168.3.87,备机为 192.168.3.88。

最新的版本可以在http://www.keepalived.org获取，本示例采用的版本为1.2.16，安装过程 如【示例9-13】所示。

【示例9-13】

〜]# tar xvf keepalived-1.2.16.tar.gz # cd keepalived-1.2.16



[root@nodel

[root@nodel

\#安装依赖软件 [root@nodel [root@nodel [rootGnodel [root@nodel [rootQnodel /etc/

keepalived-1

keepalived-1

keepalived-1

keepalived-1

keepalived-1

2.16]    # yum install ~y openssl openssl-devel

2.16]    # ./configure --prefix=/usr/local/keepalived

2.16]    # make

2.16]# make install

2.16]# In -s /usr/local/keepalived/etc/keepalived



经过上面的步骤，keepalived已经安装完成，安装路径为/usr/local/keepalived，备节点操作 步骤同主节点。接下来进行配置文件的设置，如【示例9-14】所示。

【示例9-14】

| #主节点配置文件                                      |                                                              |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| [root@nodel 〜cat -n /etc/keepalived/keepalived.conf |                                                              |
| 19                                                   | ! Configuration File for keepalived                          |
| 么3                                                  | vrrp一instance VI_1 {                                        |
| 4                                                    | #指定该节点为主节点备用节点上需设置为BACKUP                  |
| 5                                                    | state MASTER                                                 |
| 6                                                    | #绑定虚拟IP的网络接口                                        |
| 7                                                    | interface enol677736                                         |
| 8                                                    | #VRRP组名，两个节点需设置一样，以指明各个节点属于同一 VRRP组 |
| 9                                                    | virtual一router_id 51                                        |
| 10                                                   | #主节点f优先级，~数值在1~254,注意从节点必须比主节点优先级低  |
| 11                                                   | priority 50                                                  |
| 12                                                   | ##组播信息发送间隔，两个节点需设置一样                       |
| 13                                                   | advert_int 1                                                 |
| 14                                                   | ##设置i证信息，两个节点需一致                                |
| 15                                                   | authentication {                                             |
| 16                                                   | auth一type PASS                                              |
| 17                                                   | auth_pass 1234                                               |
| 18                                                   | }                                                            |
| 19                                                   | #指定虚拟两个节点需设置一样                                  |
| 20                                                   | virtual_ipaddress {                                          |
| 21                                                   | 192.168.3.118                                                |
| 22                                                   | }                                                            |
| 23                                                   | }                                                            |
| 24                                                   | #虚拟IP服务                                                  |

25    virtual一server 192.168,3.118 80 {

26    #设£检査实际服务器的间隔

27    delay_loop 6

28    #指定LVS算法

29    lb-algo rr

30    #指定LVS模式

31    lb_kind nat

32

33    nat_mask 255.255.255.255

34    #持反连接设置，会话保持时间

'35    persistence一timeout 50

36    #转发协议为TCP

37    protocol TCP

38    #后端实际TCP服务配置

39    real_server 192.168.3.1 80 {

40    weight 1

备节点配置大部分配置同主节点，不同处如【示例9-15】所示。

【示例9-15】

[root@node2 # cat -n /etc/keepalived/keepalived.conf #不词于主节点，备机state设置为BACKUP 4    state BACKUP

\#优先级低于主节点 7    priority 50

\#其他配置和主节点相同

/etc/keepalived/keepalived.conf为keepalived的主配置文件。以上配置state表不主节点为 192.168.3.87,备节点为 192.168.3.88。虚拟 IP 为 192.168.3.11S 后端的真实服务器有 192.168.3.1 和192.168.3.2,当通过192.168.3.118访问Web服务时，自动转到后端的真实节点，后端节点 的权重相同，类似轮询的模式。Apache服务的部署可参考其他章节，此处不再赘述。

9.3.3 keepalived启动与测试

经过上面的步骤keepalived已经部署完成，接下来进行keepalived的启动与故障模拟测试。

1.启动 keepalived

安装完毕后keepalived可以设置为系统服务启动，也可以直接通过命令行启动，命令行启

动方式如【示例9-16】所示。

【示例9-16】

\#主节点启动keepalived

froot@nodel # export PATH~/usr/local/kGepalived/sbin:$PATH:.

[root@nodel -*] # keepalived -D -f /etc/keepalived/keepalived.conf #査看服务状态

[root@nodel # ip addr list

inet 192.168.3.87/24 brd 172.16.45.255 scope global dynamic enol6777736 valid一1ft 40807sec preferred一Ift 40807sec

inet 192.168.3.118/32 scope global eno!6777736 valid 一1ft forever preferred一1ft forever #备节点启动keepalived

[root@node2 〜/usr/local/keepalived/sbin/keepalived ~D -f /etc/keepalived/keepalived.conf

[root@node2 # ip addr list

inet 192.168.3.88/24 brd 172.16.45.255 scope global dynamic eno!6777736

分别在主备节点上启动keepalived，然后通过ip命令查看服务状态，在主节点ethO接口 上绑定了 192.168.3.118这个VIP,而备节点处于监听的状态。Web服务可以通过VIP直接访 问，如【示例9-17】所示。

【示例9-17]

[root@nodel conf]# curl http://I92.168.3.118 Hello 192.168.3.1

[root@nodel conf3 # curl http: "192，168.3.118

2.测试 keepalived

故障模拟主要分为主节点重启，服务恢复，此时备节点正常服务，当主节点恢复后主节点 重新接管资源正常服务。测试过程如【示例9-18】所示。

【示例9-18】

\#主节点服务终止

[root@nodel keepalived]# reboot #备节点接管服务

[root@nodel conf]# ip addr list #部分结果省略

inet 192.168,3.118/32 scope global eno!6777736 #査看备节点日志

[root@node2 conf]# tail -f /var/log/messages

Jun 16 07:12:46 LD_192一168_3__88 keepalive<^_vrrp [54537] : VRRP一工 nstance Transition to MASTER STATE

Jun 16 07:12:47 LD_192_168_3_88 keepalived_vrrp[54537]: VRRP_Instance(VI_1) Entering MASTER STATE

Jun 16 07:12:47 LD 192 168 3 88 keepalived vrrp[54537]: VRRP Instance(VI_1) —    ——    ——— *

setting protocol VIPs.

Jun 16 07:12:47 LD 192 168 3 88 keepalived vrrp[54537]: VRRP Instance(VI一1) Sending gratuitous ARPs on ethO for 192.168.3.118

Jun 16 07:12:47 LD_192_168_3_88 keepalived一healthcheckers[54536]: Netlink reflector reports IP 192.168.3.118 added

Jun 16 07:12:52 LD_192_168_3_88 keepalived_vrrp[54537]: VRRP一Instance(VI_1) Sending gratuitous ARPs on ethO for 192.168.3.118

\#主节点恢复后査看服务情况

[root@nodel keepalived]# ip addr list

inet 192.168.3.118/32 scope global enol6777736 #査看主节点曰志

[root@nodel log]# tail /var/log/messages

Jun 16 07:16:43 LD_192_168_3__87 keepalived_vrrp[260L2]: VRRP一Instance(VI~1) Transition to MASTER STATE

Jun 16 07:16:44 LD_192_168_3_87 keepalived_vrrp[26012]: VRRP一Instance(VI一1) Entering MASTER STATE

Jun 16 07:16:44 LD_192_168_3_87 keepalived_vrrp[26012]: VRRP一Instance setting protocol VIPs.

Jun 16 07:16:44 LD_192_168_3__87 keepalived_vrrp[26012]: VRRP_Instance(VI_1) Sending gratuitous ARPs on ethl for 192.168.3.118

Jun 16 07:16:49 LD__192_168_3_87 keepalived_vrrp[26012]: VRRP^Instance(VI_1) Sending gratuitous ARPs on ethl for 192.168.3.118

当主节点故障时，备节点首先将自己设置为MASTER节点，然后接管资源并对外提供服 务，主节点故障恢复时，备节点重新设置为BACKUP模式，主节点继续提供服务。keepalived 提供了其他丰富的功能，如故障检测、健康检查、故障后的预处理等，更多信息可以查阅帮助 文档。

•得小结

互联网业务的发展要求服务器能够提供不间断服务，为避免服务器宕机而造成损失，需要 对服务器实现冗余。在众多实现服务器冗余的解决方案中，开源高可用软件Pacemaker和 keepalived是目前使用比较广泛的高可用集群软件。本章分别介绍了 Pacemaker和keepalived 的部署及应用。两者的共同点是都可以实现节点的故障探测及故障节点资源的接管，在使用方 面并没有实质性的区别，读者可根据实际情况进行选择。
