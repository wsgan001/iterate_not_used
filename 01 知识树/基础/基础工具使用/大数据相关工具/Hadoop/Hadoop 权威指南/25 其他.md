#### 附录A

### 安装 Apache Hadoop

在单机上安装Hadoop来尝试一下是非常容易的， 第10章。

向集群的安装方法，可以参见



本附录将介绍如何使用一个Apache Software Foundation发布的二进制压缩包安装 Hadoop Common、HDFS、MapReduce和YARN。本书提到的其他项目的安装方法 已经包含在相应章的开始部分。

![img](Hadoop43010757_2cdb48_2d8748-299.jpg)



另一种方法是使用一个虚拟机（例如Cloudera的Quick-Start虚拟机）进行安装， 虚拟机自带有预装和配置好的Hadoop服务。

接下来的安装指导方法适用于基于Unix的系统，包括MAC OS X（虽然它不是一 个产品平台，但是非常适用于开发）。

##### A.1先决条件

必须确保安装的是合适版本的Java。可以通过Hadoop wiki （http://wiki.apache.org/ 伦rs/oAW）查看所安装版本的信息。以下命令可以帮助确认Java

是正确安装的。

% java -version

java version Hl.7.0^25"

]ava(TM) SE Runtime Environment (build 1.7.0一25-bl5)

Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

##### A.2安装

首先，决定以什么用户的身份来运行Hadoopo如果只是想试用Hadoop或或开发 Hadoop程序，可以用用户账号在单机上运行Hadoop。

从 Apache Hadoop 发布页面（[http://hadoop.apache.org/common/releases.htmiy^](http://hadoop.apache.org/common/releases.htmiy%5e%e8%bd%bd%e4%b8%80)[载一](http://hadoop.apache.org/common/releases.htmiy%5e%e8%bd%bd%e4%b8%80) 个稳定版的发布包（通常打包为一个gzipped tar文件），再解压缩到本地文件系统：

% tar xzf hadoop-x.y.z.tar.gz

在运行Hadoop安装程序之前，需要指定Java在本地系统中的路径。如果系统的 ]AVA_HOME环境变量已经正确地指向一个Java安装路径，且用户也希望使用该 Java安装路径，则无需进行其他配置。（这通常在一个shell启动文件中设置，例 如〜/.bash_profile歲〜/.bashrc、。否则，仍然需要编辑conf/hadoop-env.sh文件来设 置]AVA_H0ME环境变量以指定Java安装路径。例如，在Mac机器上，可如下编辑 以便指向安装的某个版本Java：

export JAVA—HOME=/Library/]ava/]avaVirtualMachines/jdkl.7•0_25•jdk/Contents/Home

很容易创建指向Hadoop安装目录（依照惯例是HADOOP_HOME）的环境变量；此外， 还要将Hado叩的二进制目录添加到命令行路径上。例如：

% export HADOOP_HOME=^/sw/hadoop-Xey•z % export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

注意，sbin目录下有运行Hadoop守护进程的脚本，因此如果计划在本地机器上运 行守护进程的话，需要将该目录包含进命令行路径中。

输入以下指令来判断Hadoop是否工作：

% hadoop version

Hadoop 2.5.1

Subversion <https://git-wip-us.apache.org/repos/asf/hadoop.git> -r

2el8dl79e4a8065b6a9f29cf2de9451891265cce

Compiled by jenkins on 2014-09-05T23:11Z

Compiled with protoc 2.5.0

From source with checksum 6424fcab95bfff8337780al81ad7c78

This command was run using /Users/tom/sw/hadoop-2.5.1/share/hadoop/common/hadoop -common-2.5.1.jar

##### A.3配置

Hadoop的各个组件均可利用XML文件进行配置。core-site.xml文件用于配置通用 属性，hdfs-site.xml文件用于配置HDFS属性，mapred-site.xml文件则用于配置 MapReduce属性，yarn-site.xml文件用于配置YARN属性。这些配置文件都放在 etc/hadoop子目录中。

上述配置文件中的属性都有默认设置，分别保存在Hadoop安装路径的 share/doc 子目录下的四个 HTML 文件中，即 core-default.html、hdfs-default.html、mapred-default.html 和 yarn-default.xml0

Hadoop有以下三种运行模式。

独立（或本地）模式无需运行任何守护进程，所有程序都在同一个JVM 上执行。在独立模式下测试和调试MapReduce程序很方便，因此该模式 在开发阶段比较合适。

.伪分布模式Hadoop守护进程运行在本地机器上，模拟一个小规模的 集群。

•全分布模式Hadoop守护进程运行在一个集群上。此模式的设置请参 见第10章。

在特定模式下运行Hadoop需要关注两个因素：正确设置属性和启动Hadoop守护 进程。表A-1列举了配置各种模式所需要的最小属性集合。在独立模式下，将使 用本地文件系统和本地MapReduce作业运行器；在分布模式下，将启动HDFS和 YARN守护进程，此外还需配置MapReduce以便能够使用YARN。

| 表A-1.不同模式的关键配置属性 |                                |                              |                              |                             |
| ---------------------------- | ------------------------------ | ---------------------------- | ---------------------------- | --------------------------- |
| 组件名称 Common              | 属性名称 fs.defaultFS          | 独立模式 <file:///> （默认） | 伪分布模式 hdfs://localhost/ | 全分布模式 hdfs: "namenode/ |
| HDFS                         | dfs.replication                | N/A                          | 1                            | 3 （默认）                  |
| • MapReduce                  | mapreduce. framework.name      | local （默认）               | Yarn                         | yarn                        |
| YARN                         | yarn.resourcemanager. hostname | N/A                          | Localhost                    | resourcemanager             |
|                              | yarn.nodemanager.aux-          | N/A                          | mapreduce一shuffle           | mapreduce一shuffle          |

services

可以阅读10.3节，了解更多配置信息。

##### A.4独立模式

由于默认属性专为本模式所设定，且本模式无需运行任何守护进程，因此在独立 模式下不需要更多操作。

##### A.5伪分布模式

在伪分布模式下，使用如下内容创建配置文件，并将其放在etc/hadoop目录下。 也可以把etc/hadoop目录复制到另一个位置，然后把kzYe.xw/这些配置文件放在

该目录下。这种方法的优点是，将配置设置和安装文件隔离开。如果是这么做 的，那么需要将环境变量HADOOP_CONF_DIR设置成指向那个新目录，或确定在启动 守护进程时使用--config选项）。

<?xml version:"1•0"?>

<!-- core-site.xml -->

〈configuration〉

<property>

<name>fs.defaultFS</name>

<value>hdfs://localhost/</value>

</property>

〈/configuration〉

<?xml version="1.0,,?>

<卜-hdfs-site.xml -->

〈configuration〉

<property>

<name>dfs.replication</name>

<value>l</value>

</property>

〈/configuration〉

<?xml version=Hle0H?>

<!-- mapred-site.xml -->

〈configuration〉

<property>

<name>mapreduce.framework.name</name>

<value>yarn</value>

</property>

〈/configuration〉

<?xml version=H1.0n?>

<!-- yarn-site.xml -->

〈configuration〉

〈property〉

<name>yarn.resourcemanager.hostname</name> <value>localhost</value>

</property>

<property>

<name>yarn.nodemanager.aux-services</name> <value>mapreduce一shuffle" value〉 </property>

〈/configuration〉

1.配置SSH

如前所述，在伪分布模式下工作时必须启动守护进程，而启动守护进程的前提是 使用需要提供的脚本成功安装SSH。Hadoop并不严格区分伪分布模式和全分布模 式，它只是在集群内的（多台）主机（由^文件定义）上启动守护进程：SSH连接 到各个主机并启动一个守护进程。伪分布模式是全分布模式的一个特例。在伪分 布模式下，（单）主机就是本地计算机Gocalhost），因此需要确保用户能够用SSH连 接到本地主机，并且可以不输入密码登录。

首先，确保SSH已经安装，且服务器正在运行。例如，在Ubuntu上可通过以下 指令进行测试：

% sudo apt-get install ssh

![img](Hadoop43010757_2cdb48_2d8748-300.jpg)



在Mac OS X系统中，确保远程登录（在系统“首选项”丨“共享”下）对当前用 户（或所有用户）可用。

然后基于空口令生成一个新SSH密钥，以实现无密码登录。

% ssh-keygen -t rsa -P •• -f 〜/.ssh/id一rsa % cat 〜/.ssh/id_rsa*pub >> ~/.ssh/authorized一keys

如果正运行的是ssh-agent，可能还需要运行ssh-add0

用以下指令测试是否能够连接：

% ssh localhost

如果成功，则无需键入密码。

2.格式化HDFS文件系统

在首次使用Hadoop前，必须格式化文件系统。只需键入如下指令：

% hdfs namenode -format

3.启动和终止守护进程

为启动HDFS、YARN和MapReduce守护进程，键入如下指令:

% start-dfs.sh

% start-yarn.sh

% mr-jobhistory-daemon.sh start historyserver

£ 如果配置文件并不位于默认的conf目录中，则或者在运行脚本前输出 HADOOP_CONF_DIR环境变量，或者启动守护进程时使用--config选项，该选 硕带有-条绝对路径指向配置目录；

% start-dfSeSh --config path-to-config-directory % start-yarn.sh --config path -to-config-directory % mr-jobhistory-daemon.sh --config path-to-config-directory start historyserver

本地计算机将启动以下守护进程：一个namenode、一个辅助namenode、一个 datanode(HDFS)、一个资源管理器、一个节点管理器(YARN)和一个历史服务器 (MapReduce)0可以浏览/o识目录(在Hadoop安装目录下)中的日志文件来检查守 护进程是否成功启动，或通过Web界面：在[http://localhost:50070/](http://localhost:50070/%e6%9f%bb%e7%9c%8bnamenode%e3%80%81%e5%9c%a8http://localhost:8088/%e6%9f%bb%e7%9c%8b%e8%b5%84%e6%ba%90%e3%80%81%e7%ae%a1%e5%be%b5%e5%99%a8%e3%80%81%e5%9c%a8A%e5%8d%b3.%e5%bd%bb&%e6%9f%bb%e7%9c%8b%e5%8e%86%e5%8f%b2%e6%9c%8d%e5%8a%a1%e5%99%a8%e3%80%82%e6%ad%a4%e5%a4%96%ef%bc%8c)[査看namenode、在http://localhost:8088/査看资源、管徵器、在A即.彻&査看历史服务器。此外，](http://localhost:50070/%e6%9f%bb%e7%9c%8bnamenode%e3%80%81%e5%9c%a8http://localhost:8088/%e6%9f%bb%e7%9c%8b%e8%b5%84%e6%ba%90%e3%80%81%e7%ae%a1%e5%be%b5%e5%99%a8%e3%80%81%e5%9c%a8A%e5%8d%b3.%e5%bd%bb&%e6%9f%bb%e7%9c%8b%e5%8e%86%e5%8f%b2%e6%9c%8d%e5%8a%a1%e5%99%a8%e3%80%82%e6%ad%a4%e5%a4%96%ef%bc%8c) Java的jps命令也能查看守护进程是否正在运行。

终止守护进程也很容易，示例如下:

% mr-jobhistory-daemon.sh stop historyserver % stop-yarn.sh % stop-dfSeSh

4.创建用户目录

为自己创建一个主目录需键入以下指令：

I

% hadoop fs -mkdir -p /user/$USER

##### A.6全分布模式

在集群上安装Hadoop还需要考虑更多因素，所以第10章专门对此模式进行了全

的描述。

### 关于CDH

CDH（全称 Cloudera’s Distribution Including Apache Hadoop，即 Cloudera 公司发布 的Apache Hadoop）是一个集成的基于Apache Hadoop的栈，包含了系统开发所需 的所有组件，这些组件已经经过测试、并且被打包在一起了。Cloudera公司的这 款发布包有多种形式，包括Linux包、虚拟机映像、tar文件和在云上运行CDH 的工具。CDH是在Apache 2.0许可下发布的自由软件，用户可从 <http://www.cloudera.com/cdh> 获得。

以CDH5为例，它包含下列包，其中许多包在本书中做过介绍。

Apache Avro

一个跨语言的数据序列化库，包括丰富的数据结构、一种快速压缩的二 进制格式和RPC。

•    Apache Crunch

高层Java API库，用于创建可以在MapReduce或Spark上运行的数据处 理管线。

•    Apache DataFu（，化中）

非常有用的统计类用户定义函数集（UDF）库，用于大规模分析。

•    Apache Flume

高可靠、可配置的数据流集合。

Apache Hadoop

高度可伸缩的数据存储（HDFS）、资源管理（YARN）和数据处理 （MapReduce）。

![img](Hadoop43010757_2cdb48_2d8748-302.jpg)



Apache HBase

面向列的实时数据库，支持随机读/写访问。

Apache Hive

用于大数据集合的类SQL查询和表 Hue

提供Web UI界面，便于操作Hadoop数据。

Cloudera Impala

支持面向HDFS或HBase的交互式、低延迟SQL查询。

Kite SDK

API库，范例和文档，用于构建Hadoop顶层应用。

Apache Mahout

可伸缩的机器学习和数据挖掘算法

Apache Oozie

用于相互依赖的Hadoop作业的工作流调度器。

Apache Parquet （孵化中）

一种有效的支持嵌套式数据的列式存储格式。

Apache Pig

支持大数据集开发的数据流语言。

Cloudera Search

自由文本、Google风格的搜索方案，支持对Hadoop数据的搜索。

Apache Sentry （孵化中）

对Hadoop用户的基于角色的细粒度访问控制。

Apache Spark

支持Scala、Java和Python语言的集群计算框架，支持基于内存的大规 模数据处理。

Apache Sqoop

支持结构化数据存储（如关系型数据库）和Hadoop之间的高效数据传输。

Apache ZooKeeper

面向分布式应用的高可用协调服务。

Cloudera还提供Cloudera manager来支持部署和操作CDH的Hadoop集群 要下载 CDH 和 Cloudera manager,请访问 <http://www.cloudera.com/downloads/0>



### 准备NCDC气象数据

这里首先简要介绍如何准备原始气象数据文件，以便我们能用Hadoop对它们进行 分析。如果打算得到一份数据副本供Hadoop处理，可按照本书配套网站（网址为

给出的指导进行操作。接下来，首先说明如何处理原

始的气象文件。

原始数据实际是一组经过bzip2压缩的Mr文件。每个年份有价值的数据单独放在 一个文件中。部分文件列举如下：

\1901.    tar.bz2

\1902.    tar.bz2

\1903.    tar.bz2

參參參

2000.tar.bz2

各个Zar文件包含一个gz/p压缩文件，描述某一年度所有气象站的天气记录。（事 实上，由于在存档中的各个文件已经预先压缩过，因此再利用bzip2对存档压缩 就稍显多余了）。示例如下：

% tar jxf 1901.tar.bz2 % Is -1 1901 | head

011990-99999-1950.gz 011990-99999-1950.gz

011990-99999-1950.gz

由于气象站数以万计，所以整个数据集实际上是由大量小文件构成的。鉴于 Hadoop对少量的大文件的处理更容易、更高效（参见8.2.1节），所以在本例中，我 们将每个年度的数据解压缩到一个文件中，并以年份命名。上述操作可由一个 MapReduce程序来完成，以充分利用其并行处理能力的优势。下面具体看看这个 程序。

该程序只有一个map函数，无reduce函数，因为map函数可并行处理所有文件操 作，无需整合步骤。这项处理任务能够用一个Unix脚本进行处理，因而在这里使 用面向MapReduce的Streaming接口比较恰当。请看范例C-1。

范例C-1.利用bash脚本来处理原始的NCDC数据文件并将其存储在HDFS中

\#!/usr/bin/env bash

\# NLinelnputFormat gives a single line: key is offset, value is S3 URI read offset s3file

林 Retrieve file from S3 to local disk

echo Hreporter:status:Retrieving $s3file" >&2

$HADOOP_HOME/bin/hadoop fs -get $s3file .

\# Un-bzip and un-tar the local file

target='basename $s3file .tar.bz2'

mkdir -p $target

echo Hreporter:status:Un-tarring $s3file to ^target" >&2 tar jxf basename $s3file' -C $target # Un-gzip each station file and concat into one file echo "reporter:status:Un-gzipping $target" >&2 for file in $target/*/*

do

gunzip -c $file >> $target.all

echo "reporter:status:Processed $filen >&2

Done # Put gzipped version into HDFS

echo "reporter:status:Gzipping $target and putting in HDFS" >&2 gzip -c $target.all | $HADOOP_HOME/bin/hadoop fs -put - gz/$target.gz

输入是一个小的文本文件Gw企_files.txt），列出了所有待处理文件（这些文件放在S3 文件系统中，因此能够以Hadoop所认可的S3URI的方式被引用）。示例如下：

s3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2 s3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2 s3n://hadoopbook/ncdc/raw/isd-2000.tar.bz2

由于输入格式被指定为NLinelnputFormat,每个mapper接受一行输入（包含必 须处理的文件）。处理过程在脚本中解释，但简单说来，它会解压缩bzip2文件， 然后将该年份所有文件整合为一个文件。最后，该文件以gz/p进行压缩并复制至 HDFS之中。注意，可以使用指令hadoop fs -put •从标准输入中获得数据。

状态消息输出到“标准错误”并以reporter:status为前缀，这样可以解释为 MapReduce状态更新。这告诉Hadoop该脚本正在运行，并未挂起。

运行Streaming作业的脚本如下:

% hadoop jar $JWDOOP」M/share/hado^/tools/lib/hadoop-streaming-*.jar\

-D mapred•reduce.tasks=0 \

-D mapred.map.tasks.speculative.execution=false \

•D mapred.task.timeout=12000000 \

-input ncdc_files.txt \

-inputformat org.apache•hadoop•mapred.lib.NLinelnputFormat \ -output output \

-mapper load一ncdc一map.sh \

-file load一ncdcjnap.sh

由此易知，这是一个“只有map”的作业，因为reduce任务数为0。以上脚本还 关闭了推测执行(speculative execution),因此重复的任务不会写相同的文件(7.4.3 节所讨论的方法也是可行的)。任务超时参数被设置为一个比较大的值，使得 Hadoop.不会杀掉那些运行时间较长的任务，例如，在解档文件或将文件复制到 HDFS时，或者当进展状态未被报告时。

最后，调用&哗将文件从HDFS中复制出来，再存档到S3文件系统中。

准备NCDC气象数据



701

 新版和旧版 Java MapReduce API

本书通篇使用的Java MapReduce API被称为“新API”，它替代了功能上等同的 旧API。尽管Hadoop同时提供新旧MapReduce API，但它们彼此并不兼容。如果 愿意的话可以使用旧API,因为对于本书中所有MapReduce范例，相应的使用旧 API的范例代码都可以到本书的配套网站下载（在oldapi包中）。

新版旧版API有下面几个值得注意的差异。

新 API 在 org.apache.hadoop.mapreduce 包（子包）中。旧 API 仍然在 org.apache.hadoop.mapred 中0

新API偏好在接口上定义抽象类，因为这样更利于演进。该特性意味着 可以在一个抽象类中添加一个方法（带有默认的实现），而无需破坏该类 中旧的实现®。例如，旧API中的Mapper和Reducer接口在新API中 都变成了抽象类。

•新API大量使用了 context object，使得用户代码能够与MapReduce系统

通信。例如，新的Context本质上统一了旧API中的以下三种角色 JobConf, OutputCollector和 Reporter。

•在新版和旧版API中，键值对都是被推送给mapper和reducer。但是， 新API允许mapper和reducer通过重写run（）方法来控制执行流。例 如，可以以批为单位处理记录，或者在所有记录都被处理前终止本次执

①从技术角度而言，如果类中原有的方法和新增的方法签名相同，那么这种改变几乎必然会破 坏原有方法的实现，正如Jim des Rivieres ft “Evolving Java-based APIs” 一文（网址为 httpMiUy/adding^pijnethod）中解释的那样。出于实用目的，我们将这种改变视为可兼容的。

行。在旧API中，mapper可以通过写一个MapRunnable类来达到同样 的目的，但对于reducer来说没有等价的手段。

•新API通过］ob类来完成作业控制，旧API中对应的是JobClient, 在新API中已经删除该类。

新API对配置功能进行了整合。旧API专门有一个用于作业配置的 ］obConf 对象，是 Hadoop 的 vanilla 对象 Configuration(用于配置守 护进程，详情参见6.1节)的一个扩展。新API中，作业配置通过 Configuration完成，可能会用到］ob类的一些辅助类(helper)方法。

输出文件的命名略有不同：旧API中，map和reduce输出命名格式为 part-nnnnn,而在新API中，map输出命名为-朗朗《，reduce输出 命名为part-r-nnnnn(nnnnn是个整数，表示该部分的编号，从00000 开始)。

•新API中，用户可重写的方法声明抛出的异常是java.lang. InterruptedException0意味着用户可以自己写代码处理中断，这 样，如果需要，系统框架可以友好地取消长时间运行的操作。®

•新 API 中，reduce()方法用 java.lang.Iterable 传递值，而旧 API 使用java.lang.Iterator传递值。这种改变使得使用Java的for-each循环结构实现值的迭代更加容易。

for (VALUEIN value : values) { ... }

fy 使用新API且在Hadoop 1上编译的程序，如果要在Hadoop 2上运行，则需重 新编译。这是因为新Mapreduce API中的一些类在Hadoop 1和Hadoop 2两个 版本之间改变了接口。现象就是一个运行时的错误，如下所示：

java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop. mapreduce.TaskAttemptContext，but class was expected.

范例D-l给出了一个MaxTemperature应用，该程序来自2.3.2节，这里使用旧 API对其进行了重写。新旧的差异之处用粗体予以标注。

①详情可以参见 Brian Goetz 的文章 “Java theory and practice: Dealing with InterruptedException”， 网址为 http:"bit. ly/interruptedexception。

新版和旧版 Java MapReduce API 703

把Mapper和Reducer类转换为新API时，不要忘记将map()方法和reduce() 方法的签名修改成新的形式。为了扩展新Mapper或Reducer类而仅修改原有 的类不会导致编译错误或警告错误，因为这些类分别提供了 map()方法和 reduce()方法的标识形式。但是，你的mapper或reducer代码将不会被调用，这 会导致一些难以诊断的错误。

使用^override注释map()和reduce()方法，这样一来，Java编译器就会捕 捉到这些错误。

范例D-1.这个应用程序在气象数据集中找出最高气温(使用旧的MapReduce API) public class OldMaxTemperature {

static class OldMaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable>Text, Text, IntWritable> {

秦

private static final int MISSING = 9999;

^Override

public void map(LongWritable key. Text value,

OutputCollector<Textj IntWritable> output. Reporter reporter)

throws IOException {

String line = value.toString();

String year = line.substring(15, 19); int airTemperature;

if (line.charAt(87) == ■+■) { // parseint doesn't like leading plus signs airTemperature = Integer.parselnt(line.substring(88, 92));

} else {

airTemperature = Integer.parselnt(line.substring(87， 92));

}

String quality = line.substring^，93);

if (airTemperature != MISSING && quality.matches(u[01459]H)) { output.collect(new Text(year)new IntWritable(airTemperature));

}

}

}

t

static class OldMaxTemperatureReducer extends MapReduceBase implements Reducer<Text>IntWritable, Text, IntWritable> {

^Override

public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output. Reporter reporter) throws IOException {

int maxValue = Integer.MIN.VALUE; while (values.hasNext()) {

maxValue = Math.max(maxValue^ values.next().get());

}

output.collect(key, new IntWritable(maxValue));

}

}

704 附录D

public static void main(String[] args) throws IOException { if (args.length != 2) {

System.err.println("Usage: OldMaxTemperature 〈input path> 〈output path〉"); System•exit(-1);

}

JobConf conf = new JobConf(OldMaxTemperature.class); conf.set〕obName("Max temperature");

FileInputFormat.addInputPath(conf, new Path(args[0])); FileOutputFormat.setOutputPath(confnew Path(args[l]));

conf.setMapperClass(01dMaxTemperatureMapper.class); conf.setReducerClass(01dMaxTemperatureReducer.class);

conf.setOutputKeyClass(Text.class);

conf.setOutputValueClass(IntWritable.class);

]obClient.run]ob(conf);

}

###### 关于作者

Tom White是最杰出的Hadoop专家之一。自2007年2月以来，Tom White—直是 Apache Hadoop的提交者(committer),也是Apache软件基金会的成员。Tom是 Cloudera的软件工程师，他是Cloudera的首批员工，对Apache和Cloudera做出 了举足轻重的贡献。在此之前，他是一名独立的Hadoop顾问，帮助公司搭建、使 用和扩展Hadoop。他是很多行业大会的专题演讲人，比如ApacheCon、OSCON 和Strata。Tom在英国剑桥大学获得数学学士学位，在利兹大学获得科学哲学硕士 学位。他目前与家人居住在威尔士。

###### 关于译者

王海博士，解放军理工大学通信工程学院教授，博导，教研中心主任， 无线自组网网络的设计与研发工作，主持国家自然科学基金、国家863 等多项国家级课题，近5年获军队科技进步二等奖1项，三等奖6项， 发明人申请国家发明专利十余项，发表学术论文50余篇。

长期从事



计划课题 作为第一



华东博士，现任南京医科大学计算机教研室教师，一直致力于计算机辅助教学的 相关技术研究，陆续开发了人体解剖学网络自主学习考试平台、诊断学自主学习 平台和面向执业医师考试的预约化考试平台等系统，并在各个学科得到广泛的使 用，获得全国高等学校计算机课件评比一等奖和三等奖各一项。主编、副主编教 材两部，获发明专利一项、软件著作权多项。

刘喻博士，长期从事软件开发、软件测试和软件工程化管理工作，目前任教于清 华大学软件所。

吕粤海，长期从事军事通信网络技术研究与软件开发工作，先后通过华为光网络 高级工程师认证、思科网络工程师认证。

###### 关于封面图片

《Hadoop权威指南》封面上的动物是非洲象。非洲象属中的大象是地球上最大的 陆地动物（体形略大于其表亲亚洲象），可以通过耳朵来辨认，它们的耳朵与亚 洲大陆的形状相似。成年非洲雄象高12英尺（约3米），最高能到4.1米，重 12 000磅（约5.4吨），但重的能到15 000磅（约6.8吨），最高记录是10吨。 成年非洲雌象身高10英尺，体重8 000〜11 000磅。甚至小象也身形高大，刚出 生时体重就接近200磅（约90公斤），身高3英尺（约0.9米）左右。

非洲象生活在撒哈拉以南的非洲地区。陆地上大部分非洲象生活在稀树大草原和 干旱的矮树丛。有些非洲象生活在沙漠地区，有些则生活在山区。

非洲象这一物种在其生活的丛林和草原生态系统中扮演着非常重要的角色。许多 植物种子要发芽，只能通过非洲象的消化道来完成。据估计，非洲西部近三分之 一树种依赖于非洲象的消化方式来发芽。非洲象食用大量的草本植物，会对植被 和丛林的结构产生影响。例如，在自然条件下，非洲象食用大量草本植物之后， 热带雨林中会形成空隙，阳光可以投射到这些区域，从而促进植物的生长。非洲 象对许多动植物都有影响，之所以称之为基础物种，是因为它们可以使自己的栖 息地持续存在。

封面图片来自于Dover Pictorial Archive。封面字体使用的是Adobe ITC Garamond。 正文字体是汉仪书宋一简；页眉字体是汉仪中等线简+MyriadPro-regular;代码字 体是 LucasFont 的 Consolas。

### O.REILLY@



###### Hadoop权威指南：大数据的存储与分析（第4版）



准备好充分释放数据的潜能了吗？借助于最新版《Hado叩权威指南》， 你将学习如何使用Apache Hadoop构建和维护高稳定性、高伸缩性的分 布式系统。本书是程序员的理想读物，可以帮助他们分析任何大小的数



“你终于有机会向大师学习Hadoop 了——不仅是技术，还有常识和大 实话。”



据集。本书同时也是为管理员写的，可以帮助他们了解如何安装和运行



Doug Cutting



Hadoop 集群。



Cloudera



新版本新增了对丫ARN和Hadoop相关开源项目•的介绍，比如 Parquet, Flume，Crunch和Spark。通过本书，读者可以了解到Hdoop的 最新动态，进一步探索Hadoop在医疗保健系统以及地理数据处理方面的 重要作用。



本书可以帮助读者实现以下目标：

■学习并掌握MapReduce、HDFS和YARN等基本组件 ■深入探索MapReduce，包括用它来开发应用程序的具体步骤



安装和维护Hadoop集



LU



在 YARN 上运行 HDFS 和 MapReduce



学习并掌握两种数据格式：用于数据序列化的Avro和用于嵌套数 据的 Parquet

学会使用Flume （用于流数据）和Sqoop （用于块数据转换）等 数据摄取工具



Tom White，Cloudera工程师 和Apache钦件基金会成员。自 2007年2月以来，他一直担任 Hadoop项目的负责人。他曾 为oreilly.com，java.net和IBM的 developerWorks写过大量文章并定 期在行业大会上发表Hadoop主题 的演讲。



■理解Pig，Crunch和Spark等高级数据处理工具是如何与Hadoop 结合使用的

■学习并掌握HBase分布式数据库和ZooKeeper分布式配置服务

编程语言/HADOOP

清华大学出版社数字出版网站
