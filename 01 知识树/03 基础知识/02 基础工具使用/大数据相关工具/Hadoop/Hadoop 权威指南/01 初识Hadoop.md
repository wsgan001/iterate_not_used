初识Hadoop
千方百计综合利用更多计算机来解决问题

# 1.数据！数据!
我们生活在这个数据大爆炸的时代，很难估算全球电子设备中存储的数据总共有多少际数据公司(IDC)曾经发布报告称，2013年数字世界(digital universe)项目统计得出全球数据总量为4.4ZB(zettabyte)并预测在2020年将达 到 44 ZBO ⑦ 1ZB 等于 1021 字节，等于 1000 EB(exabytes), 1 000 000 PB (petabytes)，等于大家更熟悉的10亿TB(tenabytes)! 这远远超过了全世界每 人一块硬盘中所能保存的数据总量！


数据“洪流”有很多来源。以下面列出的为例:

- 纽约证交所每天产生的交易数据大约在4 TB至5 TB之间；
- 脸谱网（Facebook）存储的照片超过2400亿张，并以每月至少7PB的速度增长；
- 家谱网站Ancestry.com存储的数据约为10 PB;
- 互联网档案馆（The Internet Archive）存储的数据约为18.5PB,
- 瑞士日内瓦附近的大型强子对撞机每年产生的数据约为30 PB。

还有其他大量的数据。但是你可能会想它对自己又有哪些影响呢？地球人都知 道，大部分数据都戒备森严，波锁存在一些大型互联网公司（如搜索引擎公司）或 科学机构与金融机构中。大数据的出现会影响到小机构和个人吗？

我个人是这样认为的。以照片为例，我妻子的爷爷是一个骨灰级的摄影爱好者。 他成年之后，一直都在拍照。他的整个相册，包括普通胶片、幻灯片以及35mm 胶片，在扫描成高分辨率的图片之后，大约有10GB。相比之下，单单是2008 年，我家用数码相机拍的照片就有5GB。对照爷爷的照片生成速度，我家是他老人家的35倍！并且，而且这个速度还在不断增长中，因为现在拍照片真的是越来越容易。

有一种情况更普遍，个人产生的数据正在快速增长。微软研究院的MyLifeBits项 目［http://bit.ly/ms_mylifebits] 显示，在不久的将来，个人信息档案将日益普及。 MyLifeBits的一个实验是获取和保存个人的对外联系情况（包括电话、邮件和文 件），供日后存取。收集的数据中包括每分钟拍摄的照片等，数据量每月约为 1GB。当存储成本急剧下降以至于可以存储音频和视频时，MyLifeBits项目未来 的存储数据量将是现在的很多倍。

保存个人成长过程中产生的所有数据似乎逐渐成为主流，但更重要的也许是，作 为物联网一部分的机器设备产生的数据可能远远超过我们个人所产生的数据。机 器日志、RFID读卡器、传感器网络、车载GPS和零售交易数据等，所有这些都 将产生巨量的数据。

在网上公开发布的数据也在逐年增加中。组织或企业，要想在未来取得成功，不 仅需要管理好自己的数据，更需要从其他组织或企业的数据中获取有价值的 信息。

这方面的先锋有 Amazon Web Sevrvices 和 Infochimps.org ，它们所发布的共享数据集，正在促进信息共 享（information commons）,供所有人自由下载和分析（或者只需要一个合理的价格）。不同来源的信息在经过混搭和处理之后，会带来意外的效果和我们今天难以 想象的应用。

以 Astrometry.net 为例，主要查看和分析Flickr网站上天体测 量兴趣小组所拍摄的星空照片。它对每一张照片进行分析并能辨别出它来自星空 或其他天体（例如恒星和星系等）的哪一部分。虽然这项研究尚处于试验阶段，但也 表明如果可用的数据足够多（在本例中，为加有标签的图片数据），通过它们而产生 的后续应用也许会超乎这些拍照片的人最初的想象（图片分析）。<span style="color:red;">嗯</span>

有句话说得好：“大数据胜于好算法。”意思是说对于某些应用（譬如根据以往的偏好来推荐电影和音乐），不论算法有多牛，基于小数据的推荐效果往往都不如 基于大量可用数据的一般算法的推荐效果。<span style="color:red;">不知道现在还是不是这样。</span>

现在，我们已经有了大量数据，这是个好消息。但不幸的是，我们必须想方设法 好好地存储和分析这些数据。

# 2.数据的存储与分析
我们遇到的问题很简单：在硬盘存储容量多年来不断提升的同时，访问速度（硬盘 数据读取速度）却没有与时俱进。1990年，一个普通硬盘可以存储1370 MB数 据，传输速度为4.4 MB/s,因此只需要5分钟就可以读完整个硬盘中的数据。20 年过去了，1TB的硬盘成为主流，但其数据传输速度约为100 MB/s,读完整个硬盘中的数据至少得花 2.5 个小时。

读完整个硬盘中的数据需要更长时间，写入数据就别提了。一个很简单的减少读 取时间的办法是同时从多个硬盘上读数据。试想，如果我们有100个硬盘，每个 硬盘存储1%的数据，并行读取，那么不到两分钟就可以读完所有数据。

仅使用硬盘容量的1%似乎很浪费。但是我们可以存储100个数据集，每个数据集 1TB，并实现共享硬盘的读取。可以想象，用户肯定很乐于通过硬盘共享来缩短 数据分析时间；并且，从统计角度来看，用户的分析工作都是在不同时间点进行 的，所以彼此之间的干扰并不太大。<span style="color:red;">嗯，这倒是</span>

虽然如此，但要对多个硬盘中的数据并行进行读/写数据，还有更多问题要解决。

第一个需要解决的是硬件故障问题。一旦开始使用多个硬件，其中个别硬件就很 有可能发生故障。为了避免数据丢失，最常见的做法是复制(replication):系统保存数据的复本(replica)，一旦有系统发生故障，就可以使用另外保存的复本。例如，冗余硬盘阵列(RAID)就是按这个原理实现的，另外，Hadoop的文件系统 (Hadoop Distributed FileSystem，HDFS)也是一类，不过它采取的方法稍有不同，详见后文的描述。<span style="color:red;">嗯，想知道这个HDFS与 RAID 有什么区别？</span>

第二个问题是大多数分析任务需要以某种方式结合大部分数据来共同完成分析， 即从一个硬盘读取的数据可能需要与从另外99个硬盘中读取的数据结合使用。各 种分布式系统允许结合不同来源的数据进行分析，但保证其正确性是一个非常大 的挑战。MapReduce提出一个编程模型，该模型抽象出这些硬盘读/写问题并将其 转换为对一个数据集(由键-值对组成)的计算。后文将详细讨论这个模型，这样的 计算由map和reduce两部分组成，而且只有这两部分提供对外的接口。与HDFS 类似，MapReduce自身也有很高的可靠性。<span style="color:red;">嗯，想知道这个模型到底是怎么做的？怎么保证可靠性的？</span>

简而言之，Hadoop为我们提供了一个可靠的且可扩展的存储和分析平台。此外， 由于Hadoop运行在商用硬件上且是开源的，因而可以说Hadoop的使用成本是在 可承受范围内的。

# 3.查询所有数据
MapReduce看似采用了一种蛮力方法。每个查询需要处理整个数据集或至少一个 数据集的绝大部分。但反过来想，这也正是它的能力。MapReduce是一个批量查询处理器，能够在合理的时间范围内处理针对整个数据集的动态查询。它改变了 我们对数据的传统看法，解放了以前只是保存在磁带和硬盘上的数据。它让我们有机会对数据进行创新。以前需要很长时间处理才能获得结果的问题，到现在变得顷刻之间就迎刃而解，同时还可以引发新的问题和新的见解。

例如，Rackspace 公司的邮件部门 Mailtrust 就用 Hadoop 来处理邮件日志。他们写 了一条特别的査询用于帮助找出用户的地理分布。他们是这么描述的：“这些数据非常有用，我们每月运行一次 MapReduce 任务来帮助我们决定扩容时将新的邮件服务器放在哪些 Rackspace 数据中心。”<span style="color:red;">嗯，这个的确是一个问题，厉害。</span>

通过整合好几百GB的数据，用工具来分析这些数据，Rackspace的工程师能够对以往没有注意到的数据有所理解，甚至还运用这些信息来改善现有的服务。<span style="color:red;">嗯。</span>

# 4.不仅仅是批处理

从MapReduce的所有长处来看，它基本上是一个批处理系统，并不适合交互式分析。你不可能执行一条查询并在几秒内或更短的时间内得到结果。典型情况下， 执行查询需要几分钟或更多时间。因此，MapReduce 更适合那种没有用户在现场 等待查询结果的离线使用场景。<span style="color:red;">嗯。</span>

然而，从最初的原型出现以来，Hadoop的发展已经超越了批处理本身。实际上， 名词 “Hadoop” 有时被用于指代一个更大的、多个项目组成的生态系统，而不仅仅是HDFS和 MapReduce 。这些项目都属于分布式计算和大规模数据处理范畴。 这些项目中有许多都是由Apache软件基金会管理，该基金会为开源软件项目社区提供支持，其中包括最初的 HTTP server 项目（基金会的名称也来源于这个项目）。

第一个提供在线访问的组件是 HBase, 一种使用HDFS做底层存储的键值存储模型。HBase 不仅提供对单行的在线读/写访问，还提供对数据块读/写的批操作，这对于在HBase上构建应用来说是一种很好的解决方案。<span style="color:red;">什么意思？单行的在线读写访问？HBase 是键值存储模型吗？</span>

Hadoop2 中 YARN（Yet Another Resource Negotiator）的出现意味着 Hadoop 有了新处理模型。YARN是一个集群资源管理系统，允许任何一个分布式程序（不仅仅是 MapReduce）基于Hadoop集群的数据而运行。<span style="color:red;">YARN是与 HBase 类似的吗？</span>

在过去的几年中，出现了许多不同的、能与Hadoop协同工作的处理模式。以下是一些例子。

Interactive SQL（交互式 SQL）

利用 MapReduce 进行分发并使用一个分布式查询引擎，使得在Hadoop上获得 SQL查询低延迟响应的同时还能保持对大数据集规模的可扩展性。这个引擎使用指定的 “总是开启（always on）” 守护进程（如同impala）或容器重用（如同Tez上的 Hive）。<span style="color:red;">什么意思？</span>

Iterative process/ng（迭代处理）

许多算法，例如机器学习算法，自身具有迭代性，因此和那种每次迭代都从硬盘 加载的方式相比，这种在内存中保存每次中间结果集的方式更加高效。 MapReduce的架构不允许这样，但如果使用Spark就会比较直接，它在使用数据集方面展现了一种高度探究的风格。<span style="color:red;">什么是高度探究的风格？spark 做了什么</span>

Stream process/ng（流处理）

流系统，例如 Storm，Spark Streaming 或 Samza 使得在无边界数据流上运行实时、 分布式的计算，并向Hadoop存储系统或外部系统发布结果成为可能。<span style="color:red;">到底怎么做的？</span>

Search（搜索）

Solr 搜索平台能够在Hadoop集群上运行，当文档加入HDFS后就可对其进行索引，且根据 HDFS 中存储的索引为搜索查询提供服务。

无论Hadoop上出现了多少不同的处理框架，就批处理而言，MapReduce仍然有着一席之地。MapReduce提出的一些概念更具有通用性（例如，输入格式、数据集分片等），因此最好是能够了解MapReduce的工作机制。

# 5.相较于其他系统的优势
Hadoop不是历史上第一个用于数据存储和分析的分布式系统，但是Hadoop的一 些特性将它和其他那些看上去类似的系统区别开来。接下来我们将对此予以介 绍。

## 5.1关系型数据库管理系统

为什么不能用配有大量硬盘的数据库来进行大规模数据分析？我们为什么需要Hadoop? <span style="color:red;">是呀？</span>

这两个问题的答案来自于计算机硬盘的另一个发展趋势：**寻址时间的提升远远不敌于传输速率的提升**。寻址是将磁头移动到特定硬盘位置进行读/写操作的过程。 它是导致硬盘操作延迟的主要原因，而传输速率取决于硬盘的带宽。<span style="color:red;">嗯，但是现在的 ssd 的寻址速度还是不行吗？</span>

如果数据访问模式中包含大量的硬盘寻址，那么读取大量数据集就必然会花更长的时间（相较于流数据读取模式，流读取主要取决于传输速率）。另一方面，如果数据库系统只更新一小部分记录，那么传统的B树（关系型数据库中使用的一种数据结构，受限于寻址的速率）就更有优势。但数据库系统如果有大量数据更新时，B 树的效率就明显落后于MapReduce,因为需要使用“排序/合并”（sort/merge）来重建数据库。<span style="color:red;">传统的数据库系统用到了 B 树吗？要深入了解下，而且，为什么大量数据更新的时候，效率没有 MapReduce 高？</span>

在许多情况下，可以将MapReduce视为关系型数据库管理系统的补充。两个系统之间的差异如表1-1所示。
- MapReduce 比较适合解决需要以批处理方式分析整个数据集的问题，尤其是一些特定目的的分析。RDBMS 适用于索引后数据集的点查询（point query）和更新，建立索引的数据库系统能够提供对小规模数据的低延迟数据检索和快速更新。
- MapReduce 适合一次写入、多次读取数据的应用，关系型据库则更适合持续更新的数据集。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/62B45bF8Hj.png?imageslim)
<span style="color:red;">到底什么是批处理？为什么MapReduce没有事务？什么是读时模式和写时模式？完整性是什么？为什么横向扩展是现行的和非线性的？</span>


然而，关系型数据库和Hadoop系统之间的区别是模糊的。一方面，关系型数据库已经开始吸收Hadoop的一些思想，另一方面，诸如Hive这样的Hadoop系统不仅变得更具交互性(通过从MapReduce中脱离出来)，而且增加了索引和事务这样的特性，使其看上去更像传统的关系型数据库。

Hadoop 和关系型数据库的另一个区别在于它们所操作的数据集的结构化程度。结构化数据(structured data)是具有既定格式的实体化数据，如XML文档或满足特定预定义格式的数据库表。这是RDBMS包括的内容。另一方面，半结构化数据 (semi-structured data)比较松散，虽然可能有格式，但经常被忽略，所以它只能作为对数据结构的一般性指导。例如电子表格，它在结构上是由单元格组成的网格，但是每个单元格内可以保存任何形式的数据。非结构化数据(unstructured data) 没有什么特别的内部结构，例如纯文本或图像数据。Hadoop对非结构化或半结构化数据非常有效，因为它是在处理数据时才对数据进行解释(即所谓的“读时模式”)。这种模式在提供灵活性的同时避免了 RDBMS 数据加载阶段带来的高开销，因为在Hadoop中仅仅是一个文件拷贝操作。<span style="color:red;">加载阶段的高开销？</span>

关系型数据往往是规范的(normalized)，以保持其数据的完整性且不含冗余。规范给Hadoop处理带来了问题，因为它使记录读取成为非本地操作，而 Hadoop 的核心假设之一偏偏就是可以进行(高速的)流读/写操作。<span style="color:red;">什么意思？什么叫：它使记录读取成为非本地操作？</span>

Web服务器日志是典型的非规范化数据记录(例如，每次都需要记录客户端主机全名，这会导致同一客户端的全名可能多次出现)，这也是Hadoop非常适用于分析各种日志文件的原因之一。注意，Hadoop也能够做连接(join)操作，只不过这种操作没有在关系型数据库中用的多。<span style="color:red;">怎么用 Hadoop 分析日志文件？</span>

MapReduce以及Hadoop中其他的处理模型是可以随着数据规模线性伸缩的。对数 据分区后，函数原语(如map和reduce)能够在各个分区上并行工作。这意味着， 如果输入的数据量是原来的两倍，那么作业的运行时间也需要两倍。但如果集群规模扩展为原来的两倍，那么作业的运行速度却仍然与原来一样快。SQL査询一 般不具备该特性。<span style="color:red;">嗯，为什么不具备？集群规模扩展为原来的两倍，那么作业的运行速度却仍然与原来一样快 确认下？</span>

## 5.2 网格计算
高性能计算(High Performance Computing, HPC)和网格计算(Grid Computing)组织多年以来一直在研究大规模数据处理，主要使用类似于消息传递接口(Message Passing Interface, MPI)的API。从广义上讲，高性能计算采用的方法是将作业分散到集群的各台机器上，这些机器访问存储区域网络（SAN）所组成的共享文件系统。这比较适用于计算密集型的作业，但如果节点需要访问的数据量更庞大（高达几百GB，Hadoop开始施展它的魔法），很多计算节点就会因为网络带宽的瓶颈问题而不得不闲下来等数据。

Hadoop 尽量在计算节点上存储数据，以实现数据的本地快速访问。数据本地化 （data locality）特性是Hadoop数据处理的核心，并因此而获得良好的性能。意识到网络带宽是数据中心环境最珍贵的资源（到处复制数据很容易耗尽网络带宽）之后， Hadoop通过显式网络拓扑结构来保留网络带宽。注意，这种排列方式并没有降低 Hadoop对计算密集型数据进行分析的能力。<span style="color:red;">怎么在计算节点上存储数据的？怎么通过显示网络拓扑结构来保留网络带宽的？为什么没有降低 Hadoop 对计算密集型数据进行分析的能力？</span>

虽然 MPI 赋予程序员很大的控制权，但需要程序员显式处理数据流机制，包括用 C 语言构造底层的功能模块（例如套接字）和高层的数据分析算法。而 Hadoop 则在更高层次上执行任务，即程序员仅从数据模型（如MapReduce的键-值对）的角度考虑任务的执行，与此同时，数据流仍然是隐性的。

在大规模分布式计算环境下，协调各个进程的执行是一个很大的挑战。最困难的 是合理处理系统的部分失效问题（在不知道一个远程进程是否挂了的情况下）同时还需要继续完成整个计算。有了 MapReduce 这样的分布式处理框架，程序员不必操心系统失效的问题，因为框架能够检测到失败的任务并重新在正常的机器上执行。正因为采用的是无共享（shared-nothing）框架，MapReduce才能够呈现出这种特性，这意味着各个任务之间是彼此独立的。（*这里讲得太简单了一点，因为MapReduce系统本身控制着mapper输出结果传给reducer的过 程，所以在这种情况下，重新运行reducer比重新运行mapper更要小心，因为reducer需要获 取必要的mapper输出结果，如果没有，必须再次运行对应的mapper，重新生成输出结果。*）因此，从程序员的角度来看，任务的执行顺序无关紧要。相比之下，MPI程序必须显式管理自己的检查点和恢复机制，虽然赋予程序员的控制权加大了，但编程的难度也增加了。<span style="color:red;">Hadoop 是怎么处理这种系统部分失效的问题的？什么是无共享框架？为什么从程序员的角度来看，任务的执行顺序是无关紧要的？</span>

## 5.3 志愿计算

第一次听说Hadoop和MapReduce的时候，人们经常会问这个问题：“它们和 SETI@home 有什么不同？” SETI 全称为 Search for Extra-Terrestrial Intelligence（搜索外星智慧生命），在该项目中，志愿者把自己计算机CPU的空闲时间贡献出来分析无线天文望远镜的数据，借此寻找外星智慧生命信号。SETI@home是所有志愿计算项目中最有名的，其他还有“搜索大素数’ （Great Internet Mersenne Prime Search）项目与 Folding@home项目（了解蛋白质构成及其与疾病之间的关系）。<span style="color:red;">第一次知道还有这种志愿计算项目</span>

志愿计算项目将问题分成很多块，每一块称为一个工作单元（work unit），发到世界各地的计算机上进行分析。例如，SETI@hOme的工作单元是0.35 MB无线电望远镜数据，要对这等大小的数据量进行分析，一台普通计算机需要几个小时或几天时间才能完成。完成分析后，结果发送回服务器，客户端随后再获得另一个工作单元。为防止欺骗，每个工作单元要发送到3台不同的机器上执行，而且收到的结果中至少有两个相同才会被接受。<span style="color:red;">嗯。</span>

从表面上看，SETI@home 与 MapReduce好像差不多（将问题分解为独立的小块，然后并行进行计算），但事实上还是有很多明显的差异。SETI@home 问题是CPU 高度密集的，比较适合在全球成千上万台计算机上运行，因为计算所花的时间远远超过工作单元数据的传输时间。也就是说，志愿者贡献的是CPU周期，而不是网络带宽。<span style="color:red;">嗯，计算的时间远远超过传输的时间</span>

MapReduce有三大设计目标：

1. 为只需要短短几分钟或几个小时就可以完成的作业提供服务
2. 运行于同一个内部有高速网络连接的数据中心内
3. 数据中心内 的计算机都是可靠的、专门的硬件

相比之下，SETI@home 则是在接入互联网的不可信的计算机上长时间运行，这些计算机的网络带宽不同，对数据本地化也没有要求。<span style="color:red;">嗯。</span>

# 6. Apache Hadoop 发展简史

Hadoop 是 Apache Lucene 创始人道格•卡丁(Doug Cutting)创建的，Lucene 是一个应用广泛的文本搜索系统库。Hadoop 起源于开源网络搜索引擎Apache Nutch, 后者本身也是Lucene项目的一部分。

Hadoop的得名

Hadoop不是缩写，这个词是生造出来的。Hadoop之父Doug Cutting是这样解 释Hadoop来历的：

“这个名字是我的孩子给他的毛绒象玩具取的。我的命名标准是好拼读，含义宽泛，不会被用于其他地方。小朋友是这方面的高手。Googol就是他们起的。”

Hadoop生态系统中的各项目所使用的名称也往往与其功能不相关，通常也以 大象或其他动物为主题取名（例如Pig）。较小一些的组件，名称通常都有较好的 描述性（因此也更通俗）。这个原则很好，意味着我们可以望文知义，例如 namenode, 一看就知道它是用来管理文件系统命名空间（namespace）的。<span style="color:red;">namenode 是什么？</span>

从头打造一个网络搜索引擎是一个雄心勃勃的计划，不只是因为写爬虫程序很复 杂，更因为必须有一个专职团队来实现，项目中包含许许多多需要随时修改的活动部件。<span style="color:red;">为什么会有一些活动部件？</span>同时，构建这样的系统代价非常高，据迈克•加法雷拉（Mike Cafarella）和Doug Cutting 估计，一个支持10亿网页的索引系统，单单是硬件上的投入就高达50万美元，另外还有每月高达3万美元的运维费用。不过，他们认为这个工作仍然值 得投入，因为它开创的是一个优化搜索引擎算法的平台。<span style="color:red;">搜索引擎到底是什么？是怎么实现的？是通过爬虫来实现的吗？本地会存放数据吗？</span>

Nutch 项目开始于2002年，一个可以运行的网页爬取工具和搜索引擎系统很快面世。但后来，它的创造者认为这一架构的灵活性不够，不足以解决数十亿网页的 搜索问题。一篇发表于2003年的论文为此提供了帮助，文中描述的是谷歌产品架 构，该架构称为“谷歌分布式文件系统”（GFS）。GFS或类似的架构可以解决他们在网页爬取和索引过程中产生的超大文件的存储需求。特别关键的是，GFS能够节省系统管理（如管理存储节点）所花的大量时间。在2004年，Nutch的开发者开始着手做开源版本的实现，即Nutch分布式文件系统（NDFS）。

2004年，谷歌发表论文向全世界介绍他们的MapReduce系统。2005年初， Nutch的开发人员在Nutch上实现了一个MapReduce系统，到年中，Nutch的所有 主要算法均完成移植，用 MapReduce 和 NDFS 来运行。

Nutch 的 NDFS 和 MapReduce 实现不只适用于搜索领域。在2006年2月，开发人 员将NDFS和MapReduce移出Nutch形成Lucene的一个子项目，命名为 Hadoopo大约在同一时间，Doug Cutting加入雅虎，雅虎为此组织了专门的团队和资源，将Hadoop发展成能够以Web网络规模运行的系统（参见随后的补充材 料）。在2008年2月，雅虎宣布，雅虎搜索引擎使用的索引是在一个拥有1万个 内核的Hadoop集群上构建的。<span style="color:red;">这么厉害，到底是怎么实现的？</span>

2008年1月，Hadoop已成为Apache的顶级项目，证明了它的成功、多样化和生 命力。到目前为止，除雅虎之外，还有很多公司在用Hadoop,例如Last.fm、 Facebook和《纽约时报》等。

Hadoop在雅虎

构建互联网规模的搜索引擎离不开大量的数据，因此也离不开大量的机器来处 理巨量的数据。雅虎搜索引擎（Yahoo! Search）有4个主要组成部分：

- Crawler,从网页服务器爬取网页；
- WebMap,构建一个已知网页的链接图；
- Indexer、为最佳页面构建一个反向索引；
- Runtime,处理用户的查询。

WebMap 生成的链接图非常大，大约包括一万亿（1012）条边（每条边代表一个网页链接）和 一千亿（1011）个节点（每个节点代表不同的网址）。<span style="color:red;">这么厉害！到底是怎么做到的？</span>创建并分析如此大的图需要大批计算机很多天长时间运行。到2005年初，WebMap用的底层架构 Dreadnaught 需要重新设计，以便日后可以扩展到更多的节点。

Dreadnaught从20个节点成功炉展到600个，但需要完全重新设计才能进一步 扩大。Dreadnaught与MapReduce在很多方面都很相似，但灵活性更强，结构也更松散。说具体点，一个Dreadnaught作业的每一个段（fragment，也称“分块”）都可以输送到下一阶段的各个片段继续执行，排序则是通过库函数来完成的。但实际情形是，大多数 WebMap 阶段是两两一对，对应于 MapReduce。 因此，WebMap 应用不需要做大量重构操作就可以适应 MapReduce。

Eric Baldeschwieler（即EricM）组建了 一个小团队，于是我们开始设计并在GFS 和MapReduce之上用C++来建立一个新框架的原型，并打算用它来取代 Dreadnaught。尽管我们的当务之急是需要一个新的WebMap框架，但更清楚 的是，建立雅虎搜索引擎批处理平台的标准对我们更重要。使平台更通用以便支 持其他用户，才能够更好地实现新平台的均衡性投资。

与此同时，我们也关注在Hadoop（当时也是Nutch的一部分）及其进展情况。 2006年1月，雅虎聘请了 Doug Cuttingo 一个月后，我们决定放弃原型，转而 采用Hadoop。与我们的原型和设计相比，Hadoop的优势在于它已经在20个节点上实际应用过（Nutch）。这样一来，我们便能在两个月内搭建一个研究集群并能够以很快的速度帮助我们的客户使用这个新的框架。另一个显著的优点是Hadoop已经开源，比较容易（尽管也不是想象的那么容易！）从雅虎法务部门获 得许可对该开源系统进行进一步研究。因此，我们在2006年初建立了一个200节点的研究集群并暂时搁置WebMap计划，转而为研究用户提供Hadoop支持 和优化服务。

《纽约时报》的案例广为流传，他们把存档报纸扫描之后得到4TB的文件并用亚 马逊的EC2云服务将文件存为PDF格式放到网上共享。整个过程一共使用了 100台计算机，所花的时间不到24小时。如果没有亚马逊的按小时付费模式（即允 许《纽约时报》短期内访问大量机器）和Hadoop好用的并发编程模型珠联璧合， 这个项目不太可能这么快就启动和完成。<span style="color:red;">是呀！</span>


2008年4月，Hadoop打破世界纪录，成为最快的TB级数据排序系统。运行于一 个910节点的群集上，Hadoop在209秒内（不到3.5分钟）完成了对完整的1TB数 据的排序，击败了前一年的297秒冠军。同年11月，谷歌在报告中声称，它的 MapReduce对1TB数据排序只用了68秒。2009年4月，有报道称雅虎有一个团队使用Hadoop对1TB数据进行排序只花了 62秒。<span style="color:red;">到底是怎么做到的？</span>

从那以后，以更快的速度对更大规模的数据进行排序已成为一种趋势。在2014年 GraySort 基准排序大赛中，来自Databricks公司的一个团队获得并列第一。他们使用一个207个节点的Spark集群对 100TB 数据进行排序，只用了1406秒，处理速度为每分钟4.27 TB。<span style="color:red;">这么厉害，怎么做到的？</span>

目前，Hadoop被主流企业广泛使用。在工业界，Hadoop已经是公认的大数据通 用存储和分析平台，这一事实主要体现在大量直接使用或者间接包含Hadoop系统 的产品如雨后春笋般大量涌现。一些大公司包括EMC, IBM，Microsft和Oracle 以及一些专注于Hadoop的公司，如Cloudera，Hortonworks和MapR都可以提供 商业化的Hadoop支持。

# 7. 本书包含的内容

本书分为五大部分：第一部分~第三部分讲解Hadoop核心，第IV部分主要讲述 Hadoop生态系统中的相关项目，第V部分包含Hadoop实例学习。读者可以按照章节顺序阅读，也可以根据自己的需求选择阅读顺序，如图1-1所示。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180625/E22LI168gi.png?imageslim)


第一部分由5章内容组成，阐述的是Hadoop基础组件，应该在后续章节之前率先 阅读。第1章是对Hadoop的宏观介绍。第2章简要介绍MapReduce。第3章深入 剖析Hadoop文件系统，特别是HDFS。第4章讨论Hadoop集群资源管理系统

第二部分包含4章内容，对MapReduce进行深度剖析。这些内容有助于对后续章 节（如第IV部分的数据处理相关章节）的更好理解，但是可以在首次阅读时跳 过。第6章全景呈现了 MapReduce应用开发所涉及的具体步骤。第7章从用户的角度来看如何在Hadoop中实现MapReduce。第8章主要包含MapReduce编程模 型和MapReduce可以使用的各种数据格式。第9章是MapReduce高级主题，包括 排序和数据连接。

第三部分关注的是Hadoop管理，第10章和第11章主要描述如何设置和维护一个 运行HDFS和YARN（第二代MapReduce框架）的Hadoop集群。

第四部分专门介绍在Hadoop上构建或相关的项目。每一章讲述一个项目，并且很 大程度上独立于本部分其他章节，因此可按任何顺序阅读。

第四部分前两章是关于数据格式的。第12章剖析Avro, Hadoop的一种跨语言数 据序列化库。第13章描述Parquet, 一种有效的用于嵌套式数据的列式存储格式。

第14章~第15章讨论数据摄入，即如何将自己的数据输入到Hadoop系统中。第 14章介绍Flume,可以支持流数据的大批量摄入。第15章介绍Sqoop，支持在结构化数据存储（如关系型数据库）和HDFS之间高效批量传输数据。

第16章~第19章从一个比MapReduce更高一级抽象的角度，描述数据处理这样 一个共同议题。Pig（第16章）是一种用于开发大数据集的数据流语言。Hive（第17 章）是一种数据仓库，用于管理HDFS中存储的数据并提供基于SQL的查询语言。 Crunch（第18章）是一套高层次的Java API,用于写可以运行在MapReduce或 Spark上的数据处理管线程序（data processiong pipeline）。Spark（第19章）是一个面 向大规模数据处理的集群计算框架，它提供一个有向无环图（DAG，directed acyclic graph）引擎，以及支持 Scala、Java 和 Python 语言的 API。

第20章介绍HBase, 一种使用HDFS作为底层存储的，分布式的面向列的实时数 据库。第21章讲述ZooKeeper,这是一种分布式高可用性的协调服务，提供用于构建分布式应用的原语集。

最后，第五部分收集了一些实例，这些实例由以各种趣味方式使用Hadoop的用户所提供。

关于Hadoop的更多补充性资枓，例如如何在自己的机器上安装Hadoop,可以在附录中查看。
